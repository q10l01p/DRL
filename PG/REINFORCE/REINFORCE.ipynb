{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**REINFORCE**算法在策略的参数空间中直观地通过梯度上升的方法逐步提高策略 $\\pi_\\theta$ 的性能。\n",
    "\n",
    "$$ \\nabla_\\theta J(\\pi_\\theta)=\\mathbb{E}_{\\tau\\sim\\pi_\\theta}\\left[\\sum_{t=0}^\\mathrm{T}R_t\\nabla_\\theta\\sum_{t^{\\prime}=0}^t\\log\\pi_\\theta(A_{t^{\\prime}}|S_{t^{\\prime}})\\right]=\\mathbb{E}_{\\tau\\sim\\pi_\\theta}\\left[\\sum_{t^{\\prime}=0}^\\mathrm{T}\\nabla_\\theta\\log\\pi_\\theta(A_{t^{\\prime}}|S_{t^{\\prime}})\\sum_{t=t^{\\prime}}^\\mathrm{T}R_t\\right]\\tag{1} $$\n",
    "\n",
    "上述式子中 $\\sum_{t=i}^{\\mathrm{T}}R_t$ 可以看成是智能体在状态 $S_i$ 处选择动作 $A_i$，并在之后执行当前策略的情况下，从第 $i$ 步开始获得的累计奖励。\n",
    "\n",
    "事实上，$\\sum_{t=i}^{\\mathrm{T}}R_t$ 也可以看成 $Q_i(A_i,S_i)$, 在第 $i$ 步状态$S_i$ 处采取动作 $A_i$, 并在之后执行当前策略的 $Q$ 值。\n",
    "\n",
    "<font color=yellow>通过给不同的动作所对应的梯度根据它们的累计奖励赋予不同的权重，鼓励智能体选择那些累计奖励较高的动作 $A_{i}$。</font>\n",
    "\n",
    "只要把上述式子中的 $T$ 替换成 $\\infty$ 并赋予$R_t$ 以 $\\gamma^t$ 的权重，扩展到折扣因子为 $\\gamma$ 的无限范围。\n",
    "\n",
    "$$\\nabla J(\\theta)=\\mathbb{E}_{\\tau\\sim\\pi_\\theta}\\left[\\sum_{t'=0}^\\infty\\nabla_\\theta\\log\\pi_\\theta(A_{t'}|S_{t'})\\gamma^{t'}\\sum_{t=t'}^\\infty\\gamma^{t-t'}R_t\\right]\\tag{2}$$\n",
    "\n",
    "- 优点\n",
    "  - 简单直观\n",
    "- 缺点\n",
    "  - 对梯度的估计有较大的方差<sup><a href=\"#ref1\">1</a></sup>。\n",
    "\n",
    "> 1. <p name = \"ref1\">对于一个长度为 L 的轨迹，奖励 $R_t$ 的随机性可能对 L 呈指数级增长。</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import evaluate_policy, str2bool, test_policy, PolicyNet, all_seed\n",
    "from datetime import datetime\n",
    "import gymnasium as gym\n",
    "import os, shutil\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    # 创建命令行参数解析器\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # 添加各种命令行参数\n",
    "    parser.add_argument('--algo_name',default='REINFORCE_1',type=str,help=\"算法名\")\n",
    "    parser.add_argument('--dvc', type=str, default='cuda', help='运行设备: cuda 或 cpu')\n",
    "    parser.add_argument('--env_name', type=str, default='CartPole-v1', help='环境名')\n",
    "    parser.add_argument('--render_mode', type=str, default='rgb_array', help='环境渲染模式')\n",
    "    parser.add_argument('--write', type=str2bool, default=True, help='使用SummaryWriter记录训练')\n",
    "    parser.add_argument('--render', type=str2bool, default=False, help='是否渲染')\n",
    "    parser.add_argument('--Loadmodel', type=str2bool, default=False, help='是否加载预训练模型')\n",
    "    parser.add_argument('--ModelIdex', type=int, default=100000, help='要加载的模型索引')\n",
    "    parser.add_argument('--deque_maxlen',default=30,type=int)\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=1, help='随机种子')\n",
    "    parser.add_argument('--Max_train_steps', type=int, default=5e7, help='最大训练步数')\n",
    "    parser.add_argument('--save_interval', type=int, default=2e3, help='模型保存间隔，以步为单位')\n",
    "    parser.add_argument('--eval_interval', type=int, default=1e3, help='模型评估间隔，以步为单位')\n",
    "    parser.add_argument('--test_interval', type=int, default=2e3, help='视频保存间隔，以步为单位')\n",
    "\n",
    "    parser.add_argument('--gamma', type=float, default=0.98, help='折扣因子')\n",
    "    parser.add_argument('--K_epochs', type=int, default=1, help='网络更新次数')\n",
    "    parser.add_argument('--net_width', type=int, default=256, help='隐藏网络宽度')\n",
    "    parser.add_argument('--lr', type=float, default=5e-4, help='学习率')\n",
    "    \n",
    "    # 解析命令行参数\n",
    "    args = parser.parse_args([])\n",
    "    args = {**vars(args)}  # 转换成字典类型    \n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_args(args):\n",
    "    ## 打印超参数\n",
    "    print(\"超参数\")\n",
    "    print(''.join(['=']*80))\n",
    "    tplt = \"{:^20}\\t{:^20}\\t{:^20}\"\n",
    "    print(tplt.format(\"Name\", \"Value\", \"Type\"))\n",
    "    for k,v in args.items():\n",
    "        print(tplt.format(k,v,str(type(v))))   \n",
    "    print(''.join(['=']*80))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE:\n",
    "    def __init__(self, kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.policy_net = PolicyNet(self.state_dim, self.net_width, self.action_dim).to(self.dvc)\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=self.lr)  # 使用Adam优化器\n",
    "\n",
    "    def choose_action(self, state):  # 根据动作概率分布随机采样\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.dvc)\n",
    "        probs = self.policy_net(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "    \n",
    "    def discount_and_norm_rewards(self, reward_list):\n",
    "        \"\"\"\n",
    "        计算折扣回报并对其进行归一化处理。\n",
    "\n",
    "        返回:\n",
    "            np.ndarray: 归一化后的折扣回报数组\n",
    "        \"\"\"\n",
    "        # 初始化折扣回报数组\n",
    "        G_list = np.zeros_like(reward_list)\n",
    "        \n",
    "        # 计算折扣回报（G值）\n",
    "        G = 0\n",
    "        for i in reversed(range(len(reward_list))):  # 从最后一步算起\n",
    "            G = G * self.gamma + reward_list[i]\n",
    "            G_list[i] = G\n",
    "\n",
    "        # 归一化处理（均值为0，标准差为1）\n",
    "        G_list -= np.mean(G_list)\n",
    "        G_list /= np.std(G_list)\n",
    "\n",
    "        # 返回归一化后的折扣回报数组\n",
    "        return G_list\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        reward_list = transition_dict['rewards']\n",
    "        state_list = transition_dict['states']\n",
    "        action_list = transition_dict['actions']\n",
    "\n",
    "        G_list = self.discount_and_norm_rewards(reward_list)\n",
    "        G_list = torch.tensor(G_list, dtype=torch.float).to(self.dvc)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        for i in reversed(range(len(reward_list))):  # 从最后一步算起\n",
    "            state = torch.tensor([state_list[i]], dtype=torch.float).to(self.dvc)\n",
    "            action = torch.tensor([action_list[i]]).view(-1, 1).to(self.dvc)\n",
    "            log_prob = torch.log(self.policy_net(state).gather(1, action))\n",
    "            loss = -log_prob * G_list[i]  # 每一步的损失函数\n",
    "            loss.backward()  # 反向传播计算梯度\n",
    "        self.optimizer.step()  # 梯度下降\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def save(self, episode):\n",
    "        \"\"\"\n",
    "        保存当前训练模型的Actor和Critic参数到文件\n",
    "\n",
    "        参数:\n",
    "        - episode: 当前训练的episode数，用于在文件名中标识不同的保存点\n",
    "        \"\"\"\n",
    "        model_path = f\"model/{cfg['path']}\"\n",
    "        # 检查是否存在'model'文件夹，如果不存在则创建\n",
    "        try:\n",
    "            os.makedirs(model_path)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "\n",
    "        torch.save(self.policy_net.state_dict(), f\"{model_path}/ppo_critic{episode}.pth\")\n",
    "\n",
    "\n",
    "    def load(self, episode):\n",
    "        \"\"\"\n",
    "        从文件加载之前保存的Actor和Critic参数\n",
    "\n",
    "        参数:\n",
    "        - episode: 要加载的保存点的episode数\n",
    "        \"\"\"\n",
    "\n",
    "        self.policy_net.load_state_dict(torch.load(\"./model/policy_net{}.pth\".format(episode)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_agent_config(cfg, path):\n",
    "    \"\"\"\n",
    "    配置环境和代理\n",
    "\n",
    "    参数:\n",
    "    - cfg: 包含配置信息的字典\n",
    "    - path: 模型保存路径\n",
    "\n",
    "    返回:\n",
    "    - env: Gym 环境\n",
    "    - agent: PPO 代理\n",
    "\n",
    "    说明:\n",
    "    1. 创建指定环境并设置渲染模式。\n",
    "    2. 如果配置中设置了种子，则为环境设置种子。\n",
    "    3. 获取环境的状态空间维度和动作空间维度。\n",
    "    4. 更新配置字典中的状态维度和动作维度。\n",
    "    5. 创建 PPO 代理。\n",
    "\n",
    "    注意:\n",
    "    - PPO 代理的创建依赖于配置信息和模型保存路径。\n",
    "    \"\"\"\n",
    "    env = gym.make(cfg['env_name'], render_mode=cfg['render_mode'])  # 1. 创建环境\n",
    "    eval_env = gym.make(cfg['env_name'], render_mode=cfg['render_mode'])\n",
    "    if cfg['seed'] != 0:\n",
    "        all_seed(env, seed=cfg['seed'])  # 2. 如果配置中设置了种子，则为环境设置种子\n",
    "\n",
    "    n_states = env.observation_space.shape[0]  # 3. 获取状态空间维度\n",
    "    n_actions = env.action_space.n  # 获取动作空间维度\n",
    "    max_e_steps = env._max_episode_steps  # 最大步数\n",
    "    print(f\"状态空间维度：{n_states}，动作空间维度：{n_actions}，最大步数：{max_e_steps}\")\n",
    "    cfg.update({\"state_dim\": n_states, \"action_dim\": n_actions, \"max_e_steps\": max_e_steps})  # 4. 更新n_states和n_actions到cfg参数中\n",
    "\n",
    "    agent = REINFORCE(cfg)  # 5. 创建 PPO 代理\n",
    "    return env, eval_env, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态空间维度：4，动作空间维度：2，最大步数：500\n",
      "超参数\n",
      "================================================================================\n",
      "        Name        \t       Value        \t        Type        \n",
      "     algo_name      \t    REINFORCE_1     \t   <class 'str'>    \n",
      "        dvc         \t        cuda        \t   <class 'str'>    \n",
      "      env_name      \t    CartPole-v1     \t   <class 'str'>    \n",
      "    render_mode     \t     rgb_array      \t   <class 'str'>    \n",
      "       write        \t         1          \t   <class 'bool'>   \n",
      "       render       \t         0          \t   <class 'bool'>   \n",
      "     Loadmodel      \t         0          \t   <class 'bool'>   \n",
      "     ModelIdex      \t       100000       \t   <class 'int'>    \n",
      "    deque_maxlen    \t         30         \t   <class 'int'>    \n",
      "        seed        \t         1          \t   <class 'int'>    \n",
      "  Max_train_steps   \t     50000000.0     \t  <class 'float'>   \n",
      "   save_interval    \t       2000.0       \t  <class 'float'>   \n",
      "   eval_interval    \t       1000.0       \t  <class 'float'>   \n",
      "   test_interval    \t       2000.0       \t  <class 'float'>   \n",
      "       gamma        \t        0.98        \t  <class 'float'>   \n",
      "      K_epochs      \t         1          \t   <class 'int'>    \n",
      "     net_width      \t        256         \t   <class 'int'>    \n",
      "         lr         \t       0.0005       \t  <class 'float'>   \n",
      "        path        \tdevice:cuda/CartPole-v1/seed:1/REINFORCE_1/net_width-256-gamma-0.98-K_epochs-1-lr-0.0005\t   <class 'str'>    \n",
      "     state_dim      \t         4          \t   <class 'int'>    \n",
      "     action_dim     \t         2          \t<class 'numpy.int64'>\n",
      "    max_e_steps     \t        500         \t   <class 'int'>    \n",
      "     mean_break     \t       495.0        \t  <class 'float'>   \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 获取参数\n",
    "cfg = get_args()\n",
    "\n",
    "path = f\"device:{cfg['dvc']}/{cfg['env_name']}/seed:{cfg['seed']}/{cfg['algo_name']}/net_width-{cfg['net_width']}-gamma-{cfg['gamma']}-K_epochs-{cfg['K_epochs']}-lr-{cfg['lr']}\"\n",
    "cfg.update({\"path\":path}) # 更新n_states和n_actions到cfg参数中\n",
    "\n",
    "base_dir = f\"log/{cfg['path']}\"\n",
    "\n",
    "env, eval_env, agent = env_agent_config(cfg, path)\n",
    "\n",
    "cfg.update({\"mean_break\":cfg['max_e_steps'] * 0.99})\n",
    "\n",
    "print_args(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg):\n",
    "    print(\"开始训练\")\n",
    "    env_seed = cfg['seed']\n",
    "    # 使用TensorBoard记录训练曲线\n",
    "    if cfg['write']:\n",
    "        writepath = 'runs/{}'.format(cfg['path']) # 构建TensorBoard日志路径\n",
    "        if os.path.exists(writepath): shutil.rmtree(writepath)  # 如果路径已存在，则删除该路径及其内容\n",
    "        writer = SummaryWriter(log_dir=writepath)  # 创建TensorBoard写入器，指定日志路径\n",
    "\n",
    "    # 如果指定了加载模型的选项，则加载模型\n",
    "    if cfg['Loadmodel']:\n",
    "        print(\"加载模型\")\n",
    "        agent.load(cfg['ModelIdex'])\n",
    "\n",
    "    # 如果选择渲染模式\n",
    "    if cfg['render']:\n",
    "        while True:\n",
    "            # 在环境中评估智能体的性能，并输出奖励\n",
    "            ep_r = evaluate_policy(env, agent, turns=1)\n",
    "            print('Env: ', cfg['env_name'],' Episode Reward: ', {ep_r})\n",
    "    else:\n",
    "        total_steps = 0  # 记录训练步数\n",
    "        test_steps = 0  # 记录测试步数\n",
    "        scores_deque = deque(maxlen=cfg['deque_maxlen'])\n",
    "\n",
    "        # 在达到最大训练步数前一直进行训练\n",
    "        while total_steps < cfg['Max_train_steps']:\n",
    "            episode_return = 0  # 记录每个episode的奖励\n",
    "            transition_dict = {\n",
    "                'states': [],\n",
    "                'actions': [],\n",
    "                'next_states': [],\n",
    "                'rewards': [],\n",
    "                'dones': []\n",
    "                }\n",
    "            \n",
    "            s, info = env.reset(seed=env_seed)  # 重置环境，使用环境种子\n",
    "            env_seed += 1\n",
    "            done = False\n",
    "\n",
    "            # 与环境进行交互并训练\n",
    "            while not done:\n",
    "                # 选择动作和动作对应的对数概率\n",
    "                a = agent.choose_action(s)  # 在训练时使用随机动作\n",
    "                s_next, r, dw, tr, info = env.step(a)  # 执行动作并获取下一个状态、奖励以及其他信息\n",
    "                done = (dw or tr)  # 如果游戏结束（死亡或胜利），则done为True\n",
    "\n",
    "                # 存储当前的转移数据\n",
    "                transition_dict['states'].append(s)\n",
    "                transition_dict['actions'].append(a)\n",
    "                transition_dict['next_states'].append(s_next)\n",
    "                transition_dict['rewards'].append(r)\n",
    "                transition_dict['dones'].append(done)\n",
    "                s = s_next\n",
    "                episode_return += r  # 累计奖励\n",
    "                \n",
    "            total_steps += 1\n",
    "            \n",
    "            scores_deque.append(episode_return)\n",
    "\n",
    "            # 更新智能体的策略\n",
    "            for _ in range(cfg['K_epochs']):\n",
    "                loss = agent.update(transition_dict)  # 执行PPO算法的训练步骤\n",
    "            if cfg['write']:\n",
    "                writer.add_scalar('Loss', loss, global_step=total_steps)\n",
    "                writer.add_scalar('Episode_Reward', episode_return, global_step=total_steps)\n",
    "                writer.add_scalar('Episode_Average_Reward', np.mean(scores_deque), global_step=total_steps)\n",
    "\n",
    "            # 如果达到记录和日志的时间\n",
    "            if total_steps % cfg['eval_interval'] == 0:\n",
    "                # 在评估环境中评估智能体，并输出平均奖励\n",
    "                score = evaluate_policy(eval_env, agent, total_steps, turns=3)  # 对策略进行3次评估，取平均值\n",
    "                scores_deque.append(score)\n",
    "                test_steps += 1\n",
    "                if cfg['write']:\n",
    "                    writer.add_scalar('Score_ep', score, global_step=total_steps)  # 将评估得分记录到TensorBoard\n",
    "                    writer.add_scalar('Score_Average', np.mean(scores_deque), global_step=total_steps)\n",
    "                print('EnvName:', cfg['env_name'], 'seed:', cfg['seed'],\n",
    "                        'steps: {}k'.format(int(total_steps / 1000)), 'score:', score)\n",
    "                    \n",
    "            if total_steps % cfg['test_interval'] == 0:\n",
    "                test_policy(eval_env, agent, total_steps, turns=1, path=cfg['path'], cfg=cfg)\n",
    "\n",
    "            # 如果达到保存模型的时间\n",
    "            if total_steps % cfg['save_interval'] == 0:\n",
    "                print(\"保存模型\")\n",
    "                agent.save(total_steps)  # 保存模型\n",
    "\n",
    "            if (np.mean(scores_deque) >= cfg['mean_break']) and (len(scores_deque) >= cfg['deque_maxlen']):\n",
    "                print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(total_steps, np.mean(scores_deque)))\n",
    "                test_policy(eval_env, agent, total_steps, turns=1, path=cfg['path'], cfg=cfg)\n",
    "                print(\"保存模型\")\n",
    "                agent.save(total_steps)\n",
    "                env.close()\n",
    "                eval_env.close()\n",
    "                return\n",
    "\n",
    "        env.close()\n",
    "        eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13632/1776135542.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  state = torch.tensor([state], dtype=torch.float).to(self.dvc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment solved in 460 episodes!\tAverage Score: 500.00\n",
      "保存视频\n",
      "保存模型\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL_3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
