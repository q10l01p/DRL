{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import evaluate_policy, str2bool, Actor, Double_Q_Critic, ReplayBuffer, test_policy, Reward_adapter\n",
    "from datetime import datetime\n",
    "import gymnasium as gym\n",
    "import os, shutil\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "from collections import deque\n",
    "\n",
    "from torch.distributions import Beta,Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    # 创建命令行参数解析器\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # 添加各种命令行参数\n",
    "    parser.add_argument('--algo_name',default='TD3',type=str,help=\"算法名\")\n",
    "    parser.add_argument('--dvc', type=str, default='cuda', help='运行设备: cuda 或 cpu')\n",
    "    parser.add_argument('--env_name', type=str, default='Pendulum-v1', help='环境名')\n",
    "    parser.add_argument('--render_mode', type=str, default='rgb_array', help='环境渲染模式')\n",
    "    parser.add_argument('--write', type=str2bool, default=True, help='使用SummaryWriter记录训练')\n",
    "    parser.add_argument('--render', type=str2bool, default=False, help='是否渲染')\n",
    "    parser.add_argument('--Loadmodel', type=str2bool, default=False, help='是否加载预训练模型')\n",
    "    parser.add_argument('--ModelIdex', type=int, default=2350000, help='要加载的模型索引')\n",
    "    parser.add_argument('--deque_maxlen',default=20,type=int)\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=1, help='随机种子')\n",
    "    parser.add_argument('--Max_train_steps', type=int, default=5e7, help='最大训练步数')\n",
    "    parser.add_argument('--save_interval', type=int, default=5e4, help='模型保存间隔，以步为单位')\n",
    "    parser.add_argument('--eval_interval', type=int, default=2e3, help='模型评估间隔，以步为单位')\n",
    "    parser.add_argument('--test_interval', type=int, default=5e4, help='视频保存间隔，以步为单位')\n",
    "    parser.add_argument('--update_every', type=int, default=50, help='training frequency')\n",
    "\n",
    "    parser.add_argument('--gamma', type=float, default=0.99, help='折扣因子')\n",
    "    parser.add_argument('--net_width', type=int, default=256, help='隐藏网络宽度')\n",
    "    parser.add_argument('--a_lr', type=float, default=1e-4, help='Learning rate of actor')\n",
    "    parser.add_argument('--c_lr', type=float, default=1e-4, help='Learning rate of critic')\n",
    "    parser.add_argument('--batch_size', type=int, default=256, help='切片轨迹的长度')\n",
    "    parser.add_argument('--random_steps', type=int, default=500, help='切片轨迹的长度')\n",
    "    parser.add_argument('--explore_noise', type=float, default=0.15, help='exploring noise when interacting')\n",
    "    parser.add_argument('--explore_noise_decay', type=float, default=0.998, help='Decay rate of explore noise')\n",
    "    parser.add_argument('--delay_freq', type=int, default=1, help='Delayed frequency for Actor and Target Net')\n",
    "    \n",
    "    # 解析命令行参数\n",
    "    args = parser.parse_args([])\n",
    "    args = {**vars(args)}  # 转换成字典类型    \n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_args(args):\n",
    "    ## 打印超参数\n",
    "    print(\"超参数\")\n",
    "    print(''.join(['=']*80))\n",
    "    tplt = \"{:^20}\\t{:^20}\\t{:^20}\"\n",
    "    print(tplt.format(\"Name\", \"Value\", \"Type\"))\n",
    "    for k,v in args.items():\n",
    "        print(tplt.format(k,v,str(type(v))))   \n",
    "    print(''.join(['=']*80))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_seed(env, seed):\n",
    "    \"\"\"\n",
    "    设置随机种子以确保实验的可重复性\n",
    "\n",
    "    参数:\n",
    "    - env: Gym 环境，用于训练模型\n",
    "    - seed: 随机种子值\n",
    "\n",
    "    说明:\n",
    "    1. 使用给定的随机种子设置 NumPy、Python、PyTorch 和 CUDA 的随机生成器。\n",
    "    2. 禁用 CUDA 的非确定性操作以确保实验结果的一致性。\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)  # 设置 NumPy 随机种子\n",
    "    random.seed(seed)  # 设置 Python 随机种子\n",
    "    torch.manual_seed(seed)  # 设置 PyTorch 随机种子\n",
    "    torch.cuda.manual_seed(seed)  # 设置 PyTorch CUDA 随机种子\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # 设置 Python Hash 随机种子\n",
    "    torch.backends.cudnn.deterministic = True  # 禁用 CUDA 非确定性操作以确保实验结果的一致性\n",
    "    torch.backends.cudnn.benchmark = False  # 禁用 CUDA 非确定性操作以确保实验结果的一致性\n",
    "    torch.backends.cudnn.enabled = False  # 禁用 CUDA 非确定性操作以确保实验结果的一致性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD3_agent():\n",
    "    def __init__(self, kwargs):\n",
    "        \"\"\"\n",
    "        初始化强化学习代理\n",
    "\n",
    "        参数:\n",
    "        - kwargs: 包含初始化参数的字典\n",
    "\n",
    "        说明:\n",
    "        1. 更新对象的属性，使用传递的关键字参数。\n",
    "        2. 设置策略噪声、噪声裁剪和软更新参数。\n",
    "        3. 初始化演员网络，优化器，以及演员网络的目标网络。\n",
    "        4. 初始化双 Q 评论者网络，优化器，以及双 Q 评论者网络的目标网络。\n",
    "        5. 初始化经验回放缓冲区。\n",
    "        \"\"\"\n",
    "\n",
    "        self.__dict__.update(kwargs)  # 1. 更新对象的属性，使用传递的关键字参数。\n",
    "        self.policy_noise = 0.2 * self.max_action  # 2. 设置策略噪声\n",
    "        self.noise_clip = 0.5 * self.max_action  # 设置噪声裁剪\n",
    "        self.tau = 0.005  # 设置软更新参数\n",
    "        self.delay_counter = 0\n",
    "\n",
    "        self.actor = Actor(self.state_dim, self.action_dim, self.net_width, self.max_action).to(self.dvc)  # 3. 初始化演员网络\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.a_lr)  # 初始化演员网络的优化器\n",
    "        self.actor_target = copy.deepcopy(self.actor)  # 初始化演员网络的目标网络\n",
    "\n",
    "        self.q_critic = Double_Q_Critic(self.state_dim, self.action_dim, self.net_width).to(self.dvc)  # 4. 初始化双 Q 评论者网络\n",
    "        self.q_critic_optimizer = torch.optim.Adam(self.q_critic.parameters(), lr=self.c_lr)  # 初始化双 Q 评论者网络的优化器\n",
    "        self.q_critic_target = copy.deepcopy(self.q_critic)  # 初始化双 Q 评论者网络的目标网络\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(self.state_dim, self.action_dim, max_size=int(1e6), dvc=self.dvc)  # 5. 初始化经验回放缓冲区\n",
    "\n",
    "    def select_action(self, state, deterministic):\n",
    "        \"\"\"\n",
    "        选择动作\n",
    "\n",
    "        参数:\n",
    "        - state: 当前状态\n",
    "        - deterministic: 是否使用确定性策略选择动作\n",
    "\n",
    "        返回:\n",
    "        - 动作\n",
    "\n",
    "        说明:\n",
    "        1. 将状态从 [x, x, ..., x] 转换为 [[x, x, ..., x]]。\n",
    "        2. 通过 Actor 网络获取动作。\n",
    "        3. 如果是确定性策略，则直接返回动作。\n",
    "        4. 如果是非确定性策略，则添加噪声并返回，确保在动作范围内。\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state[np.newaxis, :]).to(self.dvc)  # 1. 将状态转换为网络输入格式\n",
    "            a = self.actor(state).cpu().numpy()[0]  # 2. 获取动作\n",
    "\n",
    "            if deterministic:\n",
    "                return a  # 3. 返回动作（确定性策略）\n",
    "\n",
    "            else:\n",
    "                noise = np.random.normal(0, self.max_action * self.explore_noise, size=self.action_dim)  # 4. 生成噪声\n",
    "                return (a + noise).clip(-self.max_action, self.max_action)  # 5. 添加噪声并确保在动作范围内\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        训练双 Q 评论者和演员网络\n",
    "\n",
    "        说明:\n",
    "        1. 更新延迟计数器。\n",
    "        2. 从经验回放缓冲区中采样状态、动作、奖励、下一个状态和是否结束的标志。\n",
    "        3. 生成目标动作的噪声并应用裁剪。\n",
    "        4. 计算平滑目标动作和目标 Q 值。\n",
    "        5. 计算目标 Q 值。\n",
    "        6. 计算当前 Q 值。\n",
    "        7. 计算 Q 评论者网络的损失。\n",
    "        8. 执行 Q 评论者网络的优化步骤。\n",
    "        9. 如果延迟计数器大于延迟频率，更新演员网络。\n",
    "        10. 执行演员网络的优化步骤。\n",
    "        11. 执行软更新，更新目标网络参数。\n",
    "        12. 重置延迟计数器。\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. 更新延迟计数器。\n",
    "        self.delay_counter += 1\n",
    "        \n",
    "        a_loss = 0\n",
    "        q_loss = 0\n",
    "\n",
    "        # 2. 从经验回放缓冲区中采样状态、动作、奖励、下一个状态和是否结束的标志。\n",
    "        with torch.no_grad():\n",
    "            s, a, r, s_next, dw = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "            # 3. 生成目标动作的噪声并应用裁剪。\n",
    "            target_a_noise = (torch.randn_like(a) * self.policy_noise).clamp(-self.noise_clip, self.noise_clip)\n",
    "\n",
    "            # 4. 计算平滑目标动作和目标 Q 值。\n",
    "            smoothed_target_a = (self.actor_target(s_next) + target_a_noise).clamp(-self.max_action, self.max_action)\n",
    "            target_Q1, target_Q2 = self.q_critic_target(s_next, smoothed_target_a)\n",
    "\n",
    "            # 5. 计算目标 Q 值。\n",
    "            target_Q = torch.min(target_Q1, target_Q2)\n",
    "            target_Q = r + (~dw) * self.gamma * target_Q  # dw: die or win\n",
    "\n",
    "        # 6. 计算当前 Q 值。\n",
    "        current_Q1, current_Q2 = self.q_critic(s, a)\n",
    "\n",
    "        # 7. 计算 Q 评论者网络的损失。\n",
    "        q_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "        self.q_critic_optimizer.zero_grad()\n",
    "        q_loss.backward()\n",
    "        self.q_critic_optimizer.step()\n",
    "        q_loss = q_loss.item()\n",
    "\n",
    "        if self.delay_counter > self.delay_freq:\n",
    "            # 9. 如果延迟计数器大于延迟频率，更新演员网络。\n",
    "            # Update the Actor\n",
    "            a_loss = -self.q_critic.Q1(s, self.actor(s)).mean()\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            a_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            a_loss = a_loss.item()\n",
    "\n",
    "            # 11. 执行软更新，更新目标网络参数。\n",
    "            with torch.no_grad():\n",
    "                for param, target_param in zip(self.q_critic.parameters(), self.q_critic_target.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "            # 12. 重置延迟计数器。\n",
    "            self.delay_counter = 0\n",
    "            \n",
    "        return a_loss, q_loss\n",
    "\n",
    "    def save(self, episode):\n",
    "        \"\"\"\n",
    "        保存当前训练模型的Actor和Critic参数到文件\n",
    "\n",
    "        参数:\n",
    "        - episode: 当前训练的episode数，用于在文件名中标识不同的保存点\n",
    "        \"\"\"\n",
    "        model_path = f\"model/{cfg['path']}\"\n",
    "        # 检查是否存在'model'文件夹，如果不存在则创建\n",
    "        try:\n",
    "            os.makedirs(model_path)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        # 保存Critic的参数到文件\n",
    "        torch.save(self.q_critic.state_dict(), f\"{model_path}/ddpg_critic{episode}.pth\")\n",
    "        # 保存Actor的参数到文件\n",
    "        torch.save(self.actor.state_dict(), f\"{model_path}/ppo_actor{episode}.pth\")\n",
    "\n",
    "    def load(self, episode):\n",
    "        \"\"\"\n",
    "        从文件加载之前保存的Actor和Critic参数\n",
    "\n",
    "        参数:\n",
    "        - episode: 要加载的保存点的episode数\n",
    "        \"\"\"\n",
    "        model_path = f\"model/{cfg['path']}\"\n",
    "        # 加载之前保存的Critic的参数\n",
    "        self.q_critic.load_state_dict(torch.load(f\"{model_path}/ddpg_critic{episode}.pth\"))\n",
    "        # 加载之前保存的Actor的参数\n",
    "        self.actor.load_state_dict(torch.load(f\"{model_path}/ppo_actor{episode}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_agent_config(cfg, path):\n",
    "    \"\"\"\n",
    "    配置环境和代理\n",
    "\n",
    "    参数:\n",
    "    - cfg: 包含配置信息的字典\n",
    "    - path: 模型保存路径\n",
    "\n",
    "    返回:\n",
    "    - env: Gym 环境\n",
    "    - agent: PPO 代理\n",
    "\n",
    "    说明:\n",
    "    1. 创建指定环境并设置渲染模式。\n",
    "    2. 如果配置中设置了种子，则为环境设置种子。\n",
    "    3. 获取环境的状态空间维度和动作空间维度。\n",
    "    4. 更新配置字典中的状态维度和动作维度。\n",
    "    5. 创建 PPO 代理。\n",
    "\n",
    "    注意:\n",
    "    - PPO 代理的创建依赖于配置信息和模型保存路径。\n",
    "    \"\"\"\n",
    "    env = gym.make(cfg['env_name'], render_mode=cfg['render_mode'])  # 1. 创建环境\n",
    "    eval_env = gym.make(cfg['env_name'], render_mode=cfg['render_mode'])\n",
    "    if cfg['seed'] != 0:\n",
    "        all_seed(env, seed=cfg['seed'])  # 2. 如果配置中设置了种子，则为环境设置种子\n",
    "\n",
    "    n_states = env.observation_space.shape[0]  # 3. 获取状态空间维度\n",
    "    n_actions = env.action_space.shape[0]  # 获取动作空间维度\n",
    "    max_action = float(env.action_space.high[0]) # 获取动作空间的最大值\n",
    "    max_e_steps = env._max_episode_steps  # 最大步数\n",
    "    print(f\"状态空间维度：{n_states}，动作空间维度：{n_actions}，最大步数：{max_e_steps}\")\n",
    "    cfg.update({\"state_dim\": n_states, \"action_dim\": n_actions, \"max_e_steps\": max_e_steps, \"max_action\": max_action})  # 4. 更新n_states和n_actions到cfg参数中\n",
    "\n",
    "    agent = TD3_agent(cfg)  # 5. 创建 PPO 代理\n",
    "    return env, eval_env, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态空间维度：3，动作空间维度：1，最大步数：200\n",
      "超参数\n",
      "================================================================================\n",
      "        Name        \t       Value        \t        Type        \n",
      "     algo_name      \t        TD3         \t   <class 'str'>    \n",
      "        dvc         \t        cuda        \t   <class 'str'>    \n",
      "      env_name      \t    Pendulum-v1     \t   <class 'str'>    \n",
      "    render_mode     \t     rgb_array      \t   <class 'str'>    \n",
      "       write        \t         1          \t   <class 'bool'>   \n",
      "       render       \t         0          \t   <class 'bool'>   \n",
      "     Loadmodel      \t         0          \t   <class 'bool'>   \n",
      "     ModelIdex      \t      2350000       \t   <class 'int'>    \n",
      "    deque_maxlen    \t         20         \t   <class 'int'>    \n",
      "        seed        \t         1          \t   <class 'int'>    \n",
      "  Max_train_steps   \t     50000000.0     \t  <class 'float'>   \n",
      "   save_interval    \t      50000.0       \t  <class 'float'>   \n",
      "   eval_interval    \t       2000.0       \t  <class 'float'>   \n",
      "   test_interval    \t      50000.0       \t  <class 'float'>   \n",
      "    update_every    \t         50         \t   <class 'int'>    \n",
      "       gamma        \t        0.99        \t  <class 'float'>   \n",
      "     net_width      \t        256         \t   <class 'int'>    \n",
      "        a_lr        \t       0.0001       \t  <class 'float'>   \n",
      "        c_lr        \t       0.0001       \t  <class 'float'>   \n",
      "     batch_size     \t        256         \t   <class 'int'>    \n",
      "    random_steps    \t        500         \t   <class 'int'>    \n",
      "   explore_noise    \t        0.15        \t  <class 'float'>   \n",
      "explore_noise_decay \t       0.998        \t  <class 'float'>   \n",
      "     delay_freq     \t         1          \t   <class 'int'>    \n",
      "        path        \tdevice:cuda/Pendulum-v1/seed:1/TD3/net_width-256-gamma-0.99-a_lr-0.0001-c_lr-0.0001-batch_size-256\t   <class 'str'>    \n",
      "     state_dim      \t         3          \t   <class 'int'>    \n",
      "     action_dim     \t         1          \t   <class 'int'>    \n",
      "    max_e_steps     \t        200         \t   <class 'int'>    \n",
      "     max_action     \t        2.0         \t  <class 'float'>   \n",
      "     mean_break     \t   100000000000.0   \t  <class 'float'>   \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "cfg = get_args()\n",
    "\n",
    "path = f\"device:{cfg['dvc']}/{cfg['env_name']}/seed:{cfg['seed']}/{cfg['algo_name']}/net_width-{cfg['net_width']}-gamma-{cfg['gamma']}-a_lr-{cfg['a_lr']}-c_lr-{cfg['c_lr']}-batch_size-{cfg['batch_size']}\"\n",
    "cfg.update({\"path\":path}) # 更新n_states和n_actions到cfg参数中\n",
    "\n",
    "base_dir = f\"log/{cfg['path']}\"\n",
    "\n",
    "env, eval_env, agent = env_agent_config(cfg, path)\n",
    "\n",
    "cfg.update({\"mean_break\":10e10})\n",
    "\n",
    "print_args(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg):\n",
    "    print(\"开始训练\")\n",
    "    env_seed = cfg['seed']\n",
    "    # 使用TensorBoard记录训练曲线\n",
    "    if cfg['write']:\n",
    "        writepath = 'runs/{}'.format(cfg['path']) # 构建TensorBoard日志路径\n",
    "        if os.path.exists(writepath): shutil.rmtree(writepath)  # 如果路径已存在，则删除该路径及其内容\n",
    "        writer = SummaryWriter(log_dir=writepath)  # 创建TensorBoard写入器，指定日志路径\n",
    "\n",
    "    # 如果指定了加载模型的选项，则加载模型\n",
    "    if cfg['Loadmodel']:\n",
    "        print(\"加载模型\")\n",
    "        agent.load(cfg['ModelIdex'])\n",
    "\n",
    "    # 如果选择渲染模式\n",
    "    if cfg['render']:\n",
    "        while True:\n",
    "            # 在环境中评估智能体的性能，并输出奖励\n",
    "            ep_r = evaluate_policy(env, agent, turns=1)\n",
    "            print('Env: ', cfg['env_name'],' Episode Reward: ', {ep_r})\n",
    "    else:\n",
    "        total_steps = 0\n",
    "        scores_deque = deque(maxlen=cfg['deque_maxlen'])\n",
    "        a_loss_deque = deque(maxlen=cfg['update_every'])\n",
    "        c_loss_deque = deque(maxlen=cfg['update_every'])\n",
    "\n",
    "        # 在达到最大训练步数前一直进行训练\n",
    "        while total_steps < cfg['Max_train_steps']:\n",
    "            # 重置环境，获取初始状态\n",
    "            s, info = env.reset(seed=env_seed)  # 重置环境，使用环境种子\n",
    "            env_seed += 1\n",
    "            done = False\n",
    "\n",
    "            # 与环境进行交互并训练\n",
    "            while not done:\n",
    "                if total_steps < cfg['random_steps']:\n",
    "                    a = env.action_space.sample()\n",
    "                else:\n",
    "                    a = agent.select_action(s, deterministic=False)\n",
    "                # 选择动作和动作对应的对数概率\n",
    "                s_next, r, dw, tr, info = env.step(a) # 与环境交互\n",
    "                r = Reward_adapter(r, cfg['env_name'])  # 调整奖励\n",
    "                done = (dw or tr)  # 如果游戏结束（死亡或胜利），则done为True\n",
    "\n",
    "                # 存储当前的转移数据\n",
    "                agent.replay_buffer.add(s, a, r, s_next, dw)\n",
    "                s = s_next\n",
    "                total_steps += 1\n",
    "\n",
    "                # 如果达到更新时间\n",
    "                # if total_steps >= cfg['random_steps']:\n",
    "                if (total_steps >= cfg['random_steps']) and (total_steps % cfg['update_every'] == 0):\n",
    "                    for j in range(cfg['update_every']):\n",
    "                        a_loss, c_loss = agent.train()  # 执行PPO算法的训练步骤\n",
    "                        a_loss_deque.append(a_loss)\n",
    "                        c_loss_deque.append(c_loss)\n",
    "                    if cfg['write']:\n",
    "                        writer.add_scalar('Loss_a', np.mean(a_loss_deque), global_step=total_steps)\n",
    "                        writer.add_scalar('Loss_c', np.mean(c_loss_deque), global_step=total_steps)\n",
    "\n",
    "                # 如果达到记录和日志的时间\n",
    "                if (total_steps % cfg['eval_interval'] == 0) and (total_steps >= cfg['random_steps']):\n",
    "                    agent.explore_noise *= cfg['explore_noise_decay'] # 衰减策略噪声\n",
    "                    # 在评估环境中评估智能体，并输出平均奖励\n",
    "                    score = evaluate_policy(eval_env, agent, turns=3, cfg=cfg)  # 对策略进行3次评估，取平均值\n",
    "                    scores_deque.append(score)\n",
    "                    if cfg['write']:\n",
    "                        writer.add_scalar('Score_ep', score, global_step=total_steps)  # 将评估得分记录到TensorBoard\n",
    "                        writer.add_scalar('Score_Average', np.mean(scores_deque), global_step=total_steps)\n",
    "                    print('EnvName:', cfg['env_name'], 'seed:', cfg['seed'],\n",
    "                          'steps: {}k'.format(int(total_steps / 1000)), 'score:', score)\n",
    "                    \n",
    "                if (total_steps % cfg['test_interval'] == 0) and (total_steps >= cfg['random_steps']):\n",
    "                    print(\"测试模型\")\n",
    "                    test_policy(eval_env, agent, total_steps, turns=1, path=cfg['path'], cfg=cfg)\n",
    "\n",
    "                # 如果达到保存模型的时间\n",
    "                if (total_steps % cfg['save_interval']  == 0) and (total_steps >= cfg['random_steps']):\n",
    "                    print(\"保存模型\")\n",
    "                    agent.save(total_steps)  # 保存模型\n",
    "\n",
    "                if (np.mean(scores_deque) >= cfg['mean_break']) and (len(scores_deque) >= cfg['deque_maxlen']):\n",
    "                    print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(total_steps, np.mean(scores_deque)))\n",
    "                    test_policy(eval_env, agent, total_steps, turns=1, path=cfg['path'], cfg=cfg)\n",
    "                    print(\"保存模型\")\n",
    "                    agent.save(total_steps)\n",
    "                    env.close()\n",
    "                    eval_env.close()\n",
    "                    return\n",
    "\n",
    "        env.close()\n",
    "        eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/q1001p/anaconda3/envs/DRL_3.11/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/q1001p/anaconda3/envs/DRL_3.11/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvName: Pendulum-v1 seed: 1 steps: 2k score: -1461\n",
      "EnvName: Pendulum-v1 seed: 1 steps: 4k score: -590\n",
      "EnvName: Pendulum-v1 seed: 1 steps: 6k score: -88\n",
      "EnvName: Pendulum-v1 seed: 1 steps: 8k score: -204\n",
      "EnvName: Pendulum-v1 seed: 1 steps: 10k score: -164\n"
     ]
    }
   ],
   "source": [
    "train(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL_3.11",
   "language": "python",
   "name": "drl_3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
