# PPO

PPO是**基于AC架构**的，也就是说，PPO也有两个网络，分别是Actor和Critic，这是因为AC架构有一个好处。这个好处就是**解决了连续动作空间的问题**。

## 正态分布

首先，我们要想办法处理连续动作的输出问题。

我们先说离散动作。离散动作就像一个个的按钮，按一个按钮就能智能体就做一个动作。就像在CartPole游戏里的智能体，只有0,1两个动作分别代表向左走，向右走。

那什么是连续动作呢。这就相当于这些按钮不但有开关的概念，而且还有力度大小的概念。就像我们开车，不但是前进后退转弯，并且要控制油门踩多深，刹车踩多少的，转弯时候转向转多少的问题。

于是问题来了，在离散动作空间的问题中，我们最终输出的策略呈现这样的形式。

<div align=center>
<img width="800" src="https://pic2.zhimg.com/80/v2-e125990a3eb3a9e1d88bd2aa6a61c085_720w.jpg"/>
</div>
<div align=center></div>

假设动作空间有只有action1 和 action2，有40%的概率选择action1 ，60%概率选择action2。也就是说在这个状态下的策略分布: $\pi = [0.4, 0.6]$。

同理，假如有多个动作，我们同样可以这样表示。

<div align=center>
<img width="800" src="https://pic2.zhimg.com/80/v2-5095e3e1c64efba0fac609fc82416fe9_720w.jpg"/>
</div>
<div align=center></div>

我们可以理解，把连续型概率切成很多很多份。每一份的数值，就代表该动作的概率。

<div align=center>
<img width="800" src="https://pic4.zhimg.com/80/v2-551ad9aef5390691aded75b6ae9d70f7_720w.jpg"/>
</div>
<div align=center></div>

在连续型，我们不再用数组表示，而是用函数表示。例如，策略分布函数: P = （action）代表在策略 下，选择某个action的概率P。

但这就有个问题，用**神经网络**预测输出的策略是**一个固定的shape**，而不是连续的。那又什么办法可以表示连续型的概率呢？

我们先假定策略分布函数**服从一个特殊的分布**，这个特殊的分布**可以用一两个参数表示它的图像**。

**正态分布**就是这样一个分布，他只需要**两个参数**，就可以表示了。

<div align=center>
<img width="800" src="https://pic1.zhimg.com/80/v2-643927bfb8fcee684e4e2e30a311c0e4_720w.webp"/>
</div>
<div align=center></div>

正态分布长得就是这个形状，**中间高，两边低**。他的形状由两个参数表示 $\sigma$，$\mu$。

- $\mu$ 表示平均数，也就是整个正态分布的**中轴线**。$\mu$ 的变化，表示整个图像向**左右移动**。
- $\sigma$ 表示方差，当 $\sigma$ 越大，图像越扁平；$\sigma$ 约小，图像越突出。而最大值所在的位置，就是中轴线。

我们的神经网络可以直接输出 $\mu$ 和 $\sigma$，就能获得整个**策略的概率密度函数**了。

现在我们已经有概率密度函数，那么当我们要按概率选出一个动作时，就只需要按照整个密度函数抽样出来就可以了。

## AC的问题

现在，我们解决了AC的一个问题，证明AC的框架既能处理连续状态空间问题，也能处理连续动作空间问题。

但AC还有一个问题。AC产生的数据，只能进行1次更新，更新完就只能丢掉，等待下一次跑游戏的数据。

那为什么只能用一次呢，像DQN也可以用经验回放，把以前的数据存起来，更新之后用？AC为什么就不行呢？

我们先从策略说起。

PG，是一个在线策略。因为PG用于产生数据的策略（行为策略），和需要更新的策略（目标策略）是一致。而DQN则是一个离线策略。我们会让智能体在环境互动一定次数，获得数据。用这些数据优化策略后，继续跑新的数据。

一个简化的例子：

<div align=center>
<img width="800" src="https://pic2.zhimg.com/80/v2-70462cacabead8c76b1db4328711d1fd_720w.webp"/>
</div>
<div align=center></div>

假设，我们已知在同一个环境下，有两个动作可以选择。现在两个策略，分别是P和B：$P: [0.5,0.5] B: [0.1,0.9]$

现在我们按照两个策略，进行采样；也就是分别按照这两个策略，以S状态下出发，与环境进行10次互动。获得如图数据。那么，我们可以用B策略下获得的数据，更新P吗？

答案是不行，我们可以回顾一下PG算法，PG算法会按照TD-error作为权重，更新策略。权重越大，更新幅度越大；权重越小，更新幅度越小。

从如下示意图看到，如果用行动策略B[0.1,0.9]产出的数据，对目标策略P进行更新，动作1会被更新1次，而动作2会更新9次。虽然动作A的TD-error比较大，但由于动作2更新的次数更多，最终动作2的概率会比动作1的要大。

<div align=center>
<img width="800" src="https://pic4.zhimg.com/80/v2-83960864fedc9436f74244ab999261fb_720w.webp"/>
</div>
<div align=center></div>

这自然不是我们期望看到的更新结果，因为动作1的TD-error比动作2要大，我们希望选择概率动作1的能更多呀。

但为什么DQN可以多次重复使用数据呢？

<div align=center>
<img width="800" src="https://pic3.zhimg.com/80/v2-c75e83b36e07b89f07ba23003413fc6a_720w.webp"/>
</div>
<div align=center></div>

我们可以从两个角度看：

1. 更新Q值，和策略无关。 在同一个动作出发，可能会通往不同的state，但其中的概率是有环境所决定的，而不是我们的策略所决定的。所以我们**产生的数据和策略并没有关系**。
2. 在**DQN**的更新中，我们是有"目标"的。 虽然目标比较飘忽，但每次更新，其实都是尽量**向目标靠近**。无论更新多少次，最终都会在目标附近徘徊。但**PG**算法，更新是不断**远离原来的策略分布**的，所以远离多少，远离的次数比例，我们都必须把握好。

## Important-sampling

如果我们想用策略B抽样出来的数据，来更新策略P也不是不可以。但我们要把td-error乘以一个重要性权重（IW：importance weight）。

> 重要性权重：IW = P（a）/ B（a）

在PPO，就是**目标策略出现动作a的概率除以行为策略出现a的概率**。

回到我们之前的例子，我们可以计算出，每个动作的重要性权重，$P: [0.5,0.5] B: [0.1,0.9]$

<div align=center>
<img width="800" src="https://pic2.zhimg.com/80/v2-fe8a235dd4626287e0bbf7d49a674be5_720w.webp"/>
</div>
<div align=center></div>

我们把重要性权重乘以td-error，我们发现，a1的td-error大幅提升，而a2的td-error减少了。现在即使我们用P策略: $[0.5,0.5]$ 进行更新，a1提升的概率也会比a2的更多。

PPO应用了importance sampling，使得我们用行为策略获取的数据，能够更新目标策略，把AC从在线策略，变成离线策略。

## N步更新

在示例代码中，还使用了N步更新的技术。我们把之前的TD叫做TD(0)，而N步更新为TD(n)。可以看成TD(0)其实是TD(n)的一种特殊情况。

<div align=center>
<img width="800" src="https://pic2.zhimg.com/80/v2-818c99813720c50ee6aaa2716bedec09_720w.webp"/>
</div>
<div align=center></div>

如图，实际上我们只需要计算最后的V(s')，根据这个估算的V(s'), 我们反推经过的所有state的V值。这个其实和PG估算G的过程是一样的，只不过我们并不需要走到最后，而是中途截断，用网络估算。

## 总结

1. 我们可以用AC来解决连续型控制问题。方法是输入 $\mu$ 和 $\sigma$，构造一个正态分布来表示策略;
2. PPO延展了TD(0)，变成TD(N)的N步更新;
3. AC是一个在线算法，但为了增加AC的效率，我们希望把它变成一个离线策略，这样就可以多次使用数据了。为了解决这个问题，PPO使用了**重要性采样**。

但实际上，P策略和B策略差异并不能太大，为了能处理这个问题，有两个做法，PPO1 和 PPO2 。
