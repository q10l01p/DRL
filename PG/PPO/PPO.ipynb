{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cb485b2-094f-45fb-83e1-3c056ce5ec38",
   "metadata": {},
   "source": [
    "# PPO\n",
    "PPO与TRPO的动机相同：在使用当前数据的情况下，我们如何能够在策略上迈出尽可能大的改进步骤，而不会走得太远以至于意外地导致性能崩溃？\n",
    "\n",
    "TRPO尝试通过复杂的二阶方法解决这个问题，而PPO是一阶方法，使用一些其他技巧来确保新策略接近旧策略。\n",
    "\n",
    "PPO方法在实施上显著简单，并且经验证实际上至少与TRPO一样表现良好。\n",
    "\n",
    "有两个主要的PPO变体：**PPO-Penalty**和**PPO-Clip**。\n",
    "\n",
    "## PPO-Penalty\n",
    "**PPO-Penalty**大致上解决了像TRPO一样的KL受限更新，但是在目标函数中对KL散度进行惩罚，而不是将其作为硬约束，并且在训练过程中自动调整惩罚系数，以便适当地进行缩放。\n",
    "\n",
    "## PPO-Clip\n",
    "**PPO-Clip**在目标函数中没有KL散度项，也没有任何约束。相反，它依赖于目标函数中的专门剪切，以消除新策略远离旧策略的动机。\n",
    "\n",
    "## 小结\n",
    "- PPO 是一种同策略算法\n",
    "- PPO 可用于离散或连续空间的环境\n",
    "- Spinning Up实现的PPO支持使用MPI进行并行化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76f089d-bab8-4b1e-968f-b18564e59506",
   "metadata": {},
   "source": [
    "# 数学解释\n",
    "PPO-Clip通过以下方式更新策略：\n",
    "\n",
    "$$\\theta_{k+1} = \\arg \\max_{\\theta} \\underset{s,a \\sim \\pi_{\\theta_k}}{\\mathbb{E}}\\left[ L(s,a,\\theta_k, \\theta)\\right],$$\n",
    "\n",
    "通常采用**多步（通常是小批量）SGD**来最大化这个目标。其中，$L$由以下公式给出：\n",
    "\n",
    "$$L(s,a,\\theta_k,\\theta) = \\min\\left( \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)} A^{\\pi_{\\theta_k}}(s,a), \\;\\; \\text{clip}\\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}, 1 - \\epsilon, 1+\\epsilon \\right) A^{\\pi_{\\theta_k}}(s,a) \\right),$$\n",
    "\n",
    "其中 $\\epsilon$ 是一个（较小的）超参数，大致表示**新策略允许离旧策略有多远**。\n",
    "\n",
    "这是一个相当复杂的表达式，一眼看上去很难理解它在做什么，或者它如何帮助保持新策略接近旧策略。\n",
    "\n",
    "事实上，这个目标函数有一个[相当简化的版本](https://drive.google.com/file/d/1PDzn9RPvaXjJFZkGeapMHbHGiWWW20Ey/view \"请查看有关PPO-Clip目标简化形式推导的说明。\")，稍微容易理解一些（也是我们在代码中实现的版本）：\n",
    "\n",
    "$$L(s,a,\\theta_k,\\theta) = \\min\\left( \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)} A^{\\pi_{\\theta_k}}(s,a), \\;\\; g(\\epsilon, A^{\\pi_{\\theta_k}}(s,a)) \\right),$$\n",
    "\n",
    "其中\n",
    "\n",
    "$$g(\\epsilon, A) = \\left\\{ \\begin{array}{ll} (1 + \\epsilon) A & A \\geq 0 \\\\ (1 - \\epsilon) A & A < 0. \\end{array} \\right.$$\n",
    "\n",
    "为了理解这个表达式背后的直觉，让我们看一个单独的状态动作对$(s,a)$，并思考不同的情况。\n",
    "\n",
    "当**优势是正值**时：假设该状态-动作对的优势为正值，那么其对目标的贡献就变为\n",
    "\n",
    "$$L(s,a,\\theta_k,\\theta) = \\min\\left( \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}, (1 + \\epsilon) \\right) A^{\\pi_{\\theta_k}}(s,a).$$\n",
    "\n",
    "因为优势是正值，如果动作变得更有可能发生——即，如果 $\\pi_{\\theta}(a|s)$ 增加，目标将增加。但是，在这个项中的 min 函数对目标增加的幅度进行了限制。\n",
    "\n",
    "一旦 $\\pi_{\\theta}(a|s) > (1+\\epsilon) \\pi_{\\theta_k}(a|s)$，min 函数会生效，该项就达到了 $(1+\\epsilon) A^{\\pi_{\\theta_k}}(s,a)$ 的上限。\n",
    "\n",
    "因此：新策略不会受益于远离旧策略太远。\n",
    "\n",
    "优势为负值时：假设该状态-动作对的优势为负值，那么其对目标的贡献就变为\n",
    "\n",
    "$$L(s,a,\\theta_k,\\theta) = \\max\\left( \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}, (1 - \\epsilon) \\right) A^{\\pi_{\\theta_k}}(s,a).$$\n",
    "\n",
    "因为优势是负值，如果动作变得不太可能发生——即，如果 $\\pi_{\\theta}(a|s)$ 减小，目标将增加。但是，在这个项中的 max 函数对目标增加的幅度进行了限制。\n",
    "\n",
    "一旦 $\\pi_{\\theta}(a|s) < (1-\\epsilon) \\pi_{\\theta_k}(a|s)$，max 函数会生效，该项就达到了 $(1-\\epsilon) A^{\\pi_{\\theta_k}}(s,a)$ 的上限。\n",
    "\n",
    "因此，再次强调：新策略不会受益于远离旧策略太远。\n",
    "\n",
    "<div align=center>\n",
    "<img width=\"550\" src=\"https://miro.medium.com/v2/resize:fit:1100/format:webp/1*MpPiARNoNGCxJE2a8m9itA.png \"/>\n",
    "</div>\n",
    "<div align=center></div>\n",
    "\n",
    "到目前为止，我们看到剪切作为正则化器，通过消除使策略发生显著变化的动机，而超参数 $\\epsilon$ 对应于新策略可以离旧策略多远而仍然对目标有利。\n",
    "\n",
    ">尽管这种剪切方法在很大程度上确保了合理的策略更新，但仍然有可能得到一个与旧策略相距太远的新策略。\n",
    ">不同的PPO实现使用了许多技巧来防止这种情况发生。\n",
    ">在我们这里的实现中，我们采用了一种特别简单的方法：**提前停止**。\n",
    ">**如果新策略与旧策略之间的平均KL散度超过了阈值，我们就停止梯度更新步骤**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80059ad8-4994-412d-865b-b348bcdbecaa",
   "metadata": {},
   "source": [
    "# 探索与利用\n",
    "\n",
    "PPO以在线方式训练随机策略。\n",
    "\n",
    "这意味着它**通过根据其随机策略的最新版本对动作进行采样来进行探索**。动作选择中的随机性程度取决于初始条件和训练过程。\n",
    "\n",
    "随着训练的进行，随着更新规则鼓励策略利用已经发现的奖励，策略通常会逐渐变得不太随机。这可能导致策略被困在**局部最优解中**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6426ce1-2b64-4f64-b821-67563bc9421a",
   "metadata": {},
   "source": [
    "# 伪代码\n",
    "PPO-Clip：\n",
    "\n",
    "输入：初始策略参数 $\\theta_0$，初始值函数参数 $\\phi_{0}$\n",
    "\n",
    "1. for $k=0,1,2,...$ do\n",
    "    1. 运行策略 $\\pi_k=\\pi(\\theta_k)$ 在环境中，收集轨迹集合 $D_k=\\{\\tau_i\\}$\n",
    "    2. 计算回报至终点 $\\hat{R}_t.$\n",
    "    3. 基于当前值函数 $V_\\mathrm{\\phi k}$，计算优势估计 $\\hat{A}_t$（使用任何优势估计方法）。\n",
    "    4. 通过最大化 PPO-Clip 目标更新策略：\n",
    "       $$\\theta_{k+1}=\\arg\\max_{\\theta}\\frac{1}{|\\mathcal{D}_{k}|T}\\sum_{\\tau\\in\\mathcal{D}_{k}}\\sum_{t=0}^{T}\\min\\left(\\frac{\\pi_{\\theta}(a_{t}|s_{t})}{\\pi_{\\theta_{k}}(a_{t}|s_{t})}\\hat{A}^{\\pi_{\\theta}}(s_{t},a_{t}),\\:g(\\epsilon,\\hat{A}^{\\pi_{\\theta}}_{k}(s_{t},a_{t}))\\right),$$\n",
    "   通常使用 Adam 随机梯度上升法。\n",
    "\n",
    "    6. 通过均方误差回归拟合值函数：\n",
    "       $$\\phi_{k+1}=\\arg\\min_{\\phi}\\frac{1}{|\\mathcal{D}_{k}|T}\\sum_{\\tau\\in\\mathcal{D}_{k}}\\sum_{t=0}^{T}\\left(V_{\\phi}(s_{t})-\\hat{R}_{t}\\right)^{2},$$\n",
    "       通常使用某种梯度下降算法。\n",
    "\n",
    "2. end for"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dfe3d5-973d-4cea-a4ef-645f2c8384a9",
   "metadata": {},
   "source": [
    "# 经验\n",
    "1. PPO在稀疏奖励下，表现可能不佳\n",
    "    1. MountainCar-v0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45438d6-a567-4529-bff9-b99de73785fe",
   "metadata": {},
   "source": [
    "## 智能体\n",
    "### 根据输入状态选择动作\n",
    "1. 将输入状态转换为 PyTorch 浮点型张量并发送到指定设备。\n",
    "   ```python\n",
    "   s = torch.from_numpy(s).float().to(self.dvc)\n",
    "   ```\n",
    "3. 使用训练好的 actor 模型获取动作概率分布。\n",
    "    ```python\n",
    "    pi = self.actor.pi(s, softmax_dim=0)\n",
    "    ```\n",
    "4. 如果 `1deterministic` 为 True，选择概率最高的动作。\n",
    "    ```python\n",
    "    a = torch.argmax(pi).item()\n",
    "    return a, None\n",
    "    ```\n",
    "5. 如果 `deterministic` 为 False，从概率分布中采样一个动作，并返回该动作及其对数概率。\n",
    "    ```python\n",
    "    m = Categorical(pi)  # 使用分类分布构建采样器\n",
    "    a = m.sample().item()  # 从分布中采样一个动作\n",
    "    pi_a = pi[a].item()  # 获取选择动作的对数概率\n",
    "    return a, pi_a\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926a560f-609c-44c3-80fa-a0b397234b54",
   "metadata": {},
   "source": [
    "### 训练\n",
    "1. 执行 PPO 算法的训练过程。\n",
    "    1. exploration decay：更新探索参数 entropy_coef。\n",
    "       ```python\n",
    "       # 更新熵系数，乘以熵系数衰减率\n",
    "       self.entropy_coef *= self.entropy_coef_decay\n",
    "       ```\n",
    "       \n",
    "    2. 准备 PyTorch 数据：将经验数据转换为 PyTorch 张量，并发送到指定设备。\n",
    "       ```python\n",
    "       # 将存储的环境状态、动作、奖励、下一个状态、动作的对数概率、完成标志以及权重转换为PyTorch张量，并发送到指定设备\n",
    "       s = torch.from_numpy(self.s_hoder).to(self.dvc)  # 当前状态\n",
    "       a = torch.from_numpy(self.a_hoder).to(self.dvc)  # 动作\n",
    "       r = torch.from_numpy(self.r_hoder).to(self.dvc)  # 奖励\n",
    "       s_next = torch.from_numpy(self.s_next_hoder).to(self.dvc)  # 下一个状态\n",
    "       old_prob_a = torch.from_numpy(self.logprob_a_hoder).to(self.dvc)  # 动作的对数概率\n",
    "       done = torch.from_numpy(self.done_hoder).to(self.dvc)  # 完成标志\n",
    "       dw = torch.from_numpy(self.dw_hoder).to(self.dvc)  # 权重\n",
    "       ```\n",
    "       \n",
    "    3. 使用 **TD+GAE+LongTrajectory** 方法计算 Advantage 和 TD 目标。\n",
    "        1. 获取当前状态值 vs 和下一个状态值 vs_。\n",
    "            ```PYTHON\n",
    "            # 计算当前状态的值函数 vs 和下一个状态的值函数 vs_\n",
    "            vs = self.critic(s)\n",
    "            vs_ = self.critic(s_next)\n",
    "            ```\n",
    "            \n",
    "        2. 计算 deltas（TD error）= r + gamma * vs_ * (~dw) - vs。\n",
    "            ```PYTHON\n",
    "            # 使用torch.no_grad()上下文，禁用梯度计算以提高计算效率\n",
    "            with torch.no_grad():\n",
    "                # 计算当前状态的值函数 vs 和下一个状态的值函数 vs_\n",
    "                vs = self.critic(s)\n",
    "                vs_ = self.critic(s_next)\n",
    "\n",
    "                # 计算时序差分误差（deltas），考虑奖励、折扣因子、下一个状态值函数、权重和完成标志\n",
    "                deltas = r + self.gamma * vs_ * (~dw) - vs\n",
    "                # 将时序差分误差转换为NumPy数组\n",
    "                deltas = deltas.cpu().flatten().numpy()\n",
    "                # 初始化优势列表，初始值为0\n",
    "                adv = [0]\n",
    "            ```\n",
    "            \n",
    "        5. 计算 Advantage，使用 GAE 方法考虑长时间轨迹。\n",
    "            ```PYTHON\n",
    "            # 通过反向迭代计算优势（advantage）\n",
    "            for dlt, done in zip(deltas[::-1], done.cpu().flatten().numpy()[::-1]):\n",
    "                # 计算当前时间步的优势，考虑时序差分误差、折扣因子、权重和完成标志\n",
    "                advantage = dlt + self.gamma * self.lambd * adv[-1] * (~done)\n",
    "                # 将计算得到的优势添加到优势列表\n",
    "                adv.append(advantage)\n",
    "            # 反转优势列表，以保持正确的时间顺序\n",
    "            adv.reverse()\n",
    "\n",
    "            # 剔除优势列表的最后一个元素，以去除冗余添加的初始值\n",
    "            adv = copy.deepcopy(adv[0:-1])\n",
    "            # 将优势列表转换为PyTorch张量，并进行形状调整\n",
    "            adv = torch.tensor(adv).unsqueeze(1).float().to(self.dvc)\n",
    "            ```\n",
    "        7. 计算 TD 目标，td_target = adv + vs。\n",
    "            ```PYTHON\n",
    "            # 计算时序差分目标（td_target）\n",
    "            td_target = adv + vs\n",
    "            ```\n",
    "        9. 如果开启 Advantage normalization，则对 Advantage 进行标准化。\n",
    "            ```PYTHON\n",
    "            # 如果启用了优势归一化，对优势进行归一化处理\n",
    "            if self.adv_normalization:\n",
    "                # 计算优势的均值和标准差，并进行归一化处理\n",
    "                adv = (adv - adv.mean()) / ((adv.std() + 1e-4))\n",
    "            ```\n",
    "\n",
    "    6. PPO 更新：执行多次 PPO 更新，每次包括 actor 和 critic 的更新。\n",
    "       \n",
    "        1. Slice long trajectory：将长时间轨迹切分为短时间轨迹，以执行小批量 PPO 更新。\n",
    "            ```PYTHON\n",
    "            # 计算优化迭代次数，确保能够处理所有样本\n",
    "            optim_iter_num = int(math.ceil(s.shape[0] / self.batch_size))\n",
    "            ```\n",
    "        3. 循环执行 PPO 更新过程。\n",
    "            1. Shuffle trajectory：对轨迹进行随机排列，以改善训练效果。\n",
    "                ```PYTHON\n",
    "                # 迭代执行多个优化步骤（K_epochs次）\n",
    "                for _ in range(self.K_epochs):\n",
    "                # 随机打乱样本索引的顺序\n",
    "                perm = np.arange(s.shape[0])\n",
    "                np.random.shuffle(perm)\n",
    "                # 将打乱后的索引转换为PyTorch张量并发送到指定设备\n",
    "                perm = torch.LongTensor(perm).to(self.dvc)\n",
    "\n",
    "                # 根据打乱后的索引重新排列数据，使用深度复制以避免原始数据被修改\n",
    "                s, a, td_target, adv, old_prob_a = \\\n",
    "                    s[perm].clone(), a[perm].clone(), td_target[perm].clone(), adv[perm].clone(), old_prob_a[perm].clone()\n",
    "                ```\n",
    "            2. Mini-batch PPO update：对每个 mini-batch 执行 PPO 更新。\n",
    "                ```PYTHON\n",
    "                # 遍历每个优化迭代\n",
    "                for i in range(optim_iter_num):\n",
    "                    # 确定当前迭代的样本索引范围\n",
    "                    index = slice(i * self.batch_size, min((i + 1) * self.batch_size, s.shape[0]))\n",
    "                ```\n",
    "                - Actor 更新：\n",
    "                    - 获取 actor 输出的动作概率分布 prob。\n",
    "                      ```PYTHON\n",
    "                      prob = self.actor.pi(s[index], softmax_dim=1)\n",
    "                      ```\n",
    "                    - 计算 entropy。\n",
    "                      ```PYTHON\n",
    "                      entropy = Categorical(prob).entropy().sum(0, keepdim=True)\n",
    "                      ```\n",
    "                    - 获取选择动作的对数概率 prob_a。\n",
    "                      ```python\n",
    "                      prob_a = prob.gather(1, a[index])\n",
    "                      ```\n",
    "                    - 计算 PPO 损失 a_loss。\n",
    "                      ```PYTHON\n",
    "                      # 计算新概率与旧概率的比率\n",
    "                      ratio = torch.exp(torch.log(prob_a) - torch.log(old_prob_a[index]))\n",
    "                      # 计算Clipped Surrogate函数的第一部分\n",
    "                      surr1 = ratio * adv[index]\n",
    "                      # 计算Clipped Surrogate函数的第二部分，使用torch.clamp函数将比率限制在[1 - clip_rate, 1 + clip_rate]范围内\n",
    "                      surr2 = torch.clamp(ratio, 1 - self.clip_rate, 1 + self.clip_rate) * adv[index]\n",
    "                      # 计算Actor的损失，包括负的Clipped Surrogate函数最小值和熵正则项\n",
    "                      a_loss = -torch.min(surr1, surr2) - self.entropy_coef * entropy\n",
    "                      ```\n",
    "                    - 使用反向传播和梯度裁剪更新 actor 模型参数。\n",
    "                      ```PYTHON\n",
    "                      # 将Actor的梯度置零，以防止梯度累积\n",
    "                      self.actor_optimizer.zero_grad()\n",
    "                      # 计算Actor的损失对参数的梯度\n",
    "                      a_loss.mean().backward()\n",
    "                      # 使用梯度裁剪，防止梯度爆炸\n",
    "                      torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 40)\n",
    "                      # 更新Actor的参数，执行一步优化器\n",
    "                      self.actor_optimizer.step()\n",
    "                      ```\n",
    "                - Critic 更新：\n",
    "                    - 计算 critic 的损失 c_loss。\n",
    "                      ```PYTHON\n",
    "                      # 计算均方误差损失，即Critic网络输出值与目标值的差的平方的均值\n",
    "                      c_loss = (self.critic(s[index]) - td_target[index]).pow(2).mean()\n",
    "                      ```\n",
    "                    - 添加 L2 正则化项（如果设置了 l2_reg）。\n",
    "                      ```PYTHON\n",
    "                      # 添加L2正则项，对Critic的权重参数进行平方求和，并乘以L2正则化系数self.l2_reg\n",
    "                      for name, param in self.critic.named_parameters():\n",
    "                          if 'weight' in name:\n",
    "                              c_loss += param.pow(2).sum() * self.l2_reg\n",
    "                      ```\n",
    "                    - 使用反向传播更新 critic 模型参数。\n",
    "                      ```PYTHON\n",
    "                      # 对Critic进行梯度清零，计算梯度，更新参数\n",
    "                      self.critic_optimizer.zero_grad()\n",
    "                      c_loss.backward()\n",
    "                      self.critic_optimizer.step()\n",
    "                      ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d64978-e24e-4b1e-a16b-617e2773ea6a",
   "metadata": {},
   "source": [
    "### 存储经验数据\n",
    "将每个时间步的经验数据存储到相应的缓存数组中，以便后续的训练过程使用。\n",
    "```python\n",
    "def put_data(self, s, a, r, s_next, logprob_a, done, dw, idx):\n",
    "    \"\"\"\n",
    "    存储经验数据到对应的缓存中\n",
    "\n",
    "    参数:\n",
    "    - s: 当前状态\n",
    "    - a: 采取的动作\n",
    "    - r: 获得的奖励\n",
    "    - s_next: 下一个状态\n",
    "    - logprob_a: 采取动作的对数概率\n",
    "    - done: 是否完成当前 episode\n",
    "    - dw: 重要性采样权重\n",
    "    - idx: 存储的索引位置\n",
    "    \"\"\"\n",
    "    self.s_hoder[idx] = s  # 存储当前状态到状态缓存中\n",
    "    self.a_hoder[idx] = a  # 存储采取的动作到动作缓存中\n",
    "    self.r_hoder[idx] = r  # 存储获得的奖励到奖励缓存中\n",
    "    self.s_next_hoder[idx] = s_next  # 存储下一个状态到下一个状态缓存中\n",
    "    self.logprob_a_hoder[idx] = logprob_a  # 存储采取动作的对数概率到对数概率缓存中\n",
    "    self.done_hoder[idx] = done  # 存储是否完成当前 episode 到完成标志缓存中\n",
    "    self.dw_hoder[idx] = dw  # 存储重要性采样权重到权重缓存中\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b7b114-b8a1-45c5-9003-7db46a3032c8",
   "metadata": {},
   "source": [
    "### 保存当前训练模型\n",
    "保存当前训练模型的Actor和Critic参数到文件\n",
    "```python\n",
    "def put_data(self, s, a, r, s_next, logprob_a, done, dw, idx):\n",
    "    \"\"\"\n",
    "    将经验数据存储到缓存中\n",
    "\n",
    "    参数:\n",
    "    - s: 当前状态\n",
    "    - a: 采取的动作\n",
    "    - r: 获得的奖励\n",
    "    - s_next: 下一个状态\n",
    "    - logprob_a: 采取动作的对数概率\n",
    "    - done: 是否完成当前 episode\n",
    "    - dw: 重要性采样权重\n",
    "    - idx: 存储索引\n",
    "\n",
    "    说明:\n",
    "    将每个时间步的经验数据存储到相应的缓存数组中，以便后续的训练过程使用。\n",
    "    \"\"\"\n",
    "    self.s_hoder[idx] = s  # 存储当前状态到状态缓存中\n",
    "    self.a_hoder[idx] = a  # 存储采取的动作到动作缓存中\n",
    "    self.r_hoder[idx] = r  # 存储获得的奖励到奖励缓存中\n",
    "    self.s_next_hoder[idx] = s_next  # 存储下一个状态到下一个状态缓存中\n",
    "    self.logprob_a_hoder[idx] = logprob_a  # 存储采取动作的对数概率到对数概率缓存中\n",
    "    self.done_hoder[idx] = done  # 存储是否完成当前 episode 到完成标志缓存中\n",
    "    self.dw_hoder[idx] = dw  # 存储重要性采样权重到权重缓存中\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0037bb31-4985-4381-8dc8-dc8045fb7cba",
   "metadata": {},
   "source": [
    "### 加载模型\n",
    "```python\n",
    "def load(self, episode):\n",
    "    \"\"\"\n",
    "    加载之前保存的 Critic 和 Actor 模型参数\n",
    "\n",
    "    参数:\n",
    "    - episode: 需要加载的模型参数所对应的训练 episode 数\n",
    "\n",
    "    说明:\n",
    "    1. 从指定路径加载之前保存的 Critic 模型参数。\n",
    "    2. 从指定路径加载之前保存的 Actor 模型参数。\n",
    "    \"\"\"\n",
    "    # 加载之前保存的 Critic 的参数\n",
    "    self.critic.load_state_dict(torch.load(\"./model/ppo_critic{}.pth\".format(episode)))\n",
    "    # 加载之前保存的 Actor 的参数\n",
    "    self.actor.load_state_dict(torch.load(\"./model/ppo_actor{}.pth\".format(episode)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d720ff59-7339-41c3-ba51-c13c933cac94",
   "metadata": {},
   "source": [
    "# 代码\n",
    "## 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34048760-32db-4d27-93b3-a1efc7efd43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import evaluate_policy, str2bool, test_policy\n",
    "from datetime import datetime\n",
    "import gymnasium as gym\n",
    "import os, shutil\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f72687b-683c-48a1-886f-40e687b12688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    # 创建命令行参数解析器\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # 添加各种命令行参数\n",
    "    parser.add_argument('--algo_name',default='PPO',type=str,help=\"算法名\")\n",
    "    parser.add_argument('--dvc', type=str, default='cuda', help='运行设备: cuda 或 cpu')\n",
    "    parser.add_argument('--env_name', type=str, default='CartPole-v1', help='环境名')\n",
    "    parser.add_argument('--render_mode', type=str, default='rgb_array', help='环境渲染模式')\n",
    "    parser.add_argument('--write', type=str2bool, default=True, help='使用SummaryWriter记录训练')\n",
    "    parser.add_argument('--render', type=str2bool, default=False, help='是否渲染')\n",
    "    parser.add_argument('--Loadmodel', type=str2bool, default=False, help='是否加载预训练模型')\n",
    "    parser.add_argument('--ModelIdex', type=int, default=100000, help='要加载的模型索引')\n",
    "    parser.add_argument('--deque_maxlen',default=10,type=int)\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=1, help='随机种子')\n",
    "    parser.add_argument('--T_horizon', type=int, default=500, help='长轨迹的长度')\n",
    "    parser.add_argument('--Max_train_steps', type=int, default=5e7, help='最大训练步数')\n",
    "    parser.add_argument('--save_interval', type=int, default=1e5, help='模型保存间隔，以步为单位')\n",
    "    parser.add_argument('--eval_interval', type=int, default=2e3, help='模型评估间隔，以步为单位')\n",
    "    parser.add_argument('--test_interval', type=int, default=1e5, help='视频保存间隔，以步为单位')\n",
    "\n",
    "    parser.add_argument('--gamma', type=float, default=0.99, help='折扣因子')\n",
    "    parser.add_argument('--lambd', type=float, default=0.95, help='GAE因子')\n",
    "    parser.add_argument('--clip_rate', type=float, default=0.2, help='PPO剪切率')\n",
    "    parser.add_argument('--K_epochs', type=int, default=20, help='PPO更新次数')\n",
    "    parser.add_argument('--net_width', type=int, default=256, help='隐藏网络宽度')\n",
    "    parser.add_argument('--lr', type=float, default=2e-4, help='学习率')\n",
    "    parser.add_argument('--l2_reg', type=float, default=0, help='Critic的L2正则化系数')\n",
    "    parser.add_argument('--batch_size', type=int, default=64, help='切片轨迹的长度')\n",
    "    parser.add_argument('--entropy_coef', type=float, default=0, help='Actor的熵系数')\n",
    "    parser.add_argument('--entropy_coef_decay', type=float, default=0.99, help='熵系数的衰减率')\n",
    "    parser.add_argument('--adv_normalization', type=str2bool, default=False, help='优势值正则化')\n",
    "    \n",
    "    # 解析命令行参数\n",
    "    args = parser.parse_args([])\n",
    "    args = {**vars(args)}  # 转换成字典类型    \n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aeb6494-e5a7-401e-8bbb-c8400d5ac704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_args(args):\n",
    "    ## 打印超参数\n",
    "    print(\"超参数\")\n",
    "    print(''.join(['=']*80))\n",
    "    tplt = \"{:^20}\\t{:^20}\\t{:^20}\"\n",
    "    print(tplt.format(\"Name\", \"Value\", \"Type\"))\n",
    "    for k,v in args.items():\n",
    "        print(tplt.format(k,v,str(type(v))))   \n",
    "    print(''.join(['=']*80))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dfe23e-fa8a-4062-99c7-2fd2e0a25148",
   "metadata": {},
   "source": [
    "## 网络搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce4172c-4951-48a6-93e0-f71211dfe25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight(m):\n",
    "    \"\"\"\n",
    "    初始化神经网络模型权重的函数\n",
    "\n",
    "    参数:\n",
    "    - m: 神经网络模型的模块\n",
    "\n",
    "    说明:\n",
    "    1. 判断模块是否为线性层（nn.Linear）。\n",
    "    2. 如果是线性层，使用 Xavier 正态分布初始化权重，偏置初始化为零。\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Linear):  # 判断模块是否为线性层\n",
    "        nn.init.xavier_normal_(m.weight)  # 使用 Xavier 正态分布初始化权重\n",
    "        nn.init.constant_(m.bias, 0.0)  # 偏置初始化为零\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, net_width):\n",
    "        \"\"\"\n",
    "        Actor 神经网络模型\n",
    "\n",
    "        参数:\n",
    "        - state_dim: 输入状态的维度\n",
    "        - action_dim: 输出动作的维度\n",
    "        - net_width: 神经网络的宽度\n",
    "\n",
    "        说明:\n",
    "        1. 定义三个全连接层，分别是输入层、隐藏层1和隐藏层2。\n",
    "        2. 输出层是动作的维度。\n",
    "\n",
    "        注意:\n",
    "        - forward 方法用于前向传播，pi 方法用于获取动作的概率分布。\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        self.l1 = nn.Linear(state_dim, net_width)  # 输入层\n",
    "        self.l2 = nn.Linear(net_width, net_width)  # 隐藏层1\n",
    "        self.l3 = nn.Linear(net_width, action_dim)  # 隐藏层2，输出动作维度\n",
    "        \n",
    "        self.apply(init_weight)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "\n",
    "        参数:\n",
    "        - state: 输入状态\n",
    "\n",
    "        返回:\n",
    "        - n: 前向传播的输出\n",
    "\n",
    "        说明:\n",
    "        使用 tanh 激活函数进行前向传播。\n",
    "        \"\"\"\n",
    "        n = torch.relu(self.l1(state))\n",
    "        n = torch.relu(self.l2(n))\n",
    "        return n\n",
    "\n",
    "    def pi(self, state, softmax_dim=0):\n",
    "        \"\"\"\n",
    "        获取动作的概率分布\n",
    "\n",
    "        参数:\n",
    "        - state: 输入状态\n",
    "        - softmax_dim: softmax 操作的维度，默认为0\n",
    "\n",
    "        返回:\n",
    "        - prob: 动作的概率分布\n",
    "\n",
    "        说明:\n",
    "        使用 softmax 函数获取动作的概率分布。\n",
    "        \"\"\"\n",
    "        n = self.forward(state)\n",
    "        prob = F.softmax(self.l3(n), dim=softmax_dim)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a71b9b9-e57b-404c-a59e-b2e1f38201af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, net_width):\n",
    "        \"\"\"\n",
    "        Critic 神经网络模型\n",
    "\n",
    "        参数:\n",
    "        - state_dim: 输入状态的维度\n",
    "        - net_width: 神经网络的宽度\n",
    "\n",
    "        说明:\n",
    "        定义三个全连接层，分别是输入层、隐藏层1和隐藏层2。\n",
    "        输出层是一个单一节点，用于估计状态的值。\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # 定义神经网络的三个全连接层\n",
    "        self.C1 = nn.Linear(state_dim, net_width)  # 输入层\n",
    "        self.C2 = nn.Linear(net_width, net_width)  # 隐藏层1\n",
    "        self.C3 = nn.Linear(net_width, 1)  # 隐藏层2，输出状态值\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "\n",
    "        参数:\n",
    "        - state: 输入状态\n",
    "\n",
    "        返回:\n",
    "        - v: 估计的状态值\n",
    "\n",
    "        说明:\n",
    "        使用 ReLU 激活函数进行前向传播。\n",
    "        \"\"\"\n",
    "        v = torch.relu(self.C1(state))\n",
    "        v = torch.relu(self.C2(v))\n",
    "        v = self.C3(v)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda24d7a-2f2b-41b1-a7b8-27fae8cd6825",
   "metadata": {},
   "source": [
    "## 环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6b705f5-c58f-4c0b-abfa-f69be23cdcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_seed(env, seed):\n",
    "    \"\"\"\n",
    "    设置随机种子以确保实验的可重复性\n",
    "\n",
    "    参数:\n",
    "    - env: Gym 环境，用于训练模型\n",
    "    - seed: 随机种子值\n",
    "\n",
    "    说明:\n",
    "    1. 使用给定的随机种子设置 NumPy、Python、PyTorch 和 CUDA 的随机生成器。\n",
    "    2. 禁用 CUDA 的非确定性操作以确保实验结果的一致性。\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)  # 设置 NumPy 随机种子\n",
    "    random.seed(seed)  # 设置 Python 随机种子\n",
    "    torch.manual_seed(seed)  # 设置 PyTorch 随机种子\n",
    "    torch.cuda.manual_seed(seed)  # 设置 PyTorch CUDA 随机种子\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # 设置 Python Hash 随机种子\n",
    "    torch.backends.cudnn.deterministic = True  # 禁用 CUDA 非确定性操作以确保实验结果的一致性\n",
    "    torch.backends.cudnn.benchmark = False  # 禁用 CUDA 非确定性操作以确保实验结果的一致性\n",
    "    torch.backends.cudnn.enabled = False  # 禁用 CUDA 非确定性操作以确保实验结果的一致性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a42d2f23-24e5-4553-9f94-9c6a14d89807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_agent_config(cfg, path):\n",
    "    \"\"\"\n",
    "    配置环境和代理\n",
    "\n",
    "    参数:\n",
    "    - cfg: 包含配置信息的字典\n",
    "    - path: 模型保存路径\n",
    "\n",
    "    返回:\n",
    "    - env: Gym 环境\n",
    "    - agent: PPO 代理\n",
    "\n",
    "    说明:\n",
    "    1. 创建指定环境并设置渲染模式。\n",
    "    2. 如果配置中设置了种子，则为环境设置种子。\n",
    "    3. 获取环境的状态空间维度和动作空间维度。\n",
    "    4. 更新配置字典中的状态维度和动作维度。\n",
    "    5. 创建 PPO 代理。\n",
    "\n",
    "    注意:\n",
    "    - PPO 代理的创建依赖于配置信息和模型保存路径。\n",
    "    \"\"\"\n",
    "    env = gym.make(cfg['env_name'], render_mode=cfg['render_mode'])  # 1. 创建环境\n",
    "    eval_env = gym.make(cfg['env_name'], render_mode=cfg['render_mode'])\n",
    "    if cfg['seed'] != 0:\n",
    "        all_seed(env, seed=cfg['seed'])  # 2. 如果配置中设置了种子，则为环境设置种子\n",
    "\n",
    "    n_states = env.observation_space.shape[0]  # 3. 获取状态空间维度\n",
    "    n_actions = env.action_space.n  # 获取动作空间维度\n",
    "    max_e_steps = env._max_episode_steps  # 最大步数\n",
    "    print(f\"状态空间维度：{n_states}，动作空间维度：{n_actions}，最大步数：{max_e_steps}\")\n",
    "    cfg.update({\"state_dim\": n_states, \"action_dim\": n_actions, \"max_e_steps\": max_e_steps})  # 4. 更新n_states和n_actions到cfg参数中\n",
    "\n",
    "    agent = PPO_discrete(cfg)  # 5. 创建 PPO 代理\n",
    "    return env, eval_env, agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d3273-e133-4c39-b3e7-24eb6cdd8758",
   "metadata": {},
   "source": [
    "## 智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05ac69b9-fdd0-42ef-b758-c77f8e79143f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PPO_discrete():\n",
    "    def __init__(self, kwargs):\n",
    "        \"\"\"\n",
    "        初始化PPO代理的超参数和模型\n",
    "\n",
    "        参数:\n",
    "        - kwargs: 包含所有超参数的字典\n",
    "\n",
    "        说明:\n",
    "        1. 通过字典更新self的属性，将超参数初始化为代理的属性。\n",
    "        2. 创建Actor和Critic模型，并初始化优化器。\n",
    "        3. 构建用于存储轨迹数据的缓存。\n",
    "        \"\"\"\n",
    "        # 使用字典中的键值对更新self的属性，将超参数初始化为代理的属性\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        # 创建Actor模型，接收状态维度、动作维度和网络宽度作为输入\n",
    "        self.actor = Actor(self.state_dim, self.action_dim, self.net_width).to(self.dvc)\n",
    "        # 创建Actor模型的优化器，使用Adam优化器，学习率为self.lr\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.lr)\n",
    "        # 创建Critic模型，接收状态维度和网络宽度作为输入\n",
    "        self.critic = Critic(self.state_dim, self.net_width).to(self.dvc)\n",
    "        # 创建Critic模型的优化器，使用Adam优化器，学习率为self.lr\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.lr)\n",
    "\n",
    "        # 初始化存储状态的缓存，形状为(T_horizon, state_dim)\n",
    "        self.s_hoder = np.zeros((self.T_horizon, self.state_dim), dtype=np.float32)\n",
    "        # 初始化存储动作的缓存，形状为(T_horizon, 1)\n",
    "        self.a_hoder = np.zeros((self.T_horizon, 1), dtype=np.int64)\n",
    "        # 初始化存储奖励的缓存，形状为(T_horizon, 1)\n",
    "        self.r_hoder = np.zeros((self.T_horizon, 1), dtype=np.float32)\n",
    "        # 初始化存储下一个状态的缓存，形状为(T_horizon, state_dim)\n",
    "        self.s_next_hoder = np.zeros((self.T_horizon, self.state_dim), dtype=np.float32)\n",
    "        # 初始化存储对数概率的缓存，形状为(T_horizon, 1)\n",
    "        self.logprob_a_hoder = np.zeros((self.T_horizon, 1), dtype=np.float32)\n",
    "        # 初始化存储完成标志的缓存，形状为(T_horizon, 1)\n",
    "        self.done_hoder = np.zeros((self.T_horizon, 1), dtype=np.bool_)\n",
    "        # 初始化存储重要性采样权重的缓存，形状为(T_horizon, 1)\n",
    "        self.dw_hoder = np.zeros((self.T_horizon, 1), dtype=np.bool_)\n",
    "\n",
    "    def select_action(self, s, deterministic):\n",
    "        \"\"\"\n",
    "        根据输入状态选择动作\n",
    "\n",
    "        参数:\n",
    "        - s: 当前环境状态，NumPy 数组\n",
    "        - deterministic: 一个布尔值，确定是否选择确定性动作\n",
    "\n",
    "        返回:\n",
    "        - 如果 deterministic 为 True，返回选择的动作和 None；\n",
    "        - 如果 deterministic 为 False， 返回选择的动作和相应动作的对数概率。\n",
    "\n",
    "        说明:\n",
    "        1. 将输入状态转换为 PyTorch 浮点型张量并发送到指定设备。\n",
    "        2. 使用训练好的 actor 模型获取动作概率分布。\n",
    "        3. 如果 deterministic 为 True，选择概率最高的动作。\n",
    "        4. 如果 deterministic 为 False，从概率分布中采样一个动作，并返回该动作及其对数概率。\n",
    "        \"\"\"\n",
    "\n",
    "        s = torch.from_numpy(s).float().to(self.dvc)  # 将输入状态转换为 PyTorch 浮点型张量并发送到指定设备\n",
    "        with torch.no_grad():\n",
    "            pi = self.actor.pi(s, softmax_dim=0)  # 使用 actor 模型获取动作概率分布\n",
    "\n",
    "            if deterministic:\n",
    "                a = torch.argmax(pi).item()  # 选择概率最高的动作\n",
    "                return a, None\n",
    "            else:\n",
    "                m = Categorical(pi)  # 使用分类分布构建采样器\n",
    "                a = m.sample().item()  # 从分布中采样一个动作\n",
    "                pi_a = pi[a].item()  # 获取选择动作的对数概率\n",
    "                return a, pi_a\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        训练 PPO 算法，包括计算 Advantage 和 TD 目标，以及执行 PPO 更新。\n",
    "\n",
    "        说明:\n",
    "        1. 执行 PPO 算法的训练过程。\n",
    "\n",
    "            a. exploration decay：更新探索参数 entropy_coef。\n",
    "\n",
    "            b. 准备 PyTorch 数据：将经验数据转换为 PyTorch 张量，并发送到指定设备。\n",
    "\n",
    "            c. 使用 TD+GAE+LongTrajectory 方法计算 Advantage 和 TD 目标。\n",
    "                i. 获取当前状态值 vs 和下一个状态值 vs_。\n",
    "                ii. 计算 deltas（TD error）：r + gamma * vs_ * (~dw) - vs。\n",
    "                iii. 计算 Advantage，使用 GAE 方法考虑长时间轨迹。\n",
    "                iv. 计算 TD 目标，td_target = adv + vs。\n",
    "                v. 如果开启 Advantage normalization，则对 Advantage 进行标准化。\n",
    "\n",
    "            d. PPO 更新：执行多次 PPO 更新，每次包括 actor 和 critic 的更新。\n",
    "                i. Slice long trajectory：将长时间轨迹切分为短时间轨迹，以执行小批量 PPO 更新。\n",
    "                ii. 循环执行 PPO 更新过程。\n",
    "                    A. Shuffle trajectory：对轨迹进行随机排列，以改善训练效果。\n",
    "                    B. Mini-batch PPO update：对每个 mini-batch 执行 PPO 更新。\n",
    "                        - Actor 更新：\n",
    "                            - 获取 actor 输出的动作概率分布 prob。\n",
    "                            - 计算 entropy。\n",
    "                            - 获取选择动作的对数概率 prob_a。\n",
    "                            - 计算 PPO 损失 a_loss。\n",
    "                            - 使用反向传播和梯度裁剪更新 actor 模型参数。\n",
    "                        - Critic 更新：\n",
    "                            - 计算 critic 的损失 c_loss。\n",
    "                            - 添加 L2 正则化项（如果设置了 l2_reg）。\n",
    "                            - 使用反向传播更新 critic 模型参数。\n",
    "\n",
    "        \"\"\"\n",
    "        # 更新熵系数，乘以熵系数衰减率\n",
    "        self.entropy_coef *= self.entropy_coef_decay\n",
    "\n",
    "        # 将存储的环境状态、动作、奖励、下一个状态、动作的对数概率、完成标志以及权重转换为PyTorch张量，并发送到指定设备\n",
    "        s = torch.from_numpy(self.s_hoder).to(self.dvc)  # 当前状态\n",
    "        a = torch.from_numpy(self.a_hoder).to(self.dvc)  # 动作\n",
    "        r = torch.from_numpy(self.r_hoder).to(self.dvc)  # 奖励\n",
    "        s_next = torch.from_numpy(self.s_next_hoder).to(self.dvc)  # 下一个状态\n",
    "        old_prob_a = torch.from_numpy(self.logprob_a_hoder).to(self.dvc)  # 动作的对数概率\n",
    "        done = torch.from_numpy(self.done_hoder).to(self.dvc)  # 完成标志\n",
    "        dw = torch.from_numpy(self.dw_hoder).to(self.dvc)  # 权重\n",
    "\n",
    "        # 使用torch.no_grad()上下文，禁'path'用梯度计算以提高计算效率\n",
    "        with torch.no_grad():\n",
    "            # 计算当前状态的值函数 vs 和下一个状态的值函数 vs_\n",
    "            vs = self.critic(s)\n",
    "            vs_ = self.critic(s_next)\n",
    "\n",
    "            # 计算时序差分误差（deltas），考虑奖励、折扣因子、下一个状态值函数、权重和完成标志\n",
    "            deltas = r + self.gamma * vs_ * (~dw) - vs\n",
    "            # 将时序差分误差转换为NumPy数组\n",
    "            deltas = deltas.cpu().flatten().numpy()\n",
    "            # 初始化优势列表，初始值为0\n",
    "            adv = [0]\n",
    "\n",
    "            # 通过反向迭代计算优势（advantage）\n",
    "            for dlt, done in zip(deltas[::-1], done.cpu().flatten().numpy()[::-1]):\n",
    "                # 计算当前时间步的优势，考虑时序差0.002分误差、折扣因子、权重和完成标志\n",
    "                advantage = dlt + self.gamma * self.lambd * adv[-1] * (~done)\n",
    "                # 将计算得到的优势添加到优势列表\n",
    "                adv.append(advantage)\n",
    "            # 反转优势列表，以保持正确的时间顺序\n",
    "            adv.reverse()\n",
    "\n",
    "            # 剔除优势列表的最后一个元素，以去除冗余添加的初始值\n",
    "            adv = copy.deepcopy(adv[0:-1])\n",
    "            # 将优势列表转换为PyTorch张量，并进行形状调整\n",
    "            adv = torch.tensor(adv).unsqueeze(1).float().to(self.dvc)\n",
    "\n",
    "            # 计算时序差分目标（td_target）\n",
    "            td_target = adv + vs\n",
    "\n",
    "            # 如果启用了优势归一化，对优势进行归一化处理\n",
    "            if self.adv_normalization:\n",
    "                # 计算优势的均值和标准差，并进行归一化处理\n",
    "                adv = (adv - adv.mean()) / ((adv.std() + 1e-4))\n",
    "\n",
    "        # 计算优化迭代次数，确保能够处理所有样本\n",
    "        optim_iter_num = int(math.ceil(s.shape[0] / self.batch_size))\n",
    "\n",
    "        # 迭代执行多个优化步骤（K_epochs次）\n",
    "        for _ in range(self.K_epochs):\n",
    "            # 随机打乱样本索引的顺序\n",
    "            perm = np.arange(s.shape[0])\n",
    "            np.random.shuffle(perm)\n",
    "            # 将打乱后的索引转换为PyTorch张量并发送到指定设备\n",
    "            perm = torch.LongTensor(perm).to(self.dvc)\n",
    "\n",
    "            # 根据打乱后的索引重新排列数据，使用深度复制以避免原始数据被修改\n",
    "            s, a, td_target, adv, old_prob_a = \\\n",
    "                s[perm].clone(), a[perm].clone(), td_target[perm].clone(), adv[perm].clone(), old_prob_a[perm].clone()\n",
    "\n",
    "            # 遍历每个优化迭代\n",
    "            for i in range(optim_iter_num):\n",
    "                # 确定当前迭代的样本索引范围\n",
    "                index = slice(i * self.batch_size, min((i + 1) * self.batch_size, s.shape[0]))\n",
    "\n",
    "                # 计算策略的概率分布及其熵\n",
    "                prob = self.actor.pi(s[index], softmax_dim=1)\n",
    "                entropy = Categorical(prob).entropy().sum(0, keepdim=True)\n",
    "\n",
    "                # 计算新旧概率比率、Clipped Surrogate函数及Actor的损失\n",
    "                # 通过索引操作，获取当前动作对应的概率\n",
    "                prob_a = prob.gather(1, a[index])\n",
    "                # 计算新概率与旧概率的比率\n",
    "                ratio = torch.exp(torch.log(prob_a) - torch.log(old_prob_a[index]))\n",
    "                # 计算Clipped Surrogate函数的第一部分\n",
    "                surr1 = ratio * adv[index]\n",
    "                # 计算Clipped Surrogate函数的第二部分，使用torch.clamp函数将比率限制在[1 - clip_rate, 1 + clip_rate]范围内\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_rate, 1 + self.clip_rate) * adv[index]\n",
    "                # 计算Actor的损失，包括负的Clipped Surrogate函数最小值和熵正则项\n",
    "                a_loss = -torch.min(surr1, surr2) - self.entropy_coef * entropy\n",
    "\n",
    "                # 对Actor进行梯度清零，计算梯度，进行梯度裁剪，更新参数\n",
    "                # 将Actor的梯度置零，以防止梯度累积\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                # 计算Actor的损失对参数的梯度\n",
    "                a_loss.mean().backward()\n",
    "                # 使用梯度裁剪，防止梯度爆炸\n",
    "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 40)\n",
    "                # 更新Actor的参数，执行一步优化器\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                # 计算Critic的损失，包括均方误差损失和L2正则项\n",
    "                # 计算均方误差损失，即Critic网络输出值与目标值的差的平方的均值\n",
    "                c_loss = (self.critic(s[index]) - td_target[index]).pow(2).mean()\n",
    "                # 添加L2正则项，对Critic的权重参数进行平方求和，并乘以L2正则化系数self.l2_reg\n",
    "                for name, param in self.critic.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        c_loss += param.pow(2).sum() * self.l2_reg\n",
    "\n",
    "                # 对Critic进行梯度清零，计算梯度，更新参数\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                c_loss.backward()\n",
    "                self.critic_optimizer.step()\n",
    "                \n",
    "        return a_loss, c_loss\n",
    "\n",
    "    def put_data(self, s, a, r, s_next, logprob_a, done, dw, idx):\n",
    "        \"\"\"\n",
    "        存储经验数据到对应的缓存中\n",
    "\n",
    "        参数:\n",
    "        - s: 当前状态\n",
    "        - a: 采取的动作\n",
    "        - r: 获得的奖励\n",
    "        - s_next: 下一个状态\n",
    "        - logprob_a: 采取动作的对数概率\n",
    "        - done: 是否完成当前 episode\n",
    "        - dw: 重要性采样权重\n",
    "        - idx: 存储的索引位置\n",
    "        \"\"\"\n",
    "        self.s_hoder[idx] = s  # 存储当前状态到状态缓存中\n",
    "        self.a_hoder[idx] = a  # 存储采取的动作到动作缓存中\n",
    "        self.r_hoder[idx] = r  # 存储获得的奖励到奖励缓存中\n",
    "        self.s_next_hoder[idx] = s_next  # 存储下一个状态到下一个状态缓存中\n",
    "        self.logprob_a_hoder[idx] = logprob_a  # 存储采取动作的对数概率到对数概率缓存中\n",
    "        self.done_hoder[idx] = done  # 存储是否完成当前 episode 到完成标志缓存中\n",
    "        self.dw_hoder[idx] = dw  # 存储重要性采样权重到权重缓存中\n",
    "\n",
    "    def save(self, episode):\n",
    "        \"\"\"\n",
    "        保存当前训练模型的Actor和Critic参数到文件\n",
    "\n",
    "        参数:\n",
    "        - episode: 当前训练的episode数，用于在文件名中标识不同的保存点\n",
    "        \"\"\"\n",
    "        model_path = f\"model/{cfg['path']}\"\n",
    "        # 检查是否存在'model'文件夹，如果不存在则创建\n",
    "        try:\n",
    "            os.makedirs(model_path)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        # 保存Critic的参数到文件\n",
    "        torch.save(self.critic.state_dict(), f\"{model_path}/ppo_critic{episode}.pth\")\n",
    "        # 保存Actor的参数到文件\n",
    "        torch.save(self.actor.state_dict(), f\"{model_path}/ppo_actor{episode}.pth\")\n",
    "\n",
    "    def load(self, episode):\n",
    "        \"\"\"\n",
    "        从文件加载之前保存的Actor和Critic参数\n",
    "\n",
    "        参数:\n",
    "        - episode: 要加载的保存点的episode数\n",
    "        \"\"\"\n",
    "        # 加载之前保存的Critic的参数\n",
    "        self.critic.load_state_dict(torch.load(\"./model/ppo_critic{}.pth\".format(episode)))\n",
    "        # 加载之前保存的Actor的参数\n",
    "        self.actor.load_state_dict(torch.load(\"./model/ppo_actor{}.pth\".format(episode)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230eebb2-f0cb-46ec-b5f4-26138544bbfe",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5e0026d-bfea-472c-81f3-f7a19c92219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg):\n",
    "    print(\"开始训练\")\n",
    "    env_seed = cfg['seed']\n",
    "    # 使用TensorBoard记录训练曲线\n",
    "    if cfg['write']:\n",
    "        writepath = 'runs/{}'.format(cfg['path']) # 构建TensorBoard日志路径\n",
    "        if os.path.exists(writepath): shutil.rmtree(writepath)  # 如果路径已存在，则删除该路径及其内容\n",
    "        writer = SummaryWriter(log_dir=writepath)  # 创建TensorBoard写入器，指定日志路径\n",
    "\n",
    "    # 如果指定了加载模型的选项，则加载模型\n",
    "    if cfg['Loadmodel']:\n",
    "        print(\"加载模型\")\n",
    "        agent.load(cfg['ModelIdex'])\n",
    "\n",
    "    # 如果选择渲染模式\n",
    "    if cfg['render']:\n",
    "        while True:\n",
    "            # 在环境中评估智能体的性能，并输出奖励\n",
    "            ep_r = evaluate_policy(env, agent, turns=1)\n",
    "            print('Env: ', cfg['env_name'],' Episode Reward: ', {ep_r})\n",
    "    else:\n",
    "        traj_length, total_steps, test_steps = 0, 0, 0\n",
    "        scores_deque = deque(maxlen=cfg['deque_maxlen'])\n",
    "\n",
    "        # 在达到最大训练步数前一直进行训练\n",
    "        while total_steps < cfg['Max_train_steps']:\n",
    "            \n",
    "            scores = []\n",
    "            i_episode_total = 1\n",
    "            \n",
    "            s, info = env.reset(seed=env_seed)  # 重置环境，使用环境种子\n",
    "            env_seed += 1\n",
    "            done = False\n",
    "\n",
    "            # 与环境进行交互并训练\n",
    "            while not done:\n",
    "                # 选择动作和动作对应的对数概率\n",
    "                a, logprob_a = agent.select_action(s, deterministic=False)  # 在训练时使用随机动作\n",
    "                s_next, r, dw, tr, info = env.step(a)  # 执行动作并获取下一个状态、奖励以及其他信息\n",
    "                done = (dw or tr)  # 如果游戏结束（死亡或胜利），则done为True\n",
    "\n",
    "                # 存储当前的转移数据\n",
    "                agent.put_data(s, a, r, s_next, logprob_a, done, dw, idx=traj_length)\n",
    "                s = s_next\n",
    "\n",
    "                traj_length += 1\n",
    "                total_steps += 1\n",
    "\n",
    "                # 如果达到更新时间\n",
    "                if traj_length % cfg['T_horizon'] == 0:\n",
    "                    a_loss, c_loss = agent.train()  # 执行PPO算法的训练步骤\n",
    "                    if cfg['write']:\n",
    "                        writer.add_scalar('Loss_a', np.mean(a_loss.detach().cpu().numpy()), global_step=total_steps)\n",
    "                        writer.add_scalar('Loss_c', np.mean(c_loss.detach().cpu().numpy()), global_step=total_steps)\n",
    "                    traj_length = 0\n",
    "\n",
    "                # 如果达到记录和日志的时间\n",
    "                if total_steps % cfg['eval_interval'] == 0:\n",
    "                    # 在评估环境中评估智能体，并输出平均奖励\n",
    "                    score = evaluate_policy(eval_env, agent, total_steps, turns=3)  # 对策略进行3次评估，取平均值\n",
    "                    scores_deque.append(score)\n",
    "                    test_steps += 1\n",
    "                    if cfg['write']:\n",
    "                        writer.add_scalar('Score_ep', score, global_step=total_steps)  # 将评估得分记录到TensorBoard\n",
    "                        writer.add_scalar('Score_Average', np.mean(scores_deque), global_step=total_steps)\n",
    "                    print('EnvName:', cfg['env_name'], 'seed:', cfg['seed'],\n",
    "                          'steps: {}k'.format(int(total_steps / 1000)), 'score:', score)\n",
    "                    \n",
    "                if total_steps % cfg['test_interval'] == 0:\n",
    "                    test_policy(eval_env, agent, total_steps, turns=1, path=cfg['path'])\n",
    "\n",
    "                # 如果达到保存模型的时间\n",
    "                if total_steps % cfg['save_interval'] == 0:\n",
    "                    print(\"保存模型\")\n",
    "                    agent.save(total_steps)  # 保存模型\n",
    "\n",
    "                if (np.mean(scores_deque) >= cfg['mean_break']) and (len(scores_deque) >= cfg['deque_maxlen']):\n",
    "                    print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(total_steps, np.mean(scores_deque)))\n",
    "                    test_policy(eval_env, agent, total_steps, turns=1, path=cfg['path'], cfg=cfg)\n",
    "                    print(\"保存模型\")\n",
    "                    agent.save(total_steps)\n",
    "                    env.close()\n",
    "                    eval_env.close()\n",
    "                    return\n",
    "\n",
    "        env.close()\n",
    "        eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aada328d-5f7c-48a3-8007-16307d84b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取参数\n",
    "cfg = get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1afb341-e477-4ee8-97d0-da6edd6d36ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态空间维度：4，动作空间维度：2，最大步数：500\n"
     ]
    }
   ],
   "source": [
    "path = f\"device:{cfg['dvc']}/{cfg['env_name']}/seed:{cfg['seed']}/{cfg['algo_name']}/net_width-{cfg['net_width']}T_horizon-{cfg['T_horizon']}-gamma-{cfg['gamma']}-lambd-{cfg['lambd']}-clip_rate-{cfg['clip_rate']}-K_epochs-{cfg['K_epochs']}-lr-{cfg['lr']}-l2_reg-{cfg['l2_reg']}-batch_size-{cfg['batch_size']}-entropy_coef-{cfg['entropy_coef']}-entropy_coef_decay-{cfg['entropy_coef_decay']}-adv_normalization-{cfg['adv_normalization']}\"\n",
    "cfg.update({\"path\":path}) # 更新n_states和n_actions到cfg参数中\n",
    "\n",
    "base_dir = f\"log/{cfg['path']}\"\n",
    "\n",
    "env, eval_env, agent = env_agent_config(cfg, path)\n",
    "\n",
    "cfg.update({\"mean_break\":cfg['max_e_steps'] * 0.99})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a492780c-b1b4-4c46-9cd1-85a4a1048e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "超参数\n",
      "================================================================================\n",
      "        Name        \t       Value        \t        Type        \n",
      "     algo_name      \t        PPO         \t   <class 'str'>    \n",
      "        dvc         \t        cuda        \t   <class 'str'>    \n",
      "      env_name      \t    CartPole-v1     \t   <class 'str'>    \n",
      "    render_mode     \t     rgb_array      \t   <class 'str'>    \n",
      "       write        \t         1          \t   <class 'bool'>   \n",
      "       render       \t         0          \t   <class 'bool'>   \n",
      "     Loadmodel      \t         0          \t   <class 'bool'>   \n",
      "     ModelIdex      \t       100000       \t   <class 'int'>    \n",
      "    deque_maxlen    \t         10         \t   <class 'int'>    \n",
      "        seed        \t         1          \t   <class 'int'>    \n",
      "     T_horizon      \t        500         \t   <class 'int'>    \n",
      "  Max_train_steps   \t     50000000.0     \t  <class 'float'>   \n",
      "   save_interval    \t      100000.0      \t  <class 'float'>   \n",
      "   eval_interval    \t       2000.0       \t  <class 'float'>   \n",
      "   test_interval    \t      100000.0      \t  <class 'float'>   \n",
      "       gamma        \t        0.99        \t  <class 'float'>   \n",
      "       lambd        \t        0.95        \t  <class 'float'>   \n",
      "     clip_rate      \t        0.2         \t  <class 'float'>   \n",
      "      K_epochs      \t         20         \t   <class 'int'>    \n",
      "     net_width      \t        256         \t   <class 'int'>    \n",
      "         lr         \t       0.0002       \t  <class 'float'>   \n",
      "       l2_reg       \t         0          \t   <class 'int'>    \n",
      "     batch_size     \t         64         \t   <class 'int'>    \n",
      "    entropy_coef    \t         0          \t   <class 'int'>    \n",
      " entropy_coef_decay \t        0.99        \t  <class 'float'>   \n",
      " adv_normalization  \t         0          \t   <class 'bool'>   \n",
      "        path        \tdevice:cuda/CartPole-v1/seed:1/PPO/net_width-256T_horizon-500-gamma-0.99-lambd-0.95-clip_rate-0.2-K_epochs-20-lr-0.0002-l2_reg-0-batch_size-64-entropy_coef-0-entropy_coef_decay-0.99-adv_normalization-False\t   <class 'str'>    \n",
      "     state_dim      \t         4          \t   <class 'int'>    \n",
      "     action_dim     \t         2          \t<class 'numpy.int64'>\n",
      "    max_e_steps     \t        500         \t   <class 'int'>    \n",
      "     mean_break     \t       495.0        \t  <class 'float'>   \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print_args(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86483326-9c0a-48a6-8fb2-953aa0d910c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jie/anaconda3/envs/DRL_3.11/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/jie/anaconda3/envs/DRL_3.11/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvName: CartPole-v1 seed: 1 steps: 2k score: 339\n",
      "EnvName: CartPole-v1 seed: 1 steps: 4k score: 293\n",
      "EnvName: CartPole-v1 seed: 1 steps: 6k score: 393\n",
      "EnvName: CartPole-v1 seed: 1 steps: 8k score: 442\n",
      "EnvName: CartPole-v1 seed: 1 steps: 10k score: 500\n",
      "EnvName: CartPole-v1 seed: 1 steps: 12k score: 500\n",
      "EnvName: CartPole-v1 seed: 1 steps: 14k score: 500\n",
      "EnvName: CartPole-v1 seed: 1 steps: 16k score: 500\n",
      "EnvName: CartPole-v1 seed: 1 steps: 18k score: 500\n",
      "EnvName: CartPole-v1 seed: 1 steps: 20k score: 500\n",
      "EnvName: CartPole-v1 seed: 1 steps: 22k score: 500\n",
      "EnvName: CartPole-v1 seed: 1 steps: 24k score: 500\n",
      "EnvName: CartPole-v1 seed: 1 steps: 26k score: 500\n",
      "EnvName: CartPole-v1 seed: 1 steps: 28k score: 500\n",
      "Environment solved in 28000 episodes!\tAverage Score: 500.00\n",
      "保存视频\n",
      "保存模型\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdwAAAFSCAYAAABYGW5dAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAAsTAAALEwEAmpwYAAATn0lEQVR4nO3dfWyX5f3o8U95dO0XClKhrWPqwk+2MUCHEYnzYUYNbk4ibk4Nm2aZm0qYbj5tumEkuoTM6XRkRmeyRQfzYR4xLjPOQ5QpOqfHUBTU4iMTOBwB+UEHBUqv88eO34ktD6J+uoOvV9LE7/e67/u6bJu+uW8vak0ppQQA8JHq1dMLAICPA8EFgASCCwAJBBcAEgguACQQXABIILgAkEBwISIeffTR+OQnP9nTywD2YoJLujlz5sRhhx0WlUolmpqa4qSTTorHH398j65VU1MTL7/8cvX1o48+Gr169YpKpRIDBgyIkSNHxm9/+9sPa+nv28qVK+OUU06J5ubmqKmpiddff323zjv22GNjn332iUqlEg0NDTF58uRYuXJlREScc8450a9fv6hUKrHvvvvGCSecEC+++GL13CVLlsQpp5wS9fX1MWDAgPjSl74UTzzxxG7NW1NTE3V1dVGpVKJSqcR3vvOd6lgpJS6//PIYMmRIDBkyJC6//PJ49+/NWbhwYYwbNy5qa2tj3LhxsXDhwh3Os3bt2jj11FOjrq4uDjjggJgzZ85ure8d3/72t7t87V944YU47rjjor6+PkaMGBH33XffdufcdtttMWLEiKhUKjFx4sRYsWLF+5oTPijBJdX1118fF110UVxxxRWxatWqWLZsWVxwwQVx//33v6/rdHR07HCsubk52traYv369TFz5sw499xzY8mSJR906XukV69eMXHixLj33nvf97mzZs2Ktra2aG1tjXXr1sUPfvCD6thll10WbW1t8eabb8bQoUPjnHPOiYiIV155JY488sgYPXp0vPbaa7FixYo49dRT48QTT4wnn3xyt+ZtaWmJtra2aGtri9tuu636/q233hpz586NlpaWWLRoUTzwwANxyy23RETEli1bYtKkSTFlypR4++234+yzz45JkybFli1bup1j6tSp0a9fv1i1alXMnj07zj///Fi8ePFure/xxx+PV155Zbv3Ojo6YtKkSXHyySfH2rVr49Zbb40pU6ZEa2trRPzrD2JXXHFF3H///bF27do46KCD4swzz9yt+eBDUyDJunXrSl1dXbn77ru7HX/qqafKEUccUerr60tjY2OZOnVq2bx5c3U8IsqsWbPKiBEjyoEHHliOOuqoEhGltra21NXVlTvvvLM88sgjZf/999/uug0NDeWee+4p7e3t5cILLyxNTU2lqampXHjhhaW9vb2UUrqct3z58jJ58uTS0NBQDjzwwHLjjTd2u+bly5eXffbZp6xZs6b63rPPPluGDBlStmzZUn1v69atJSLKa6+9tlufq2OOOab85je/qb6eNWtWGTVqVCmllLPPPrtceeWV1bE//elPpa6urpRSypQpU8pJJ53U5XrnnXdeOeqoo3Y5b0SUpUuXdjs2YcKEcsstt1Rf33bbbWX8+PGllFIeeuih0tzcXDo7O6vjw4cPLw8++GCX67S1tZW+ffuWl156qfrelClTyuWXX77L9W3durUccsghpaWlZbu1Pvfcc6Wurm67+U844YTyk5/8pJRSysUXX1wuuOCC6tjy5ctLRJSXX355l3PCh8UdLmmefPLJaG9vj1NPPbXb8d69e8cNN9wQq1evjieffDLmzZsXv/71r7c7Zu7cufHUU0/FkiVL4q9//WtE/PuO7Bvf+MZ2x3Z2dsZ9990X69ati9GjR8e1114bf/vb32LhwoXR0tISf//73+Oaa67pso7Ozs746le/GmPHjo3ly5fHvHnz4pe//GU89NBDXY5tbm6OCRMmbHcHO2fOnPja174Wffv2fd+fo+6sXr067r333jj00EO7jLW1tcXs2bOrYw8//HB8/etf73Lc6aefHgsWLIhNmzbtcr6jjz46GhsbY/Lkyds9Al+8eHGMHTu2+nrs2LHVu9LFixfHmDFjoqampjo+ZsyYbu9aW1tbo0+fPnHwwQd3e62dueGGG+Loo4+OMWPG7PLYUko8//zz271+7z+/exw+aoJLmjVr1kRDQ0P06dOn2/Fx48bFEUccEX369IkDDzwwvve978X8+fO3O+bHP/5x7LvvvvGJT3xih/OsWLEiBg0aFA0NDXH11VfHHXfcESNHjozZs2fH9OnTY+jQobHffvvFVVddFXfccUeX859++ul46623Yvr06dGvX7/49Kc/Heeee27ceeed3c531llnxR/+8IeI+NcP8jvvvDPOOuus3f207ND3v//9GDRoUIwdOzaampri+uuvr45dd911MWjQoBgxYkS0tbXF7373u4j4V5ybmpq6XKupqSk6Oztj7dq1O51z/vz58frrr8eLL74Yzc3NcfLJJ1cf37e1tUV9fX312Pr6+mhra4tSSpexd8Y3bNjQZY62trYYOHDgbh37bv/4xz/illtuiRkzZnQZGzlyZAwdOjR+/vOfx9atW+Mvf/lLzJ8/PzZu3BgRERMnToy77747Fi1aFJs2bYoZM2ZETU1NdRwydP+TDz4CQ4YMidWrV0dHR0e30W1tbY0f/vCH8cwzz8TGjRujo6Mjxo0bt90xw4cP3+U8zc3N8eabb3Z5f8WKFXHAAQdUXx9wwAHdbpx54403qtF+x7Zt2+Koo46KiIhKpVJ9f8mSJXHaaafFtGnTYuXKldHa2hq9evWqHvtB3HTTTdttWnq3Sy65pNu784aGhurmqndbuXJl9OrVKwYPHrzTOY8++uiIiOjXr1/ceOONMXDgwHjhhRdi9OjRUalUYv369dVj169fH5VKJWpqarqMvTM+YMCALnO8n2Pf7aKLLorp06d3CXtERN++fWPu3Lkxbdq0mDlzZhx22GFx+umnR//+/SMi4vjjj4+rr746TjvttFi/fn1cdNFFMWDAADvTSeUOlzQTJkyI/v37x9y5c7sdP//88+Mzn/lMLF26NNavXx8/+9nPtnsMGBHbPbJ8v5qbm+ONN96ovl62bFk0Nzd3OW748OFx0EEHxbp166ofGzZsiD//+c8REdUNRW1tbfGpT30qBg8eHCeeeGLcddddMWfOnDjjjDM+0Do/iOOPPz7uueeeLu/ffffdMWHChKitrX1f16upqal+DUaNGhUtLS3VsZaWlhg1alR1bNGiRdt9vRYtWlQdf7eDDz44Ojo6YunSpd1ea0fmzZsXl156aTQ2NkZjY2NE/Ot76p0dzmPGjIn58+fHmjVr4qGHHopXX301Dj/88Or5U6dOjaVLl8aqVavitNNOi46Ojvj85z//vj4f8EEILmnq6+tjxowZMXXq1Jg7d25s3Lgxtm7dGg8++GBcdtllsWHDhhg4cGBUKpV48cUX4+abb97lNYcNGxavvvrqbs1/5plnxjXXXBNvvfVWrF69OmbMmBFTpkzpctzhhx8eAwYMiJkzZ8amTZti27Zt8fzzz8fTTz+9w2ufddZZcfvtt8cf//jHLo+T29vbY/PmzRERsXnz5mhvb9+t9e6Jq666Kp544om48sorY+3atbFhw4b41a9+FbfffnvMnDlzp+cuXrw4Fi5cGNu2bYu2tra4+OKLY//994/PfvazERHxrW99K66//vpYvnx5rFixIn7xi19Ud0cfe+yx0bt377jpppti8+bNMWvWrIiIOO6447rMU1dXF5MnT47p06fHP//5z1iwYEHcf//98c1vfnOn62ttbY2WlpZYuHBh9a8cPfDAA9U9AYsWLYr29vbYuHFjXHfddbFy5crq+trb2+P555+PUkosW7Ysvvvd78aFF164yzt++FD12HYtPrZ+//vfl3HjxpXa2toybNiw8uUvf7ksWLCgzJ8/v4wcObLU1dWVL37xi+WnP/1pOfLII6vnRTc7aG+++ebS2NhY6uvry1133dXtLuV3bNq0qUybNq00NjaWxsbGMm3atLJp06ZSSve7lM8444wybNiwMmjQoDJ+/Pjy8MMP7/DfaePGjaVSqZTPfe5zXcYiosvHrrx3l/K7vXeX8ns999xz5Stf+UoZMGBAqaurK8ccc0x57LHHdjnnvHnzysEHH1xqa2vLfvvtVyZNmlRaW1ur452dneXSSy8tgwcPLoMHDy6XXnrpdruCn3322fKFL3yh7LPPPuXQQw8tzz77bHXs2muvLRMnTqy+XrNmTZk0aVKpra0tw4cPL7Nnz97l+t7rvd8Pl1xySRk0aFCpq6srEydO3G7s7bffLqNHj65+z/3oRz8qHR0d73tO+CBqSnnPMzsA4EPnkTIAJBBc6CHv/PrE93489thjH9mcs2fP7nbOXW1YynLeeed1u77zzjuvp5cGH5hHygCQwB0uACQQXABIILgAkEBwASCB4AJAAsEFgASCCwAJBBcAEgguACQQXABIILgAkEBwASCB4AJAAsEFgASCCwAJBBcAEgguACQQXABIILgAkEBwASCB4AJAAsEFgASCCwAJBBcAEgguACQQXABIILgAkEBwASCB4AJAAsEFgASCCwAJBBcAEgguACQQXABIILgAkEBwASCB4AJAAsEFgASCCwAJBBcAEgguACQQXABIILgAkEBwASCB4AJAAsEFgASCCwAJBBcAEgguACQQXABIILgAkEBwASCB4AJAAsEFgASCCwAJBBcAEgguACQQXABIILgAkEBwASCB4AJAAsEFgASCCwAJBBcAEgguACQQXABIILgAkEBwASCB4AJAAsEFgASCCwAJBBcAEgguACQQXABIILgAkEBwASCB4AJAAsEFgASCCwAJBBcAEgguACQQXABIILgAkEBwASCB4AJAAsEFgASCCwAJBBcAEgguACQQXABIILgAkEBwASCB4AJAAsEFgASCCwAJBBcAEgguACQQXABIILgAkEBwASCB4AJAAsEFgASCCwAJBBcAEgguACQQXABIILgAkEBwASCB4AJAAsEFgASCCwAJBBcAEgguACQQXABIILgAkEBwASCB4AJAAsEFgASCCwAJBBcAEgguACQQXABIILgAkEBwASCB4AJAAsEFgASCCwAJBBcAEgguACQQXABIILgAkEBwASCB4AJAgj49vQDgw7Nt6+Z48+//I6Ls+Jgh/zU+KsM+nbcoICIEF/YqZVtH/J/nH9npMbUNwwUXeoBHygCQQHABIIHgAkACwQWABIILAAkEFwASCC4AJBBcAEgguACQQHABIIHgAkACwQWABIILAAkEFwASCC4AJBBcAEgguACQQHABIIHgAkACwQWABIILAAkEFwASCC4AJBBcAEgguACQQHABIIHgAkACwQWABIILAAkEFwASCC4AJBBcAEgguACQQHABIIHgAkACwQWABIILAAkEFwASCC4AJBBcAEgguACQQHABIIHgAkACwQWABIILAAkEFwASCC4AJBBcAEgguACQQHABIIHgAkACwQWABIILAAkEFwASCC4AJBBcAEgguACQQHABIIHgAkACwQWABIILAAkEFwASCC4AJBBcAEgguACQQHABIIHgAkACwQWABIILAAkEFwASCC4AJBBcAEgguACQQHABIIHgAkACwQWABIILAAkEFwASCC4AJBBcAEgguACQQHABIIHgAkACwQWABDWllNLTiwD+7aWXXooXXnhhj86t6dwSTW/9z6jZyTFvDxwdGz8xfI+uP3DgwDjuuOP26Fz4uOvT0wsAtnfvvffGlVdeuUfnDqztHw//4psRNTtO7qxZs+KBJ1r36Ppjx46NhQsX7tG58HEnuLCX6iw1Uf7ffzWqiRI10bmzDgMfMcGFvdDGbZV4ddPY+N+bD4rO6BP1fd6K/6p9Jvbtu6qnlwYfW4ILe5mN2wbGc23HxtsdTdX31m7dP/7X+oYYU3mkB1cGH292KcNepKP0jYUbjt8utv8e6x+L/3lUrN7yyR5YGSC4sBcp0Sv+u6Nhh+ObO+uivbMucUXAOwQXABIILgAkEFzYi/Sp2RxjBzwSvaKjm9HO2L9/azT1fzV9XYDgwl6lJiKa+y+NUZXHo67321ET2yKiM/rVbIz9+y+N0ZVHo3fN1p5eJnws+WtBsBdp39IRN977VNTEU7GuY2j8d0dDlNIrPtF7Q+zX7x/xSHTGktff6ullwsfSTn+X8pFHHpm5FiAi3nzzzVi2bFlPL6NbtbW1ccghh/T0MuA/1oIFC3Y4ttPgbtmy5SNZELBjM2fOjOnTp/f0Mro1evToeOaZZ3p6GfAfq1+/fjsc2+kj5Z2dCHw0evfu3dNL2KFevXr5uQB7yKYpAEgguACQQHABIIHgAkACwQWABIILAAkEFwASCC4AJBBcAEjgf14A/2HGjx8fF198cU8vo1tNTU09vQT4/9ZOf5cyAPDh8EgZABIILgAkEFwASCC4AJBAcAEggeACQALBBYAEggsACQQXABIILgAkEFwASCC4AJBAcAEggeACQALBBYAEggsACQQXABIILgAkEFwASCC4AJBAcAEggeACQALBBYAEggsACQQXABIILgAkEFwASCC4AJBAcAEggeACQALBBYAEggsACQQXABIILgAkEFwASCC4AJBAcAEggeACQALBBYAEggsACQQXABIILgAkEFwASCC4AJBAcAEggeACQALBBYAEggsACQQXABIILgAkEFwASCC4AJBAcAEggeACQALBBYAEggsACQQXABIILgAkEFwASCC4AJBAcAEggeACQALBBYAEggsACQQXABIILgAkEFwASCC4AJBAcAEggeACQALBBYAE/xd2kKHA+Co6aQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL_3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
