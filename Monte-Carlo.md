# 蒙地卡罗算法

蒙地卡罗算法:

1. 我们把智能体放到环境的任意状态；
2. 从这个状态开始按照策略进行选择动作，并进入新的状态。
3. 重复步骤2，直到最终状态；
4. 我们从最终状态开始向前回溯：计算每个状态的G值。
5. 重复1-4多次，然后平均每个状态的G值，这就是我们需要求的V值。

## 原理

我们分成两部分，首先我们要理解每一次我们算的G值的意义。

<div align=center>
<img width="800" src=" https://pic3.zhimg.com/80/v2-4efd9aad81d7da755ebcff68166ea426_720w.webp "/>
</div>
<div align=center></div>

- 第一步，我们根据策略往前走，一直走到最后，期间我们什么都不用算，还需要记录每一个状态转移，我们获得多少奖励r即可。
- 第二步，我们从终点往前走，一遍走一遍计算G值。G值等于上一个状态的G值(记作G'),乘以一定的折扣(gamma),再加上r。

所以G值的意义在于，在这一次游戏中，**某个状态到最终状态的奖励总和**(理解时可以忽略折扣值)

<div align=center>
<img width="800" src=" https://pic2.zhimg.com/80/v2-7cb408de872e15fee07f3779d3172e89_720w.webp "/>
</div>
<div align=center></div>

当我们进行多次试验后，我们有可能会经过某个状态多次，通过回溯，也会有多个G值。 重复我们刚才说的，每一个G值，就是每次到最终状态获得的奖励总和。而V值时候某个状态下，我们通过影分身到达最终状态，所有影分身获得的奖励的平均值。

<div align=center>
<img width="800" src=" https://pic4.zhimg.com/80/v2-3ddd1531a5e5882f5468c08419c5caff_720w.webp "/>
</div>
<div align=center></div>

## 再进一步

1. G的意义：在某个路径上，状态S到最终状态的总收获。
2. V和G的关系：V是G的平均数。

到这里要注意一点：**V和策略是相关的**，那么在这里怎么体现呢？这个非常重要，因为在PPO算法中，离线策略就与这个有关。这里可以稍微先说一下。

我们仍以上图为例子，以策略A进行游戏。其中有100次经过S点，经过S点后有4条路径到达最终状态，计算G值和每条路径次数分别如下：

<div align=center>
<img width="800" src=" https://pic3.zhimg.com/80/v2-30117fed96d35b772925ad4b96570dca_720w.webp "/>
</div>
<div align=center></div>

策略A采用平均策略，这时候 $V = 5$。

现在我们采用策略B，**由于策略改变，经过某条路径的概率就会产生变化。因此最终试验经过的次数就不一样了**。

<div align=center>
<img width="800" src=" https://pic4.zhimg.com/80/v2-94e1db2b7340604e186e0781d90871cf_720w.webp "/>
</div>
<div align=center></div>

最终计算的 $V = 7.55$。

## 蒙地卡罗的缺陷

在实际引用中，蒙地卡罗虽然比动态规划消耗要少一点；而且并不需要知道整个环境模型。

但蒙地卡罗有一个比较大的缺点，就是每一次游戏，都需要先从头走到尾，再进行回溯更新。如果最终状态很难达到，那小猴子可能每一次都要转很久很久才能更新一次G值。
