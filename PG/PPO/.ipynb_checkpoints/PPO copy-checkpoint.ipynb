{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d720ff59-7339-41c3-ba51-c13c933cac94",
   "metadata": {},
   "source": [
    "# 代码\n",
    "## 超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34048760-32db-4d27-93b3-a1efc7efd43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import evaluate_policy, str2bool, test_policy, GaussianActor_musigma, GaussianActor_mu\n",
    "from datetime import datetime\n",
    "import gymnasium as gym\n",
    "import os, shutil\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "from collections import deque\n",
    "\n",
    "from torch.distributions import Beta,Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f72687b-683c-48a1-886f-40e687b12688",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    # 创建命令行参数解析器\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # 添加各种命令行参数\n",
    "    parser.add_argument('--algo_name',default='PPO',type=str,help=\"算法名\")\n",
    "    parser.add_argument('--dvc', type=str, default='cuda', help='运行设备: cuda 或 cpu')\n",
    "    parser.add_argument('--env_name', type=str, default='LunarLanderContinuous-v2', help='环境名')\n",
    "    parser.add_argument('--render_mode', type=str, default='rgb_array', help='环境渲染模式')\n",
    "    parser.add_argument('--write', type=str2bool, default=True, help='使用SummaryWriter记录训练')\n",
    "    parser.add_argument('--render', type=str2bool, default=False, help='是否渲染')\n",
    "    parser.add_argument('--Loadmodel', type=str2bool, default=False, help='是否加载预训练模型')\n",
    "    parser.add_argument('--ModelIdex', type=int, default=100000, help='要加载的模型索引')\n",
    "    parser.add_argument('--deque_maxlen',default=10,type=int)\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=1, help='随机种子')\n",
    "    parser.add_argument('--T_horizon', type=int, default=500, help='长轨迹的长度')\n",
    "    parser.add_argument('--Max_train_steps', type=int, default=5e7, help='最大训练步数')\n",
    "    parser.add_argument('--save_interval', type=int, default=1e5, help='模型保存间隔，以步为单位')\n",
    "    parser.add_argument('--eval_interval', type=int, default=2e3, help='模型评估间隔，以步为单位')\n",
    "    parser.add_argument('--test_interval', type=int, default=1e5, help='视频保存间隔，以步为单位')\n",
    "\n",
    "    parser.add_argument('--gamma', type=float, default=0.99, help='折扣因子')\n",
    "    parser.add_argument('--lambd', type=float, default=0.95, help='GAE因子')\n",
    "    parser.add_argument('--clip_rate', type=float, default=0.2, help='PPO剪切率')\n",
    "    parser.add_argument('--K_epochs', type=int, default=10, help='PPO更新次数')\n",
    "    parser.add_argument('--net_width', type=int, default=128, help='隐藏网络宽度')\n",
    "    parser.add_argument('--a_lr', type=float, default=2e-4, help='Learning rate of actor')\n",
    "    parser.add_argument('--c_lr', type=float, default=2e-4, help='Learning rate of critic')\n",
    "    parser.add_argument('--l2_reg', type=float, default=1e-3, help='Critic的L2正则化系数')\n",
    "    parser.add_argument('--a_optim_batch_size', type=int, default=64, help='lenth of sliced trajectory of actor')\n",
    "    parser.add_argument('--c_optim_batch_size', type=int, default=64, help='lenth of sliced trajectory of critic')\n",
    "    parser.add_argument('--batch_size', type=int, default=64, help='切片轨迹的长度')\n",
    "    parser.add_argument('--entropy_coef', type=float, default=0, help='Actor的熵系数')\n",
    "    parser.add_argument('--entropy_coef_decay', type=float, default=0.99, help='熵系数的衰减率')\n",
    "    parser.add_argument('--adv_normalization', type=str2bool, default=False, help='优势值正则化')\n",
    "\n",
    "    parser.add_argument('--Distribution', type=str, default='Beta', help='Should be one of Beta ; GS_ms  ;  GS_m')\n",
    "    \n",
    "    # 解析命令行参数\n",
    "    args = parser.parse_args([])\n",
    "    args = {**vars(args)}  # 转换成字典类型    \n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aeb6494-e5a7-401e-8bbb-c8400d5ac704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_args(args):\n",
    "    ## 打印超参数\n",
    "    print(\"超参数\")\n",
    "    print(''.join(['=']*80))\n",
    "    tplt = \"{:^20}\\t{:^20}\\t{:^20}\"\n",
    "    print(tplt.format(\"Name\", \"Value\", \"Type\"))\n",
    "    for k,v in args.items():\n",
    "        print(tplt.format(k,v,str(type(v))))   \n",
    "    print(''.join(['=']*80))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dfe23e-fa8a-4062-99c7-2fd2e0a25148",
   "metadata": {},
   "source": [
    "## 网络搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "024c50b1-e627-4dc7-969f-702c12f90ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaActor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, net_width):\n",
    "        \"\"\"\n",
    "        Beta分布策略网络，输出alpha和beta参数\n",
    "\n",
    "        参数:\n",
    "        - state_dim: 状态空间维度\n",
    "        - action_dim: 动作空间维度\n",
    "        - net_width: 神经网络宽度\n",
    "\n",
    "        说明:\n",
    "        1. 初始化网络结构，包括三个全连接层。\n",
    "        2. alpha_head 用于输出alpha参数。\n",
    "        3. beta_head 用于输出beta参数。\n",
    "        \"\"\"\n",
    "        super(BetaActor, self).__init__()\n",
    "\n",
    "        self.l1 = nn.Linear(state_dim, net_width)  # 1. 第一个全连接层\n",
    "        self.l2 = nn.Linear(net_width, net_width)   # 1. 第二个全连接层\n",
    "        self.alpha_head = nn.Linear(net_width, action_dim)  # 2. 输出alpha参数的全连接层\n",
    "        self.beta_head = nn.Linear(net_width, action_dim)  # 3. 输出beta参数的全连接层\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        正向传播\n",
    "\n",
    "        参数:\n",
    "        - state: 输入的状态\n",
    "\n",
    "        返回:\n",
    "        - alpha: Beta分布的alpha参数\n",
    "        - beta: Beta分布的beta参数\n",
    "\n",
    "        说明:\n",
    "        1. 通过两个双曲正切激活函数传播信号。\n",
    "        2. 使用软正切激活函数计算并输出alpha和beta参数，确保参数为正数。\n",
    "        \"\"\"\n",
    "        a = torch.tanh(self.l1(state))  # 1. 第一个双曲正切激活函数\n",
    "        a = torch.tanh(self.l2(a))  # 1. 第二个双曲正切激活函数\n",
    "\n",
    "        # 使用软正切激活函数得到 alpha 和 beta 参数，确保为正数\n",
    "        alpha = F.softplus(self.alpha_head(a)) + 1.0  # 2. 计算alpha参数\n",
    "        beta = F.softplus(self.beta_head(a)) + 1.0  # 2. 计算beta参数\n",
    "\n",
    "        return alpha, beta\n",
    "\n",
    "    def get_dist(self, state):\n",
    "        \"\"\"\n",
    "        获取动作分布\n",
    "\n",
    "        参数:\n",
    "        - state: 输入的状态\n",
    "\n",
    "        返回:\n",
    "        - dist: Beta分布\n",
    "\n",
    "        说明:\n",
    "        获取Beta分布，由alpha和beta参数构建。\n",
    "        \"\"\"\n",
    "        alpha, beta = self.forward(state)  # 获取alpha和beta参数\n",
    "        dist = Beta(alpha, beta)  # 构建Beta分布\n",
    "        return dist\n",
    "\n",
    "    def deterministic_act(self, state):\n",
    "        \"\"\"\n",
    "        根据状态进行确定性动作选择\n",
    "\n",
    "        参数:\n",
    "        - state: 输入的状态\n",
    "\n",
    "        返回:\n",
    "        - mode: Beta分布的众数\n",
    "\n",
    "        说明:\n",
    "        直接返回Beta分布的众数，用于确定性动作选择。\n",
    "        \"\"\"\n",
    "        alpha, beta = self.forward(state)  # 获取alpha和beta参数\n",
    "        mode = (alpha) / (alpha + beta)  # 计算众数\n",
    "        return mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "553e9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weight(m):\n",
    "    \"\"\"\n",
    "    初始化神经网络模型权重的函数\n",
    "\n",
    "    参数:\n",
    "    - m: 神经网络模型的模块\n",
    "\n",
    "    说明:\n",
    "    1. 判断模块是否为线性层（nn.Linear）。\n",
    "    2. 如果是线性层，使用 Xavier 正态分布初始化权重，偏置初始化为零。\n",
    "    \"\"\"\n",
    "    if isinstance(m, nn.Linear):  # 判断模块是否为线性层\n",
    "        nn.init.xavier_normal_(m.weight)  # 使用 Xavier 正态分布初始化权重\n",
    "        nn.init.constant_(m.bias, 0.0)  # 偏置初始化为零"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb900a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor_continuous(nn.Module):\n",
    "    \"\"\"\n",
    "    连续动作空间的策略网络模型\n",
    "\n",
    "    参数:\n",
    "    - state_dim: 状态空间维度\n",
    "    - hidden_dim: 隐藏层维度\n",
    "    - action_dim: 动作空间维度\n",
    "\n",
    "    属性:\n",
    "    - fc1: 全连接层，将状态映射到隐藏层\n",
    "    - fc_mu: 全连接层，映射隐藏层到动作均值\n",
    "    - fc_std: 全连接层，映射隐藏层到动作标准差\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(Actor_continuous, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)  # 输入层到隐藏层的全连接层\n",
    "        self.fc_mu = torch.nn.Linear(hidden_dim, action_dim)  # 隐藏层到动作均值的全连接层\n",
    "        self.fc_std = torch.nn.Linear(hidden_dim, action_dim)  # 隐藏层到动作标准差的全连接层\n",
    "\n",
    "        self.apply(init_weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播函数\n",
    "\n",
    "        参数:\n",
    "        - x: 输入状态\n",
    "\n",
    "        返回:\n",
    "        - mu: 动作均值\n",
    "        - std: 动作标准差\n",
    "        \"\"\"\n",
    "        x = F.relu(self.fc1(x))  # 使用 ReLU 激活函数处理输入状态到隐藏层\n",
    "        mu = 2.0 * torch.tanh(self.fc_mu(x))  # 计算动作均值并使用双曲正切函数缩放到[-2, 2]\n",
    "        std = F.softplus(self.fc_std(x))  # 计算动作标准差并使用软正切函数确保非负\n",
    "        return mu, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a71b9b9-e57b-404c-a59e-b2e1f38201af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, net_width):\n",
    "        \"\"\"\n",
    "        Critic 神经网络模型\n",
    "\n",
    "        参数:\n",
    "        - state_dim: 输入状态的维度\n",
    "        - net_width: 神经网络的宽度\n",
    "\n",
    "        说明:\n",
    "        定义三个全连接层，分别是输入层、隐藏层1和隐藏层2。\n",
    "        输出层是一个单一节点，用于估计状态的值。\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        # 定义神经网络的三个全连接层\n",
    "        self.C1 = nn.Linear(state_dim, net_width)  # 输入层\n",
    "        self.C2 = nn.Linear(net_width, net_width)  # 隐藏层1\n",
    "        self.C3 = nn.Linear(net_width, 1)  # 隐藏层2，输出状态值\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "\n",
    "        参数:\n",
    "        - state: 输入状态\n",
    "\n",
    "        返回:\n",
    "        - v: 估计的状态值\n",
    "\n",
    "        说明:\n",
    "        使用 ReLU 激活函数进行前向传播。\n",
    "        \"\"\"\n",
    "        v = torch.relu(self.C1(state))\n",
    "        v = torch.relu(self.C2(v))\n",
    "        v = self.C3(v)\n",
    "        return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda24d7a-2f2b-41b1-a7b8-27fae8cd6825",
   "metadata": {},
   "source": [
    "## 环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6b705f5-c58f-4c0b-abfa-f69be23cdcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_seed(env, seed):\n",
    "    \"\"\"\n",
    "    设置随机种子以确保实验的可重复性\n",
    "\n",
    "    参数:\n",
    "    - env: Gym 环境，用于训练模型\n",
    "    - seed: 随机种子值\n",
    "\n",
    "    说明:\n",
    "    1. 使用给定的随机种子设置 NumPy、Python、PyTorch 和 CUDA 的随机生成器。\n",
    "    2. 禁用 CUDA 的非确定性操作以确保实验结果的一致性。\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)  # 设置 NumPy 随机种子\n",
    "    random.seed(seed)  # 设置 Python 随机种子\n",
    "    torch.manual_seed(seed)  # 设置 PyTorch 随机种子\n",
    "    torch.cuda.manual_seed(seed)  # 设置 PyTorch CUDA 随机种子\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # 设置 Python Hash 随机种子\n",
    "    torch.backends.cudnn.deterministic = True  # 禁用 CUDA 非确定性操作以确保实验结果的一致性\n",
    "    torch.backends.cudnn.benchmark = False  # 禁用 CUDA 非确定性操作以确保实验结果的一致性\n",
    "    torch.backends.cudnn.enabled = False  # 禁用 CUDA 非确定性操作以确保实验结果的一致性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a42d2f23-24e5-4553-9f94-9c6a14d89807",
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_agent_config(cfg, path):\n",
    "    \"\"\"\n",
    "    配置环境和代理\n",
    "\n",
    "    参数:\n",
    "    - cfg: 包含配置信息的字典\n",
    "    - path: 模型保存路径\n",
    "\n",
    "    返回:\n",
    "    - env: Gym 环境\n",
    "    - agent: PPO 代理\n",
    "\n",
    "    说明:\n",
    "    1. 创建指定环境并设置渲染模式。\n",
    "    2. 如果配置中设置了种子，则为环境设置种子。\n",
    "    3. 获取环境的状态空间维度和动作空间维度。\n",
    "    4. 更新配置字典中的状态维度和动作维度。\n",
    "    5. 创建 PPO 代理。\n",
    "\n",
    "    注意:\n",
    "    - PPO 代理的创建依赖于配置信息和模型保存路径。\n",
    "    \"\"\"\n",
    "    env = gym.make(cfg['env_name'], render_mode=cfg['render_mode'])  # 1. 创建环境\n",
    "    eval_env = gym.make(cfg['env_name'], render_mode=cfg['render_mode'])\n",
    "    if cfg['seed'] != 0:\n",
    "        all_seed(env, seed=cfg['seed'])  # 2. 如果配置中设置了种子，则为环境设置种子\n",
    "\n",
    "    n_states = env.observation_space.shape[0]  # 3. 获取状态空间维度\n",
    "    n_actions = env.action_space.shape[0]  # 获取动作空间维度\n",
    "    max_action = float(env.action_space.high[0]) # 获取动作空间的最大值\n",
    "    max_e_steps = env._max_episode_steps  # 最大步数\n",
    "    print(f\"状态空间维度：{n_states}，动作空间维度：{n_actions}，最大步数：{max_e_steps}\")\n",
    "    cfg.update({\"state_dim\": n_states, \"action_dim\": n_actions, \"max_e_steps\": max_e_steps, \"max_action\": max_action})  # 4. 更新n_states和n_actions到cfg参数中\n",
    "\n",
    "    agent = PPO_continuous(cfg)  # 5. 创建 PPO 代理\n",
    "    return env, eval_env, agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d3273-e133-4c39-b3e7-24eb6cdd8758",
   "metadata": {},
   "source": [
    "## 智能体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05ac69b9-fdd0-42ef-b758-c77f8e79143f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PPO_continuous():\n",
    "    def __init__(self, kwargs):\n",
    "        \"\"\"\n",
    "        初始化PPO代理的超参数和模型\n",
    "\n",
    "        参数:\n",
    "        - kwargs: 包含所有超参数的字典\n",
    "\n",
    "        说明:\n",
    "        1. 通过字典更新self的属性，将超参数初始化为代理的属性。\n",
    "        2. 创建Actor和Critic模型，并初始化优化器。\n",
    "        3. 构建用于存储轨迹数据的缓存。\n",
    "        \"\"\"\n",
    "        # 使用字典中的键值对更新self的属性，将超参数初始化为代理的属性\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        # 选择 actor 分布\n",
    "        if self.Distribution == 'Beta':\n",
    "            self.actor = BetaActor(self.state_dim, self.action_dim, self.net_width).to(self.dvc)\n",
    "        elif self.Distribution == 'GS_ms':\n",
    "            self.actor = GaussianActor_musigma(self.state_dim, self.action_dim, self.net_width).to(self.dvc)\n",
    "        elif self.Distribution == 'GS_m':\n",
    "            self.actor = GaussianActor_mu(self.state_dim, self.action_dim, self.net_width).to(self.dvc)\n",
    "        else:\n",
    "            print('Dist Error')  # 分布选择错误的情况下输出错误信息\n",
    "        # 设置 actor 优化器\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.a_lr)\n",
    "\n",
    "        # 创建Critic模型，接收状态维度和网络宽度作为输入\n",
    "        self.critic = Critic(self.state_dim, self.net_width).to(self.dvc)\n",
    "        # 创建Critic模型的优化器，使用Adam优化器，学习率为self.lr\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=self.c_lr)\n",
    "        \n",
    "        self.s_hoder = np.zeros((self.T_horizon, self.state_dim),dtype=np.float32)\n",
    "        self.a_hoder = np.zeros((self.T_horizon, self.action_dim),dtype=np.float32)\n",
    "        self.r_hoder = np.zeros((self.T_horizon, 1),dtype=np.float32)\n",
    "        self.s_next_hoder = np.zeros((self.T_horizon, self.state_dim),dtype=np.float32)\n",
    "        self.logprob_a_hoder = np.zeros((self.T_horizon, self.action_dim),dtype=np.float32)\n",
    "        self.done_hoder = np.zeros((self.T_horizon, 1),dtype=np.bool_)\n",
    "        self.dw_hoder = np.zeros((self.T_horizon, 1),dtype=np.bool_)\n",
    "        \n",
    "    def select_action(self, state, deterministic):\n",
    "        \"\"\"\n",
    "        选择动作\n",
    "\n",
    "        参数:\n",
    "        - state: 输入的状态\n",
    "        - deterministic: 是否选择确定性动作\n",
    "\n",
    "        返回:\n",
    "        - 动作值\n",
    "        - 对数概率值（仅在非确定性时返回）\n",
    "\n",
    "        说明:\n",
    "        1. 使用传入的状态创建张量。\n",
    "        2. 如果是确定性动作，使用Actor的确定性动作方法获取动作。\n",
    "        3. 如果是非确定性动作，使用Actor的获取分布方法得到动作样本，并截断在[0, 1]范围内。\n",
    "        4. 计算动作的对数概率并返回结果。\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state.reshape(1, -1)).to(self.dvc)\n",
    "            if deterministic:\n",
    "                # 仅在评估策略时使用，使性能更加稳定\n",
    "                a = self.actor.deterministic_act(state)\n",
    "                return a.cpu().numpy()[0], None  # 动作的形状为(adim, 0)\n",
    "            else:\n",
    "                # 仅在与环境交互时使用\n",
    "                dist = self.actor.get_dist(state)\n",
    "                a = dist.sample()\n",
    "                a = torch.clamp(a, 0, 1)\n",
    "                logprob_a = dist.log_prob(a).cpu().numpy().flatten()\n",
    "                return a.cpu().numpy()[0], logprob_a  # 动作和对数概率的形状都为(adim, 0)\n",
    "\n",
    "    def train(self):\n",
    "        # 更新熵系数，乘以熵系数衰减率\n",
    "        self.entropy_coef *= self.entropy_coef_decay\n",
    "\n",
    "        # 将存储的环境状态、动作、奖励、下一个状态、动作的对数概率、完成标志以及权重转换为PyTorch张量，并发送到指定设备\n",
    "        s = torch.from_numpy(self.s_hoder).to(self.dvc)  # 当前状态\n",
    "        a = torch.from_numpy(self.a_hoder).to(self.dvc)  # 动作\n",
    "        r = torch.from_numpy(self.r_hoder).to(self.dvc)  # 奖励\n",
    "        s_next = torch.from_numpy(self.s_next_hoder).to(self.dvc)  # 下一个状态\n",
    "        logprob_a = torch.from_numpy(self.logprob_a_hoder).to(self.dvc)  # 动作的对数概率\n",
    "        done = torch.from_numpy(self.done_hoder).to(self.dvc)  # 完成标志\n",
    "        dw = torch.from_numpy(self.dw_hoder).to(self.dvc)  # 权重\n",
    "\n",
    "        # 使用torch.no_grad()上下文，禁'path'用梯度计算以提高计算效率\n",
    "        with torch.no_grad():\n",
    "            # 计算当前状态的值函数 vs 和下一个状态的值函数 vs_\n",
    "            vs = self.critic(s)\n",
    "            vs_ = self.critic(s_next)\n",
    "\n",
    "            # 计算时序差分误差（deltas），考虑奖励、折扣因子、下一个状态值函数、权重和完成标志\n",
    "            deltas = r + self.gamma * vs_ * (~dw) - vs\n",
    "            # 将时序差分误差转换为NumPy数组\n",
    "            deltas = deltas.cpu().flatten().numpy()\n",
    "            # 初始化优势列表，初始值为0\n",
    "            adv = [0]\n",
    "\n",
    "            # 通过反向迭代计算优势（advantage）\n",
    "            for dlt, done in zip(deltas[::-1], done.cpu().flatten().numpy()[::-1]):\n",
    "                # 计算当前时间步的优势，考虑时序差0.002分误差、折扣因子、权重和完成标志\n",
    "                advantage = dlt + self.gamma * self.lambd * adv[-1] * (~done)\n",
    "                # 将计算得到的优势添加到优势列表\n",
    "                adv.append(advantage)\n",
    "            # 反转优势列表，以保持正确的时间顺序\n",
    "            adv.reverse()\n",
    "\n",
    "            # 剔除优势列表的最后一个元素，以去除冗余添加的初始值\n",
    "            adv = copy.deepcopy(adv[0:-1])\n",
    "            # 将优势列表转换为PyTorch张量，并进行形状调整\n",
    "            adv = torch.tensor(adv).unsqueeze(1).float().to(self.dvc)\n",
    "\n",
    "            # 计算时序差分目标（td_target）\n",
    "            td_target = adv + vs\n",
    "\n",
    "            # 对优势函数进行标准化处理\n",
    "            adv = (adv - adv.mean()) / ((adv.std() + 1e-4))\n",
    "\n",
    "        # 将长轨迹切分为短轨迹，并执行小批量PPO更新\n",
    "        a_optim_iter_num = int(math.ceil(s.shape[0] / self.a_optim_batch_size))\n",
    "        c_optim_iter_num = int(math.ceil(s.shape[0] / self.c_optim_batch_size))\n",
    "\n",
    "        # 迭代执行多个优化步骤（K_epochs次）\n",
    "        for _ in range(self.K_epochs):\n",
    "            # 随机打乱样本索引的顺序\n",
    "            perm = np.arange(s.shape[0])\n",
    "            np.random.shuffle(perm)\n",
    "            # 将打乱后的索引转换为PyTorch张量并发送到指定设备\n",
    "            perm = torch.LongTensor(perm).to(self.dvc)\n",
    "\n",
    "            # 根据打乱后的索引重新排列数据，使用深度复制以避免原始数据被修改\n",
    "            s, a, td_target, adv, logprob_a = \\\n",
    "                s[perm].clone(), a[perm].clone(), td_target[perm].clone(), adv[perm].clone(), logprob_a[perm].clone()\n",
    "\n",
    "            # 遍历每个优化迭代\n",
    "            for i in range(a_optim_iter_num):\n",
    "                # 确定当前迭代的样本索引范围\n",
    "                index = slice(i * self.a_optim_batch_size, min((i + 1) * self.a_optim_batch_size, s.shape[0]))\n",
    "\n",
    "                # 获取当前优化批次的动作概率分布\n",
    "                distribution = self.actor.get_dist(s[index])\n",
    "                # 计算动作概率分布的熵\n",
    "                dist_entropy = distribution.entropy().sum(1, keepdim=True)\n",
    "                # 计算当前动作的对数概率\n",
    "                logprob_a_now = distribution.log_prob(a[index])\n",
    "\n",
    "                # 计算新概率与旧概率的比率\n",
    "                ratio = torch.exp(logprob_a_now.sum(1, keepdim=True) - logprob_a[index].sum(1, keepdim=True))\n",
    "                # 计算Clipped Surrogate函数的第一部分\n",
    "                surr1 = ratio * adv[index]\n",
    "                # 计算Clipped Surrogate函数的第二部分，使用torch.clamp函数将比率限制在[1 - clip_rate, 1 + clip_rate]范围内\n",
    "                surr2 = torch.clamp(ratio, 1 - self.clip_rate, 1 + self.clip_rate) * adv[index]\n",
    "                # 计算Actor的损失，包括负的Clipped Surrogate函数最小值和熵正则项\n",
    "                a_loss = -torch.min(surr1, surr2) - self.entropy_coef * dist_entropy\n",
    "\n",
    "                # 对Actor进行梯度清零，计算梯度，进行梯度裁剪，更新参数\n",
    "                # 将Actor的梯度置零，以防止梯度累积\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                # 计算Actor的损失对参数的梯度\n",
    "                a_loss.mean().backward()\n",
    "                # 使用梯度裁剪，防止梯度爆炸\n",
    "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 40)\n",
    "                # 更新Actor的参数，执行一步优化器\n",
    "                self.actor_optimizer.step()\n",
    "                \n",
    "            for i in range(c_optim_iter_num):\n",
    "                # 获取当前优化批次的索引范围\n",
    "                index = slice(i * self.c_optim_batch_size, min((i + 1) * self.c_optim_batch_size, s.shape[0]))\n",
    "                # 计算当前状态的值函数估计值\n",
    "                c_loss = (self.critic(s[index]) - td_target[index]).pow(2).mean()\n",
    "                # 添加 L2 正则化\n",
    "                for name,param in self.critic.named_parameters():\n",
    "                    if 'weight' in name:\n",
    "                        c_loss += param.pow(2).sum() * self.l2_reg\n",
    "\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                c_loss.backward()\n",
    "                self.critic_optimizer.step()\n",
    "                \n",
    "        return a_loss, c_loss\n",
    "\n",
    "    def put_data(self, s, a, r, s_next, logprob_a, done, dw, idx):\n",
    "        \"\"\"\n",
    "        存储经验数据到对应的缓存中\n",
    "\n",
    "        参数:\n",
    "        - s: 当前状态\n",
    "        - a: 采取的动作\n",
    "        - r: 获得的奖励\n",
    "        - s_next: 下一个状态\n",
    "        - logprob_a: 采取动作的对数概率\n",
    "        - done: 是否完成当前 episode\n",
    "        - dw: 重要性采样权重\n",
    "        - idx: 存储的索引位置\n",
    "        \"\"\"\n",
    "        self.s_hoder[idx] = s  # 存储当前状态到状态缓存中\n",
    "        self.a_hoder[idx] = a  # 存储采取的动作到动作缓存中\n",
    "        self.r_hoder[idx] = r  # 存储获得的奖励到奖励缓存中\n",
    "        self.s_next_hoder[idx] = s_next  # 存储下一个状态到下一个状态缓存中\n",
    "        self.logprob_a_hoder[idx] = logprob_a  # 存储采取动作的对数概率到对数概率缓存中\n",
    "        self.done_hoder[idx] = done  # 存储是否完成当前 episode 到完成标志缓存中\n",
    "        self.dw_hoder[idx] = dw  # 存储重要性采样权重到权重缓存中\n",
    "\n",
    "    def save(self, episode):\n",
    "        \"\"\"\n",
    "        保存当前训练模型的Actor和Critic参数到文件\n",
    "\n",
    "        参数:\n",
    "        - episode: 当前训练的episode数，用于在文件名中标识不同的保存点\n",
    "        \"\"\"\n",
    "        model_path = f\"model/{cfg['path']}\"\n",
    "        # 检查是否存在'model'文件夹，如果不存在则创建\n",
    "        try:\n",
    "            os.makedirs(model_path)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        # 保存Critic的参数到文件\n",
    "        torch.save(self.critic.state_dict(), f\"{model_path}/ppo_critic{episode}.pth\")\n",
    "        # 保存Actor的参数到文件\n",
    "        torch.save(self.actor.state_dict(), f\"{model_path}/ppo_actor{episode}.pth\")\n",
    "\n",
    "    def load(self, episode):\n",
    "        \"\"\"\n",
    "        从文件加载之前保存的Actor和Critic参数\n",
    "\n",
    "        参数:\n",
    "        - episode: 要加载的保存点的episode数\n",
    "        \"\"\"\n",
    "        # 加载之前保存的Critic的参数\n",
    "        self.critic.load_state_dict(torch.load(\"./model/ppo_critic{}.pth\".format(episode)))\n",
    "        # 加载之前保存的Actor的参数\n",
    "        self.actor.load_state_dict(torch.load(\"./model/ppo_actor{}.pth\".format(episode)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230eebb2-f0cb-46ec-b5f4-26138544bbfe",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5e0026d-bfea-472c-81f3-f7a19c92219c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg):\n",
    "    print(\"开始训练\")\n",
    "    env_seed = cfg['seed']\n",
    "    # 使用TensorBoard记录训练曲线\n",
    "    if cfg['write']:\n",
    "        writepath = 'runs/{}'.format(cfg['path']) # 构建TensorBoard日志路径\n",
    "        if os.path.exists(writepath): shutil.rmtree(writepath)  # 如果路径已存在，则删除该路径及其内容\n",
    "        writer = SummaryWriter(log_dir=writepath)  # 创建TensorBoard写入器，指定日志路径\n",
    "\n",
    "    # 如果指定了加载模型的选项，则加载模型\n",
    "    if cfg['Loadmodel']:\n",
    "        print(\"加载模型\")\n",
    "        agent.load(cfg['ModelIdex'])\n",
    "\n",
    "    # 如果选择渲染模式\n",
    "    if cfg['render']:\n",
    "        while True:\n",
    "            # 在环境中评估智能体的性能，并输出奖励\n",
    "            ep_r = evaluate_policy(env, agent, turns=1)\n",
    "            print('Env: ', cfg['env_name'],' Episode Reward: ', {ep_r})\n",
    "    else:\n",
    "        traj_length, total_steps, test_steps = 0, 0, 0\n",
    "        scores_deque = deque(maxlen=cfg['deque_maxlen'])\n",
    "\n",
    "        # 在达到最大训练步数前一直进行训练\n",
    "        while total_steps < cfg['Max_train_steps']:\n",
    "            # 重置环境，获取初始状态\n",
    "            s, info = env.reset(seed=env_seed)  # 重置环境，使用环境种子\n",
    "            env_seed += 1\n",
    "            done = False\n",
    "\n",
    "            # 与环境进行交互并训练\n",
    "            while not done:\n",
    "                # 选择动作和动作对应的对数概率\n",
    "                a, logprob_a = agent.select_action(s, deterministic=False)  # 在训练时使用随机动作\n",
    "                s_next, r, dw, tr, info = env.step(a)  # 执行动作并获取下一个状态、奖励以及其他信息\n",
    "                done = (dw or tr)  # 如果游戏结束（死亡或胜利），则done为True\n",
    "\n",
    "                # 存储当前的转移数据\n",
    "                agent.put_data(s, a, r, s_next, logprob_a, done, dw, idx=traj_length)\n",
    "                s = s_next\n",
    "\n",
    "                traj_length += 1\n",
    "                total_steps += 1\n",
    "\n",
    "                # 如果达到更新时间\n",
    "                if traj_length % cfg['T_horizon'] == 0:\n",
    "                    a_loss, c_loss = agent.train()  # 执行PPO算法的训练步骤\n",
    "                    if cfg['write']:\n",
    "                        writer.add_scalar('Loss_a', np.mean(a_loss.detach().cpu().numpy()), global_step=total_steps)\n",
    "                        writer.add_scalar('Loss_c', np.mean(c_loss.detach().cpu().numpy()), global_step=total_steps)\n",
    "                    traj_length = 0\n",
    "\n",
    "                # 如果达到记录和日志的时间\n",
    "                if total_steps % cfg['eval_interval'] == 0:\n",
    "                    # 在评估环境中评估智能体，并输出平均奖励\n",
    "                    score = evaluate_policy(eval_env, agent, total_steps, turns=3)  # 对策略进行3次评估，取平均值\n",
    "                    scores_deque.append(score)\n",
    "                    test_steps += 1\n",
    "                    if cfg['write']:\n",
    "                        writer.add_scalar('Score_ep', score, global_step=total_steps)  # 将评估得分记录到TensorBoard\n",
    "                        writer.add_scalar('Score_Average', np.mean(scores_deque), global_step=total_steps)\n",
    "                    print('EnvName:', cfg['env_name'], 'seed:', cfg['seed'],\n",
    "                          'steps: {}k'.format(int(total_steps / 1000)), 'score:', score)\n",
    "                    \n",
    "                if total_steps % cfg['test_interval'] == 0:\n",
    "                    test_policy(eval_env, agent, total_steps, turns=1, path=cfg['path'])\n",
    "\n",
    "                # 如果达到保存模型的时间\n",
    "                if total_steps % cfg['save_interval'] == 0:\n",
    "                    print(\"保存模型\")\n",
    "                    agent.save(total_steps)  # 保存模型\n",
    "\n",
    "                if (np.mean(scores_deque) >= cfg['mean_break']) and (len(scores_deque) >= cfg['deque_maxlen']):\n",
    "                    print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(total_steps, np.mean(scores_deque)))\n",
    "                    test_policy(eval_env, agent, total_steps, turns=1, path=cfg['path'], cfg=cfg)\n",
    "                    print(\"保存模型\")\n",
    "                    agent.save(total_steps)\n",
    "                    env.close()\n",
    "                    eval_env.close()\n",
    "                    return\n",
    "\n",
    "        env.close()\n",
    "        eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aada328d-5f7c-48a3-8007-16307d84b4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取参数\n",
    "cfg = get_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1afb341-e477-4ee8-97d0-da6edd6d36ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态空间维度：8，动作空间维度：2，最大步数：1000\n"
     ]
    }
   ],
   "source": [
    "path = f\"device:{cfg['dvc']}/{cfg['env_name']}/seed:{cfg['seed']}/{cfg['algo_name']}/net_width-{cfg['net_width']}T_horizon-{cfg['T_horizon']}-gamma-{cfg['gamma']}-lambd-{cfg['lambd']}-clip_rate-{cfg['clip_rate']}-K_epochs-{cfg['K_epochs']}-a_lr-{cfg['a_lr']}-c_lr-{cfg['c_lr']}-l2_reg-{cfg['l2_reg']}-batch_size-{cfg['batch_size']}-entropy_coef-{cfg['entropy_coef']}-entropy_coef_decay-{cfg['entropy_coef_decay']}-adv_normalization-{cfg['adv_normalization']}\"\n",
    "cfg.update({\"path\":path}) # 更新n_states和n_actions到cfg参数中\n",
    "\n",
    "base_dir = f\"log/{cfg['path']}\"\n",
    "\n",
    "env, eval_env, agent = env_agent_config(cfg, path)\n",
    "\n",
    "cfg.update({\"mean_break\":cfg['max_e_steps'] * 0.99})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a492780c-b1b4-4c46-9cd1-85a4a1048e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "超参数\n",
      "================================================================================\n",
      "        Name        \t       Value        \t        Type        \n",
      "     algo_name      \t        PPO         \t   <class 'str'>    \n",
      "        dvc         \t        cuda        \t   <class 'str'>    \n",
      "      env_name      \tLunarLanderContinuous-v2\t   <class 'str'>    \n",
      "    render_mode     \t     rgb_array      \t   <class 'str'>    \n",
      "       write        \t         1          \t   <class 'bool'>   \n",
      "       render       \t         0          \t   <class 'bool'>   \n",
      "     Loadmodel      \t         0          \t   <class 'bool'>   \n",
      "     ModelIdex      \t       100000       \t   <class 'int'>    \n",
      "    deque_maxlen    \t         10         \t   <class 'int'>    \n",
      "        seed        \t         1          \t   <class 'int'>    \n",
      "     T_horizon      \t        500         \t   <class 'int'>    \n",
      "  Max_train_steps   \t     50000000.0     \t  <class 'float'>   \n",
      "   save_interval    \t      100000.0      \t  <class 'float'>   \n",
      "   eval_interval    \t       2000.0       \t  <class 'float'>   \n",
      "   test_interval    \t      100000.0      \t  <class 'float'>   \n",
      "       gamma        \t        0.99        \t  <class 'float'>   \n",
      "       lambd        \t        0.95        \t  <class 'float'>   \n",
      "     clip_rate      \t        0.2         \t  <class 'float'>   \n",
      "      K_epochs      \t         10         \t   <class 'int'>    \n",
      "     net_width      \t        128         \t   <class 'int'>    \n",
      "        a_lr        \t       0.0002       \t  <class 'float'>   \n",
      "        c_lr        \t       0.0002       \t  <class 'float'>   \n",
      "       l2_reg       \t       0.001        \t  <class 'float'>   \n",
      " a_optim_batch_size \t         64         \t   <class 'int'>    \n",
      " c_optim_batch_size \t         64         \t   <class 'int'>    \n",
      "     batch_size     \t         64         \t   <class 'int'>    \n",
      "    entropy_coef    \t         0          \t   <class 'int'>    \n",
      " entropy_coef_decay \t        0.99        \t  <class 'float'>   \n",
      " adv_normalization  \t         0          \t   <class 'bool'>   \n",
      "    Distribution    \t        Beta        \t   <class 'str'>    \n",
      "        path        \tdevice:cuda/LunarLanderContinuous-v2/seed:1/PPO/net_width-128T_horizon-500-gamma-0.99-lambd-0.95-clip_rate-0.2-K_epochs-10-a_lr-0.0002-c_lr-0.0002-l2_reg-0.001-batch_size-64-entropy_coef-0-entropy_coef_decay-0.99-adv_normalization-False\t   <class 'str'>    \n",
      "     state_dim      \t         8          \t   <class 'int'>    \n",
      "     action_dim     \t         2          \t   <class 'int'>    \n",
      "    max_e_steps     \t        1000        \t   <class 'int'>    \n",
      "     max_action     \t        1.0         \t  <class 'float'>   \n",
      "     mean_break     \t       990.0        \t  <class 'float'>   \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print_args(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86483326-9c0a-48a6-8fb2-953aa0d910c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/q1001p/anaconda3/envs/DRL_3.11/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/q1001p/anaconda3/envs/DRL_3.11/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvName: LunarLanderContinuous-v2 seed: 1 steps: 2k score: -484\n",
      "EnvName: LunarLanderContinuous-v2 seed: 1 steps: 4k score: -483\n",
      "EnvName: LunarLanderContinuous-v2 seed: 1 steps: 6k score: -436\n"
     ]
    }
   ],
   "source": [
    "train(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL_3.11",
   "language": "python",
   "name": "drl_3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
