{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import evaluate_policy, str2bool, test_policy, all_seed, Actor, Critic\n",
    "from datetime import datetime\n",
    "import gymnasium as gym\n",
    "import os, shutil\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    # 创建命令行参数解析器\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # 添加各种命令行参数\n",
    "    parser.add_argument('--algo_name', default='AC', type=str, help=\"算法名\")\n",
    "    parser.add_argument('--dvc', type=str, default='cuda', help='运行设备: cuda 或 cpu')\n",
    "    parser.add_argument('--env_name', type=str, default='CartPole-v1', help='环境名')\n",
    "    parser.add_argument('--render_mode', type=str, default='rgb_array', help='环境渲染模式')\n",
    "    parser.add_argument('--write', type=str2bool, default=True, help='使用SummaryWriter记录训练')\n",
    "    parser.add_argument('--render', type=str2bool, default=False, help='是否渲染')\n",
    "    parser.add_argument('--Loadmodel', type=str2bool, default=False, help='是否加载预训练模型')\n",
    "    parser.add_argument('--ModelIdex', type=int, default=100000, help='要加载的模型索引')\n",
    "    parser.add_argument('--deque_maxlen',default=30,type=int)\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=1, help='随机种子')\n",
    "    parser.add_argument('--Max_train_steps', type=int, default=5e7, help='最大训练步数')\n",
    "    parser.add_argument('--save_interval', type=int, default=2e3, help='模型保存间隔，以步为单位')\n",
    "    parser.add_argument('--eval_interval', type=int, default=1e3, help='模型评估间隔，以步为单位')\n",
    "    parser.add_argument('--test_interval', type=int, default=2e3, help='视频保存间隔，以步为单位')\n",
    "\n",
    "    parser.add_argument('--gamma', type=float, default=0.98, help='折扣因子')\n",
    "    parser.add_argument('--K_epochs', type=int, default=1, help='网络更新次数')\n",
    "    parser.add_argument('--net_width', type=int, default=128, help='隐藏网络宽度')\n",
    "    parser.add_argument('--a_lr', type=float, default=1e-3, help='学习率')\n",
    "    parser.add_argument('--c_lr', type=float, default=1e-2, help='学习率')\n",
    "    \n",
    "    # 解析命令行参数\n",
    "    args = parser.parse_args([])\n",
    "    args = {**vars(args)}  # 转换成字典类型    \n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_args(args):\n",
    "    ## 打印超参数\n",
    "    print(\"超参数\")\n",
    "    print(''.join(['=']*80))\n",
    "    tplt = \"{:^20}\\t{:^20}\\t{:^20}\"\n",
    "    print(tplt.format(\"Name\", \"Value\", \"Type\"))\n",
    "    for k,v in args.items():\n",
    "        print(tplt.format(k,v,str(type(v))))   \n",
    "    print(''.join(['=']*80))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AC:\n",
    "    def __init__(self, kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.actor_net = Actor(self.state_dim, self.net_width, self.action_dim).to(self.dvc)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor_net.parameters(), lr=self.a_lr)  # 使用Adam优化器\n",
    "        self.critic_net = Critic(self.state_dim, self.net_width).to(self.dvc)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic_net.parameters(), lr=self.c_lr)  # 使用Adam优化器\n",
    "\n",
    "    def choose_action(self, state):  # 根据动作概率分布随机采样\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.dvc)\n",
    "        probs = self.actor_net.pi(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'], dtype=torch.float).to(self.dvc)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.dvc)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.dvc)\n",
    "        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).to(self.dvc)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.dvc)\n",
    "\n",
    "        # 时序差分目标\n",
    "        td_target = rewards + self.gamma * self.critic_net(next_states) * (1 - dones)\n",
    "        # 时序差分误差\n",
    "        td_delta = td_target - self.critic_net(states)\n",
    "        log_probs = torch.log(self.actor_net(states).gather(1, actions))\n",
    "        actor_loss = torch.mean(-log_probs * td_delta.detach())\n",
    "        # 均方误差损失函数\n",
    "        critic_loss = torch.mean(F.mse_loss(self.critic_net(states), td_target.detach()))\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        actor_loss.backward()  # 计算策略网络的梯度\n",
    "        critic_loss.backward()  # 计算价值网络的梯度\n",
    "        self.actor_optimizer.step()  # 更新策略网络的参数\n",
    "        self.critic_optimizer.step()  # 更新价值网络的参数\n",
    "    \n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def save(self, episode):\n",
    "        \"\"\"\n",
    "        保存当前训练模型的Actor和Critic参数到文件\n",
    "\n",
    "        参数:\n",
    "        - episode: 当前训练的episode数，用于在文件名中标识不同的保存点\n",
    "        \"\"\"\n",
    "        model_path = f\"model/{cfg['path']}\"\n",
    "        # 检查是否存在'model'文件夹，如果不存在则创建\n",
    "        try:\n",
    "            os.makedirs(model_path)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        # 保存Critic的参数到文件\n",
    "        torch.save(self.critic_net.state_dict(), f\"{model_path}/ac_critic{episode}.pth\")\n",
    "        # 保存Actor的参数到文件\n",
    "        torch.save(self.actor_net.state_dict(), f\"{model_path}/ac_actor{episode}.pth\")\n",
    "\n",
    "    def load(self, episode):\n",
    "        \"\"\"\n",
    "        从文件加载之前保存的Actor和Critic参数\n",
    "\n",
    "        参数:\n",
    "        - episode: 要加载的保存点的episode数\n",
    "        \"\"\"\n",
    "        # 加载之前保存的Critic的参数\n",
    "        self.critic_net.load_state_dict(torch.load(\"./model/ac_critic{}.pth\".format(episode)))\n",
    "        # 加载之前保存的Actor的参数\n",
    "        self.actor_net.load_state_dict(torch.load(\"./model/ac_actor{}.pth\".format(episode)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_agent_config(cfg, path):\n",
    "    \"\"\"\n",
    "    配置环境和代理\n",
    "\n",
    "    参数:\n",
    "    - cfg: 包含配置信息的字典\n",
    "    - path: 模型保存路径\n",
    "\n",
    "    返回:\n",
    "    - env: Gym 环境\n",
    "    - agent: PPO 代理\n",
    "\n",
    "    说明:\n",
    "    1. 创建指定环境并设置渲染模式。\n",
    "    2. 如果配置中设置了种子，则为环境设置种子。\n",
    "    3. 获取环境的状态空间维度和动作空间维度。\n",
    "    4. 更新配置字典中的状态维度和动作维度。\n",
    "    5. 创建 PPO 代理。\n",
    "\n",
    "    注意:\n",
    "    - PPO 代理的创建依赖于配置信息和模型保存路径。\n",
    "    \"\"\"\n",
    "    env = gym.make(cfg['env_name'], render_mode=cfg['render_mode'])  # 1. 创建环境\n",
    "    eval_env = gym.make(cfg['env_name'], render_mode=cfg['render_mode'])\n",
    "    if cfg['seed'] != 0:\n",
    "        all_seed(env, seed=cfg['seed'])  # 2. 如果配置中设置了种子，则为环境设置种子\n",
    "\n",
    "    n_states = env.observation_space.shape[0]  # 3. 获取状态空间维度\n",
    "    n_actions = env.action_space.n  # 获取动作空间维度\n",
    "    max_e_steps = env._max_episode_steps  # 最大步数\n",
    "    print(f\"状态空间维度：{n_states}，动作空间维度：{n_actions}，最大步数：{max_e_steps}\")\n",
    "    cfg.update({\"state_dim\": n_states, \"action_dim\": n_actions, \"max_e_steps\": max_e_steps})  # 4. 更新n_states和n_actions到cfg参数中\n",
    "\n",
    "    agent = AC(cfg)  # 5. 创建 PPO 代理\n",
    "    return env, eval_env, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态空间维度：4，动作空间维度：2，最大步数：500\n",
      "超参数\n",
      "================================================================================\n",
      "        Name        \t       Value        \t        Type        \n",
      "     algo_name      \t         AC         \t   <class 'str'>    \n",
      "        dvc         \t        cuda        \t   <class 'str'>    \n",
      "      env_name      \t    CartPole-v1     \t   <class 'str'>    \n",
      "    render_mode     \t     rgb_array      \t   <class 'str'>    \n",
      "       write        \t         1          \t   <class 'bool'>   \n",
      "       render       \t         0          \t   <class 'bool'>   \n",
      "     Loadmodel      \t         0          \t   <class 'bool'>   \n",
      "     ModelIdex      \t       100000       \t   <class 'int'>    \n",
      "    deque_maxlen    \t         30         \t   <class 'int'>    \n",
      "        seed        \t         1          \t   <class 'int'>    \n",
      "  Max_train_steps   \t     50000000.0     \t  <class 'float'>   \n",
      "   save_interval    \t       2000.0       \t  <class 'float'>   \n",
      "   eval_interval    \t       1000.0       \t  <class 'float'>   \n",
      "   test_interval    \t       2000.0       \t  <class 'float'>   \n",
      "       gamma        \t        0.98        \t  <class 'float'>   \n",
      "      K_epochs      \t         1          \t   <class 'int'>    \n",
      "     net_width      \t        128         \t   <class 'int'>    \n",
      "        a_lr        \t       0.001        \t  <class 'float'>   \n",
      "        c_lr        \t        0.01        \t  <class 'float'>   \n",
      "        path        \tdevice:cuda/CartPole-v1/seed:1/AC/net_width-128-gamma-0.98-K_epochs-1-a_lr-0.001-c_lr-0.01\t   <class 'str'>    \n",
      "     state_dim      \t         4          \t   <class 'int'>    \n",
      "     action_dim     \t         2          \t<class 'numpy.int64'>\n",
      "    max_e_steps     \t        500         \t   <class 'int'>    \n",
      "     mean_break     \t       495.0        \t  <class 'float'>   \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 获取参数\n",
    "cfg = get_args()\n",
    "\n",
    "path = f\"device:{cfg['dvc']}/{cfg['env_name']}/seed:{cfg['seed']}/{cfg['algo_name']}/net_width-{cfg['net_width']}-gamma-{cfg['gamma']}-K_epochs-{cfg['K_epochs']}-a_lr-{cfg['a_lr']}-c_lr-{cfg['c_lr']}\"\n",
    "cfg.update({\"path\":path}) # 更新n_states和n_actions到cfg参数中\n",
    "\n",
    "base_dir = f\"log/{cfg['path']}\"\n",
    "\n",
    "env, eval_env, agent = env_agent_config(cfg, path)\n",
    "\n",
    "cfg.update({\"mean_break\":cfg['max_e_steps'] * 0.99})\n",
    "\n",
    "print_args(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg):\n",
    "    print(\"开始训练\")\n",
    "    env_seed = cfg['seed']\n",
    "    # 使用TensorBoard记录训练曲线\n",
    "    if cfg['write']:\n",
    "        writepath = 'runs/{}'.format(cfg['path']) # 构建TensorBoard日志路径\n",
    "        if os.path.exists(writepath): shutil.rmtree(writepath)  # 如果路径已存在，则删除该路径及其内容\n",
    "        writer = SummaryWriter(log_dir=writepath)  # 创建TensorBoard写入器，指定日志路径\n",
    "\n",
    "    # 如果指定了加载模型的选项，则加载模型\n",
    "    if cfg['Loadmodel']:\n",
    "        print(\"加载模型\")\n",
    "        agent.load(cfg['ModelIdex'])\n",
    "\n",
    "    # 如果选择渲染模式\n",
    "    if cfg['render']:\n",
    "        while True:\n",
    "            # 在环境中评估智能体的性能，并输出奖励\n",
    "            ep_r = evaluate_policy(env, agent, turns=1)\n",
    "            print('Env: ', cfg['env_name'],' Episode Reward: ', {ep_r})\n",
    "    else:\n",
    "        total_steps = 0  # 记录训练步数\n",
    "        test_steps = 0  # 记录测试步数\n",
    "        scores_deque = deque(maxlen=cfg['deque_maxlen'])\n",
    "\n",
    "        # 在达到最大训练步数前一直进行训练\n",
    "        while total_steps < cfg['Max_train_steps']:\n",
    "            episode_return = 0  # 记录每个episode的奖励\n",
    "            transition_dict = {\n",
    "                'states': [],\n",
    "                'actions': [],\n",
    "                'next_states': [],\n",
    "                'rewards': [],\n",
    "                'dones': []\n",
    "                }\n",
    "            \n",
    "            s, info = env.reset(seed=env_seed)  # 重置环境，使用环境种子\n",
    "            env_seed += 1\n",
    "            done = False\n",
    "\n",
    "            # 与环境进行交互并训练\n",
    "            while not done:\n",
    "                # 选择动作和动作对应的对数概率\n",
    "                a = agent.choose_action(s)  # 在训练时使用随机动作\n",
    "                s_next, r, dw, tr, info = env.step(a)  # 执行动作并获取下一个状态、奖励以及其他信息\n",
    "                done = (dw or tr)  # 如果游戏结束（死亡或胜利），则done为True\n",
    "\n",
    "                # 存储当前的转移数据\n",
    "                transition_dict['states'].append(s)\n",
    "                transition_dict['actions'].append(a)\n",
    "                transition_dict['next_states'].append(s_next)\n",
    "                transition_dict['rewards'].append(r)\n",
    "                transition_dict['dones'].append(done)\n",
    "                s = s_next\n",
    "                episode_return += r  # 累计奖励\n",
    "                \n",
    "            total_steps += 1\n",
    "            \n",
    "            scores_deque.append(episode_return)\n",
    "\n",
    "            # 更新智能体的策略\n",
    "            for _ in range(cfg['K_epochs']):\n",
    "                actor_loss, critic_loss = agent.update(transition_dict)  # 执行PPO算法的训练步骤\n",
    "            if cfg['write']:\n",
    "                writer.add_scalar('Actor_Loss', actor_loss, global_step=total_steps)\n",
    "                writer.add_scalar('Critic_Loss', critic_loss, global_step=total_steps)\n",
    "                writer.add_scalar('Episode_Reward', episode_return, global_step=total_steps)\n",
    "                writer.add_scalar('Episode_Average_Reward', np.mean(scores_deque), global_step=total_steps)\n",
    "\n",
    "            # 如果达到记录和日志的时间\n",
    "            if total_steps % cfg['eval_interval'] == 0:\n",
    "                # 在评估环境中评估智能体，并输出平均奖励\n",
    "                score = evaluate_policy(eval_env, agent, total_steps, turns=3)  # 对策略进行3次评估，取平均值\n",
    "                scores_deque.append(score)\n",
    "                test_steps += 1\n",
    "                if cfg['write']:\n",
    "                    writer.add_scalar('Score_ep', score, global_step=total_steps)  # 将评估得分记录到TensorBoard\n",
    "                    writer.add_scalar('Score_Average', np.mean(scores_deque), global_step=total_steps)\n",
    "                print('EnvName:', cfg['env_name'], 'seed:', cfg['seed'],\n",
    "                        'steps: {}k'.format(int(total_steps / 1000)), 'score:', score)\n",
    "                    \n",
    "            if total_steps % cfg['test_interval'] == 0:\n",
    "                test_policy(eval_env, agent, total_steps, turns=1, path=cfg['path'], cfg=cfg)\n",
    "\n",
    "            # 如果达到保存模型的时间\n",
    "            if total_steps % cfg['save_interval'] == 0:\n",
    "                print(\"保存模型\")\n",
    "                agent.save(total_steps)  # 保存模型\n",
    "\n",
    "            if (np.mean(scores_deque) >= cfg['mean_break']) and (len(scores_deque) >= cfg['deque_maxlen']):\n",
    "                print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(total_steps, np.mean(scores_deque)))\n",
    "                test_policy(eval_env, agent, total_steps, turns=1, path=cfg['path'], cfg=cfg)\n",
    "                print(\"保存模型\")\n",
    "                agent.save(total_steps)\n",
    "                env.close()\n",
    "                eval_env.close()\n",
    "                return\n",
    "\n",
    "        env.close()\n",
    "        eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11678/3648875864.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  state = torch.tensor([state], dtype=torch.float).to(self.dvc)\n"
     ]
    }
   ],
   "source": [
    "train(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL_3.11",
   "language": "python",
   "name": "drl_3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
