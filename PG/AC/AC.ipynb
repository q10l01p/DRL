{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import evaluate_policy, str2bool, test_policy, all_seed, Actor, Critic\n",
    "from datetime import datetime\n",
    "import gymnasium as gym\n",
    "import os, shutil\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    # 创建命令行参数解析器\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # 添加各种命令行参数\n",
    "    parser.add_argument('--algo_name', default='AC', type=str, help=\"算法名\")\n",
    "    parser.add_argument('--dvc', type=str, default='cuda', help='运行设备: cuda 或 cpu')\n",
    "    parser.add_argument('--env_name', type=str, default='CartPole-v1', help='环境名')\n",
    "    parser.add_argument('--render_mode', type=str, default='rgb_array', help='环境渲染模式')\n",
    "    parser.add_argument('--write', type=str2bool, default=True, help='使用SummaryWriter记录训练')\n",
    "    parser.add_argument('--render', type=str2bool, default=False, help='是否渲染')\n",
    "    parser.add_argument('--Loadmodel', type=str2bool, default=False, help='是否加载预训练模型')\n",
    "    parser.add_argument('--ModelIdex', type=int, default=100000, help='要加载的模型索引')\n",
    "    parser.add_argument('--deque_maxlen',default=40,type=int)\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=10, help='随机种子')\n",
    "    parser.add_argument('--Max_train_steps', type=int, default=5e7, help='最大训练步数')\n",
    "    parser.add_argument('--save_interval', type=int, default=2e3, help='模型保存间隔，以步为单位')\n",
    "    parser.add_argument('--eval_interval', type=int, default=1e3, help='模型评估间隔，以步为单位')\n",
    "    parser.add_argument('--test_interval', type=int, default=2e3, help='视频保存间隔，以步为单位')\n",
    "\n",
    "    parser.add_argument('--gamma', type=float, default=0.98, help='折扣因子')\n",
    "    parser.add_argument('--K_epochs', type=int, default=1, help='网络更新次数')\n",
    "    parser.add_argument('--net_width', type=int, default=128, help='隐藏网络宽度')\n",
    "    parser.add_argument('--a_lr', type=float, default=1e-3, help='学习率')\n",
    "    parser.add_argument('--c_lr', type=float, default=1e-2, help='学习率')\n",
    "    \n",
    "    # 解析命令行参数\n",
    "    args = parser.parse_args([])\n",
    "    args = {**vars(args)}  # 转换成字典类型    \n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_args(args):\n",
    "    ## 打印超参数\n",
    "    print(\"超参数\")\n",
    "    print(''.join(['=']*80))\n",
    "    tplt = \"{:^20}\\t{:^20}\\t{:^20}\"\n",
    "    print(tplt.format(\"Name\", \"Value\", \"Type\"))\n",
    "    for k,v in args.items():\n",
    "        print(tplt.format(k,v,str(type(v))))   \n",
    "    print(''.join(['=']*80))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AC:\n",
    "    def __init__(self, kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "        self.actor_net = Actor(self.state_dim, self.net_width, self.action_dim).to(self.dvc)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor_net.parameters(), lr=self.a_lr)  # 使用Adam优化器\n",
    "        self.critic_net = Critic(self.state_dim, self.net_width).to(self.dvc)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic_net.parameters(), lr=self.c_lr)  # 使用Adam优化器\n",
    "\n",
    "    def choose_action(self, state):  # 根据动作概率分布随机采样\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.dvc)\n",
    "        probs = self.actor_net(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'], dtype=torch.float).to(self.dvc)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(self.dvc)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.dvc)\n",
    "        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).to(self.dvc)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.dvc)\n",
    "\n",
    "        # 时序差分目标\n",
    "        td_target = rewards + self.gamma * self.critic_net(next_states) * (1 - dones)\n",
    "        # 时序差分误差\n",
    "        td_delta = td_target - self.critic_net(states)\n",
    "        log_probs = torch.log(self.actor_net(states).gather(1, actions))\n",
    "        actor_loss = torch.mean(-log_probs * td_delta.detach())\n",
    "        # 均方误差损失函数\n",
    "        critic_loss = torch.mean(F.mse_loss(self.critic_net(states), td_target.detach()))\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        actor_loss.backward()  # 计算策略网络的梯度\n",
    "        critic_loss.backward()  # 计算价值网络的梯度\n",
    "        self.actor_optimizer.step()  # 更新策略网络的参数\n",
    "        self.critic_optimizer.step()  # 更新价值网络的参数\n",
    "    \n",
    "        return actor_loss, critic_loss\n",
    "\n",
    "    def save(self, episode):\n",
    "        \"\"\"\n",
    "        保存当前训练模型的Actor和Critic参数到文件\n",
    "\n",
    "        参数:\n",
    "        - episode: 当前训练的episode数，用于在文件名中标识不同的保存点\n",
    "        \"\"\"\n",
    "        model_path = f\"model/{cfg['path']}\"\n",
    "        # 检查是否存在'model'文件夹，如果不存在则创建\n",
    "        try:\n",
    "            os.makedirs(model_path)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        # 保存Critic的参数到文件\n",
    "        torch.save(self.critic_net.state_dict(), f\"{model_path}/ac_critic{episode}.pth\")\n",
    "        # 保存Actor的参数到文件\n",
    "        torch.save(self.actor_net.state_dict(), f\"{model_path}/ac_actor{episode}.pth\")\n",
    "\n",
    "    def load(self, episode):\n",
    "        \"\"\"\n",
    "        从文件加载之前保存的Actor和Critic参数\n",
    "\n",
    "        参数:\n",
    "        - episode: 要加载的保存点的episode数\n",
    "        \"\"\"\n",
    "        # 加载之前保存的Critic的参数\n",
    "        self.critic_net.load_state_dict(torch.load(\"./model/ac_critic{}.pth\".format(episode)))\n",
    "        # 加载之前保存的Actor的参数\n",
    "        self.actor_net.load_state_dict(torch.load(\"./model/ac_actor{}.pth\".format(episode)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_agent_config(cfg, path):\n",
    "    \"\"\"\n",
    "    配置环境和代理\n",
    "\n",
    "    参数:\n",
    "    - cfg: 包含配置信息的字典\n",
    "    - path: 模型保存路径\n",
    "\n",
    "    返回:\n",
    "    - env: Gym 环境\n",
    "    - agent: PPO 代理\n",
    "\n",
    "    说明:\n",
    "    1. 创建指定环境并设置渲染模式。\n",
    "    2. 如果配置中设置了种子，则为环境设置种子。\n",
    "    3. 获取环境的状态空间维度和动作空间维度。\n",
    "    4. 更新配置字典中的状态维度和动作维度。\n",
    "    5. 创建 PPO 代理。\n",
    "\n",
    "    注意:\n",
    "    - PPO 代理的创建依赖于配置信息和模型保存路径。\n",
    "    \"\"\"\n",
    "    env = gym.make(cfg['env_name'], render_mode=cfg['render_mode'])  # 1. 创建环境\n",
    "    eval_env = gym.make(cfg['env_name'], render_mode=cfg['render_mode'])\n",
    "    if cfg['seed'] != 0:\n",
    "        all_seed(env, seed=cfg['seed'])  # 2. 如果配置中设置了种子，则为环境设置种子\n",
    "\n",
    "    n_states = env.observation_space.shape[0]  # 3. 获取状态空间维度\n",
    "    n_actions = env.action_space.n  # 获取动作空间维度\n",
    "    max_e_steps = env._max_episode_steps  # 最大步数\n",
    "    print(f\"状态空间维度：{n_states}，动作空间维度：{n_actions}，最大步数：{max_e_steps}\")\n",
    "    cfg.update({\"state_dim\": n_states, \"action_dim\": n_actions, \"max_e_steps\": max_e_steps})  # 4. 更新n_states和n_actions到cfg参数中\n",
    "\n",
    "    agent = AC(cfg)  # 5. 创建 PPO 代理\n",
    "    return env, eval_env, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态空间维度：4，动作空间维度：2，最大步数：500\n",
      "超参数\n",
      "================================================================================\n",
      "        Name        \t       Value        \t        Type        \n",
      "     algo_name      \t         AC         \t   <class 'str'>    \n",
      "        dvc         \t        cuda        \t   <class 'str'>    \n",
      "      env_name      \t    CartPole-v1     \t   <class 'str'>    \n",
      "    render_mode     \t     rgb_array      \t   <class 'str'>    \n",
      "       write        \t         1          \t   <class 'bool'>   \n",
      "       render       \t         0          \t   <class 'bool'>   \n",
      "     Loadmodel      \t         0          \t   <class 'bool'>   \n",
      "     ModelIdex      \t       100000       \t   <class 'int'>    \n",
      "    deque_maxlen    \t         40         \t   <class 'int'>    \n",
      "        seed        \t         10         \t   <class 'int'>    \n",
      "  Max_train_steps   \t     50000000.0     \t  <class 'float'>   \n",
      "   save_interval    \t       2000.0       \t  <class 'float'>   \n",
      "   eval_interval    \t       1000.0       \t  <class 'float'>   \n",
      "   test_interval    \t       2000.0       \t  <class 'float'>   \n",
      "       gamma        \t        0.98        \t  <class 'float'>   \n",
      "      K_epochs      \t         1          \t   <class 'int'>    \n",
      "     net_width      \t        128         \t   <class 'int'>    \n",
      "        a_lr        \t       0.001        \t  <class 'float'>   \n",
      "        c_lr        \t        0.01        \t  <class 'float'>   \n",
      "        path        \tdevice:cuda/CartPole-v1/seed:10/AC/net_width-128-gamma-0.98-K_epochs-1-a_lr-0.001-c_lr-0.01\t   <class 'str'>    \n",
      "     state_dim      \t         4          \t   <class 'int'>    \n",
      "     action_dim     \t         2          \t<class 'numpy.int64'>\n",
      "    max_e_steps     \t        500         \t   <class 'int'>    \n",
      "     mean_break     \t       495.0        \t  <class 'float'>   \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# 获取参数\n",
    "cfg = get_args()\n",
    "\n",
    "path = f\"device:{cfg['dvc']}/{cfg['env_name']}/seed:{cfg['seed']}/{cfg['algo_name']}/net_width-{cfg['net_width']}-gamma-{cfg['gamma']}-K_epochs-{cfg['K_epochs']}-a_lr-{cfg['a_lr']}-c_lr-{cfg['c_lr']}\"\n",
    "cfg.update({\"path\":path}) # 更新n_states和n_actions到cfg参数中\n",
    "\n",
    "base_dir = f\"log/{cfg['path']}\"\n",
    "\n",
    "env, eval_env, agent = env_agent_config(cfg, path)\n",
    "\n",
    "cfg.update({\"mean_break\":cfg['max_e_steps'] * 0.99})\n",
    "\n",
    "print_args(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg):\n",
    "    print(\"开始训练\")\n",
    "    env_seed = cfg['seed']\n",
    "    # 使用TensorBoard记录训练曲线\n",
    "    if cfg['write']:\n",
    "        writepath = 'runs/{}'.format(cfg['path']) # 构建TensorBoard日志路径\n",
    "        if os.path.exists(writepath): shutil.rmtree(writepath)  # 如果路径已存在，则删除该路径及其内容\n",
    "        writer = SummaryWriter(log_dir=writepath)  # 创建TensorBoard写入器，指定日志路径\n",
    "\n",
    "    # 如果指定了加载模型的选项，则加载模型\n",
    "    if cfg['Loadmodel']:\n",
    "        print(\"加载模型\")\n",
    "        agent.load(cfg['ModelIdex'])\n",
    "\n",
    "    # 如果选择渲染模式\n",
    "    if cfg['render']:\n",
    "        while True:\n",
    "            # 在环境中评估智能体的性能，并输出奖励\n",
    "            ep_r = evaluate_policy(env, agent, turns=1)\n",
    "            print('Env: ', cfg['env_name'],' Episode Reward: ', {ep_r})\n",
    "    else:\n",
    "        total_steps = 0  # 记录训练步数\n",
    "        test_steps = 0  # 记录测试步数\n",
    "        scores_deque = deque(maxlen=cfg['deque_maxlen'])\n",
    "\n",
    "        # 在达到最大训练步数前一直进行训练\n",
    "        while total_steps < cfg['Max_train_steps']:\n",
    "            episode_return = 0  # 记录每个episode的奖励\n",
    "            transition_dict = {\n",
    "                'states': [],\n",
    "                'actions': [],\n",
    "                'next_states': [],\n",
    "                'rewards': [],\n",
    "                'dones': []\n",
    "                }\n",
    "            \n",
    "            s, info = env.reset(seed=env_seed)  # 重置环境，使用环境种子\n",
    "            env_seed += 1\n",
    "            done = False\n",
    "\n",
    "            # 与环境进行交互并训练\n",
    "            while not done:\n",
    "                # 选择动作和动作对应的对数概率\n",
    "                a = agent.choose_action(s)  # 在训练时使用随机动作\n",
    "                s_next, r, dw, tr, info = env.step(a)  # 执行动作并获取下一个状态、奖励以及其他信息\n",
    "                done = (dw or tr)  # 如果游戏结束（死亡或胜利），则done为True\n",
    "\n",
    "                # 存储当前的转移数据\n",
    "                transition_dict['states'].append(s)\n",
    "                transition_dict['actions'].append(a)\n",
    "                transition_dict['next_states'].append(s_next)\n",
    "                transition_dict['rewards'].append(r)\n",
    "                transition_dict['dones'].append(done)\n",
    "                s = s_next\n",
    "                episode_return += r  # 累计奖励\n",
    "                \n",
    "            total_steps += 1\n",
    "            \n",
    "            scores_deque.append(episode_return)\n",
    "\n",
    "            # 更新智能体的策略\n",
    "            for _ in range(cfg['K_epochs']):\n",
    "                actor_loss, critic_loss = agent.update(transition_dict)  # 执行PPO算法的训练步骤\n",
    "            if cfg['write']:\n",
    "                writer.add_scalar('Actor_Loss', actor_loss, global_step=total_steps)\n",
    "                writer.add_scalar('Critic_Loss', critic_loss, global_step=total_steps)\n",
    "                writer.add_scalar('Episode_Reward', episode_return, global_step=total_steps)\n",
    "                writer.add_scalar('Episode_Average_Reward', np.mean(scores_deque), global_step=total_steps)\n",
    "\n",
    "            # 如果达到记录和日志的时间\n",
    "            if total_steps % cfg['eval_interval'] == 0:\n",
    "                # 在评估环境中评估智能体，并输出平均奖励\n",
    "                score = evaluate_policy(eval_env, agent, total_steps, turns=3)  # 对策略进行3次评估，取平均值\n",
    "                scores_deque.append(score)\n",
    "                test_steps += 1\n",
    "                if cfg['write']:\n",
    "                    writer.add_scalar('Score_ep', score, global_step=total_steps)  # 将评估得分记录到TensorBoard\n",
    "                    writer.add_scalar('Score_Average', np.mean(scores_deque), global_step=total_steps)\n",
    "                print('EnvName:', cfg['env_name'], 'seed:', cfg['seed'],\n",
    "                        'steps: {}k'.format(int(total_steps / 1000)), 'score:', score)\n",
    "                    \n",
    "            if total_steps % cfg['test_interval'] == 0:\n",
    "                test_policy(eval_env, agent, total_steps, turns=1, path=cfg['path'], cfg=cfg)\n",
    "\n",
    "            # 如果达到保存模型的时间\n",
    "            if total_steps % cfg['save_interval'] == 0:\n",
    "                print(\"保存模型\")\n",
    "                agent.save(total_steps)  # 保存模型\n",
    "\n",
    "            if (np.mean(scores_deque) >= cfg['mean_break']) and (len(scores_deque) >= cfg['deque_maxlen']):\n",
    "                print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(total_steps, np.mean(scores_deque)))\n",
    "                test_policy(eval_env, agent, total_steps, turns=1, path=cfg['path'], cfg=cfg)\n",
    "                print(\"保存模型\")\n",
    "                agent.save(total_steps)\n",
    "                env.close()\n",
    "                eval_env.close()\n",
    "                return\n",
    "\n",
    "        env.close()\n",
    "        eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4276/3011923577.py:10: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  state = torch.tensor([state], dtype=torch.float).to(self.dvc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvName: CartPole-v1 seed: 10 steps: 1k score: 450\n",
      "Environment solved in 1546 episodes!\tAverage Score: 495.70\n",
      "保存视频\n",
      "保存模型\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL_3.11",
   "language": "python",
   "name": "drl_3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
