{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import evaluate_policy, str2bool, Actor, Q_Critic, ReplayBuffer, test_policy\n",
    "from datetime import datetime\n",
    "import gymnasium as gym\n",
    "import os, shutil\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import math\n",
    "from collections import deque\n",
    "\n",
    "from torch.distributions import Beta,Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    # 创建命令行参数解析器\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # 添加各种命令行参数\n",
    "    parser.add_argument('--algo_name',default='DDPO',type=str,help=\"算法名\")\n",
    "    parser.add_argument('--dvc', type=str, default='cuda', help='运行设备: cuda 或 cpu')\n",
    "    parser.add_argument('--env_name', type=str, default='InvertedDoublePendulum-v4', help='环境名')\n",
    "    parser.add_argument('--render_mode', type=str, default='rgb_array', help='环境渲染模式')\n",
    "    parser.add_argument('--write', type=str2bool, default=True, help='使用SummaryWriter记录训练')\n",
    "    parser.add_argument('--render', type=str2bool, default=False, help='是否渲染')\n",
    "    parser.add_argument('--Loadmodel', type=str2bool, default=False, help='是否加载预训练模型')\n",
    "    parser.add_argument('--ModelIdex', type=int, default=2350000, help='要加载的模型索引')\n",
    "    parser.add_argument('--deque_maxlen',default=20,type=int)\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=1, help='随机种子')\n",
    "    parser.add_argument('--Max_train_steps', type=int, default=5e7, help='最大训练步数')\n",
    "    parser.add_argument('--save_interval', type=int, default=5e4, help='模型保存间隔，以步为单位')\n",
    "    parser.add_argument('--eval_interval', type=int, default=2e3, help='模型评估间隔，以步为单位')\n",
    "    parser.add_argument('--test_interval', type=int, default=5e4, help='视频保存间隔，以步为单位')\n",
    "\n",
    "    parser.add_argument('--gamma', type=float, default=0.99, help='折扣因子')\n",
    "    parser.add_argument('--net_width', type=int, default=128, help='隐藏网络宽度')\n",
    "    parser.add_argument('--a_lr', type=float, default=1e-3, help='Learning rate of actor')\n",
    "    parser.add_argument('--c_lr', type=float, default=1e-3, help='Learning rate of critic')\n",
    "    parser.add_argument('--batch_size', type=int, default=128, help='切片轨迹的长度')\n",
    "    parser.add_argument('--random_steps', type=int, default=5e4, help='random steps before trianing')\n",
    "    parser.add_argument('--noise', type=float, default=0.1, help='exploring noise')\n",
    "    parser.add_argument('--tau', type=float, default=0.005, help='soft update tau')\n",
    "    \n",
    "    # 解析命令行参数\n",
    "    args = parser.parse_args([])\n",
    "    args = {**vars(args)}  # 转换成字典类型    \n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_args(args):\n",
    "    ## 打印超参数\n",
    "    print(\"超参数\")\n",
    "    print(''.join(['=']*80))\n",
    "    tplt = \"{:^20}\\t{:^20}\\t{:^20}\"\n",
    "    print(tplt.format(\"Name\", \"Value\", \"Type\"))\n",
    "    for k,v in args.items():\n",
    "        print(tplt.format(k,v,str(type(v))))   \n",
    "    print(''.join(['=']*80))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_seed(env, seed):\n",
    "    \"\"\"\n",
    "    设置随机种子以确保实验的可重复性\n",
    "\n",
    "    参数:\n",
    "    - env: Gym 环境，用于训练模型\n",
    "    - seed: 随机种子值\n",
    "\n",
    "    说明:\n",
    "    1. 使用给定的随机种子设置 NumPy、Python、PyTorch 和 CUDA 的随机生成器。\n",
    "    2. 禁用 CUDA 的非确定性操作以确保实验结果的一致性。\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(seed)  # 设置 NumPy 随机种子\n",
    "    random.seed(seed)  # 设置 Python 随机种子\n",
    "    torch.manual_seed(seed)  # 设置 PyTorch 随机种子\n",
    "    torch.cuda.manual_seed(seed)  # 设置 PyTorch CUDA 随机种子\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # 设置 Python Hash 随机种子\n",
    "    torch.backends.cudnn.deterministic = True  # 禁用 CUDA 非确定性操作以确保实验结果的一致性\n",
    "    torch.backends.cudnn.benchmark = False  # 禁用 CUDA 非确定性操作以确保实验结果的一致性\n",
    "    torch.backends.cudnn.enabled = False  # 禁用 CUDA 非确定性操作以确保实验结果的一致性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG_agent():\n",
    "    def __init__(self, kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "        self.actor = Actor(self.state_dim, self.action_dim, self.net_width, self.max_action).to(self.dvc)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=self.a_lr)\n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "\n",
    "        self.q_critic = Q_Critic(self.state_dim, self.action_dim, self.net_width).to(self.dvc)\n",
    "        self.q_critic_optimizer = torch.optim.Adam(self.q_critic.parameters(), lr=self.c_lr)\n",
    "        self.q_critic_target = copy.deepcopy(self.q_critic)\n",
    "\n",
    "        self.replay_buffer = ReplayBuffer(self.state_dim, self.action_dim, max_size=int(5e5), dvc=self.dvc)\n",
    "\n",
    "    def select_action(self, state, deterministic):\n",
    "        \"\"\"\n",
    "        选择动作\n",
    "\n",
    "        参数:\n",
    "        - state: 当前状态\n",
    "        - deterministic: 是否使用确定性策略选择动作\n",
    "\n",
    "        返回:\n",
    "        - 动作\n",
    "\n",
    "        说明:\n",
    "        1. 将状态从 [x, x, ..., x] 转换为 [[x, x, ..., x]]。\n",
    "        2. 通过 Actor 网络获取动作。\n",
    "        3. 如果是确定性策略，则直接返回动作。\n",
    "        4. 如果是非确定性策略，则添加噪声并返回，确保在动作范围内。\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state[np.newaxis, :]).to(self.dvc)  # 1. 将状态转换为网络输入格式\n",
    "            a = self.actor(state).cpu().numpy()[0]  # 2. 获取动作\n",
    "\n",
    "            if deterministic:\n",
    "                return a  # 3. 返回动作（确定性策略）\n",
    "\n",
    "            else:\n",
    "                noise = np.random.normal(0, self.max_action * self.noise, size=self.action_dim)  # 4. 生成噪声\n",
    "                return (a + noise).clip(-self.max_action, self.max_action)  # 5. 添加噪声并确保在动作范围内\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        训练方法\n",
    "\n",
    "        说明:\n",
    "        1. 从经验回放缓冲区中采样一批数据，包括当前状态(s)、动作(a)、奖励(r)、下一个状态(s_next)和终止标志(dw)。\n",
    "        2. 使用目标策略网络(actor_target)预测下一个状态的动作(target_a_next)。\n",
    "        3. 使用目标评论网络(q_critic_target)计算目标Q值(target_Q)。\n",
    "        4. 计算目标Q值，考虑折扣因子(gamma)和终止标志(dw)。\n",
    "        5. 计算当前评论网络(q_critic)的Q值(current_Q)。\n",
    "        6. 计算Q值损失(q_loss)并执行评论网络优化。\n",
    "        7. 计算动作损失(a_loss)并执行策略网络优化。\n",
    "        8. 更新目标评论网络和目标策略网络参数，使用软更新策略。\n",
    "\n",
    "        注意:\n",
    "        - 该方法执行了一次深度强化学习的训练迭代。\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            s, a, r, s_next, dw = self.replay_buffer.sample(self.batch_size)  # 1. 采样经验数据\n",
    "            target_a_next = self.actor_target(s_next)  # 2. 预测下一个状态的动作\n",
    "            target_Q = self.q_critic_target(s_next, target_a_next)  # 3. 计算目标Q值\n",
    "            target_Q = r + (~dw) * self.gamma * target_Q  # 4. 计算目标Q值，考虑折扣因子和终止标志\n",
    "\n",
    "        current_Q = self.q_critic(s, a)  # 5. 计算当前评论网络的Q值\n",
    "\n",
    "        q_loss = F.mse_loss(current_Q, target_Q)  # 6. 计算Q值损失\n",
    "\n",
    "        self.q_critic_optimizer.zero_grad()\n",
    "        q_loss.backward()\n",
    "        self.q_critic_optimizer.step()  # 6. 执行评论网络优化\n",
    "\n",
    "        a_loss = -self.q_critic(s, self.actor(s)).mean()  # 7. 计算动作损失\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        a_loss.backward()\n",
    "        self.actor_optimizer.step()  # 7. 执行策略网络优化\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for param, target_param in zip(self.q_critic.parameters(), self.q_critic_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)  # 8. 更新目标网络参数，使用软更新策略\n",
    "        \n",
    "        return a_loss, q_loss\n",
    "\n",
    "    def save(self, episode):\n",
    "        \"\"\"\n",
    "        保存当前训练模型的Actor和Critic参数到文件\n",
    "\n",
    "        参数:\n",
    "        - episode: 当前训练的episode数，用于在文件名中标识不同的保存点\n",
    "        \"\"\"\n",
    "        model_path = f\"model/{cfg['path']}\"\n",
    "        # 检查是否存在'model'文件夹，如果不存在则创建\n",
    "        try:\n",
    "            os.makedirs(model_path)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        # 保存Critic的参数到文件\n",
    "        torch.save(self.q_critic.state_dict(), f\"{model_path}/ddpg_critic{episode}.pth\")\n",
    "        # 保存Actor的参数到文件\n",
    "        torch.save(self.actor.state_dict(), f\"{model_path}/ppo_actor{episode}.pth\")\n",
    "\n",
    "    def load(self, episode):\n",
    "        \"\"\"\n",
    "        从文件加载之前保存的Actor和Critic参数\n",
    "\n",
    "        参数:\n",
    "        - episode: 要加载的保存点的episode数\n",
    "        \"\"\"\n",
    "        model_path = f\"model/{cfg['path']}\"\n",
    "        # 加载之前保存的Critic的参数\n",
    "        self.q_critic.load_state_dict(torch.load(f\"{model_path}/ddpg_critic{episode}.pth\"))\n",
    "        # 加载之前保存的Actor的参数\n",
    "        self.actor.load_state_dict(torch.load(f\"{model_path}/ppo_actor{episode}.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def env_agent_config(cfg, path):\n",
    "    \"\"\"\n",
    "    配置环境和代理\n",
    "\n",
    "    参数:\n",
    "    - cfg: 包含配置信息的字典\n",
    "    - path: 模型保存路径\n",
    "\n",
    "    返回:\n",
    "    - env: Gym 环境\n",
    "    - agent: PPO 代理\n",
    "\n",
    "    说明:\n",
    "    1. 创建指定环境并设置渲染模式。\n",
    "    2. 如果配置中设置了种子，则为环境设置种子。\n",
    "    3. 获取环境的状态空间维度和动作空间维度。\n",
    "    4. 更新配置字典中的状态维度和动作维度。\n",
    "    5. 创建 PPO 代理。\n",
    "\n",
    "    注意:\n",
    "    - PPO 代理的创建依赖于配置信息和模型保存路径。\n",
    "    \"\"\"\n",
    "    env = gym.make(cfg['env_name'], render_mode=cfg['render_mode'])  # 1. 创建环境\n",
    "    eval_env = gym.make(cfg['env_name'], render_mode=cfg['render_mode'])\n",
    "    if cfg['seed'] != 0:\n",
    "        all_seed(env, seed=cfg['seed'])  # 2. 如果配置中设置了种子，则为环境设置种子\n",
    "\n",
    "    n_states = env.observation_space.shape[0]  # 3. 获取状态空间维度\n",
    "    n_actions = env.action_space.shape[0]  # 获取动作空间维度\n",
    "    max_action = float(env.action_space.high[0]) # 获取动作空间的最大值\n",
    "    max_e_steps = env._max_episode_steps  # 最大步数\n",
    "    print(f\"状态空间维度：{n_states}，动作空间维度：{n_actions}，最大步数：{max_e_steps}\")\n",
    "    cfg.update({\"state_dim\": n_states, \"action_dim\": n_actions, \"max_e_steps\": max_e_steps, \"max_action\": max_action})  # 4. 更新n_states和n_actions到cfg参数中\n",
    "\n",
    "    agent = DDPG_agent(cfg)  # 5. 创建 PPO 代理\n",
    "    return env, eval_env, agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "状态空间维度：11，动作空间维度：1，最大步数：1000\n",
      "超参数\n",
      "================================================================================\n",
      "        Name        \t       Value        \t        Type        \n",
      "     algo_name      \t        DDPO        \t   <class 'str'>    \n",
      "        dvc         \t        cuda        \t   <class 'str'>    \n",
      "      env_name      \tInvertedDoublePendulum-v4\t   <class 'str'>    \n",
      "    render_mode     \t     rgb_array      \t   <class 'str'>    \n",
      "       write        \t         1          \t   <class 'bool'>   \n",
      "       render       \t         0          \t   <class 'bool'>   \n",
      "     Loadmodel      \t         0          \t   <class 'bool'>   \n",
      "     ModelIdex      \t      2350000       \t   <class 'int'>    \n",
      "    deque_maxlen    \t         20         \t   <class 'int'>    \n",
      "        seed        \t         1          \t   <class 'int'>    \n",
      "  Max_train_steps   \t     50000000.0     \t  <class 'float'>   \n",
      "   save_interval    \t      50000.0       \t  <class 'float'>   \n",
      "   eval_interval    \t       2000.0       \t  <class 'float'>   \n",
      "   test_interval    \t      50000.0       \t  <class 'float'>   \n",
      "       gamma        \t        0.99        \t  <class 'float'>   \n",
      "     net_width      \t        128         \t   <class 'int'>    \n",
      "        a_lr        \t       0.001        \t  <class 'float'>   \n",
      "        c_lr        \t       0.001        \t  <class 'float'>   \n",
      "     batch_size     \t        128         \t   <class 'int'>    \n",
      "    random_steps    \t      50000.0       \t  <class 'float'>   \n",
      "       noise        \t        0.1         \t  <class 'float'>   \n",
      "        tau         \t       0.005        \t  <class 'float'>   \n",
      "        path        \tdevice:cuda/InvertedDoublePendulum-v4/seed:1/DDPO/net_width-128-gamma-0.99-a_lr-0.001-c_lr-0.001-batch_size-128\t   <class 'str'>    \n",
      "     state_dim      \t         11         \t   <class 'int'>    \n",
      "     action_dim     \t         1          \t   <class 'int'>    \n",
      "    max_e_steps     \t        1000        \t   <class 'int'>    \n",
      "     max_action     \t        1.0         \t  <class 'float'>   \n",
      "     mean_break     \t   100000000000.0   \t  <class 'float'>   \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "cfg = get_args()\n",
    "\n",
    "path = f\"device:{cfg['dvc']}/{cfg['env_name']}/seed:{cfg['seed']}/{cfg['algo_name']}/net_width-{cfg['net_width']}-gamma-{cfg['gamma']}-a_lr-{cfg['a_lr']}-c_lr-{cfg['c_lr']}-batch_size-{cfg['batch_size']}\"\n",
    "cfg.update({\"path\":path}) # 更新n_states和n_actions到cfg参数中\n",
    "\n",
    "base_dir = f\"log/{cfg['path']}\"\n",
    "\n",
    "env, eval_env, agent = env_agent_config(cfg, path)\n",
    "\n",
    "cfg.update({\"mean_break\":10e10})\n",
    "\n",
    "print_args(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(cfg):\n",
    "    print(\"开始训练\")\n",
    "    env_seed = cfg['seed']\n",
    "    # 使用TensorBoard记录训练曲线\n",
    "    if cfg['write']:\n",
    "        writepath = 'runs/{}'.format(cfg['path']) # 构建TensorBoard日志路径\n",
    "        if os.path.exists(writepath): shutil.rmtree(writepath)  # 如果路径已存在，则删除该路径及其内容\n",
    "        writer = SummaryWriter(log_dir=writepath)  # 创建TensorBoard写入器，指定日志路径\n",
    "\n",
    "    # 如果指定了加载模型的选项，则加载模型\n",
    "    if cfg['Loadmodel']:\n",
    "        print(\"加载模型\")\n",
    "        agent.load(cfg['ModelIdex'])\n",
    "\n",
    "    # 如果选择渲染模式\n",
    "    if cfg['render']:\n",
    "        while True:\n",
    "            # 在环境中评估智能体的性能，并输出奖励\n",
    "            ep_r = evaluate_policy(env, agent, turns=1)\n",
    "            print('Env: ', cfg['env_name'],' Episode Reward: ', {ep_r})\n",
    "    else:\n",
    "        total_steps = 0\n",
    "        scores_deque = deque(maxlen=cfg['deque_maxlen'])\n",
    "\n",
    "        # 在达到最大训练步数前一直进行训练\n",
    "        while total_steps < cfg['Max_train_steps']:\n",
    "            # 重置环境，获取初始状态\n",
    "            s, info = env.reset(seed=env_seed)  # 重置环境，使用环境种子\n",
    "            env_seed += 1\n",
    "            done = False\n",
    "\n",
    "            # 与环境进行交互并训练\n",
    "            while not done:\n",
    "                if total_steps < cfg['random_steps']:\n",
    "                    a = env.action_space.sample()\n",
    "                else:\n",
    "                    a = agent.select_action(s, deterministic=False)\n",
    "                # 选择动作和动作对应的对数概率\n",
    "                s_next, r, dw, tr, info = env.step(a) # 与环境交互\n",
    "                done = (dw or tr)  # 如果游戏结束（死亡或胜利），则done为True\n",
    "\n",
    "                # 存储当前的转移数据\n",
    "                agent.replay_buffer.add(s, a, r, s_next, dw)\n",
    "                s = s_next\n",
    "                total_steps += 1\n",
    "\n",
    "                # 如果达到更新时间\n",
    "                if total_steps >= cfg['random_steps']:\n",
    "                    a_loss, c_loss = agent.train()  # 执行PPO算法的训练步骤\n",
    "                    if cfg['write']:\n",
    "                        writer.add_scalar('Loss_a', np.mean(a_loss.detach().cpu().numpy()), global_step=total_steps)\n",
    "                        writer.add_scalar('Loss_c', np.mean(c_loss.detach().cpu().numpy()), global_step=total_steps)\n",
    "\n",
    "                # 如果达到记录和日志的时间\n",
    "                if (total_steps % cfg['eval_interval'] == 0) and (total_steps >= cfg['random_steps']):\n",
    "                    # 在评估环境中评估智能体，并输出平均奖励\n",
    "                    score = evaluate_policy(eval_env, agent, turns=3, cfg=cfg)  # 对策略进行3次评估，取平均值\n",
    "                    scores_deque.append(score)\n",
    "                    if cfg['write']:\n",
    "                        writer.add_scalar('Score_ep', score, global_step=total_steps)  # 将评估得分记录到TensorBoard\n",
    "                        writer.add_scalar('Score_Average', np.mean(scores_deque), global_step=total_steps)\n",
    "                    print('EnvName:', cfg['env_name'], 'seed:', cfg['seed'],\n",
    "                          'steps: {}k'.format(int(total_steps / 1000)), 'score:', score)\n",
    "                    \n",
    "                if (total_steps % cfg['test_interval'] == 0) and (total_steps >= cfg['random_steps']):\n",
    "                    print(\"测试模型\")\n",
    "                    test_policy(eval_env, agent, total_steps, turns=1, path=cfg['path'], cfg=cfg)\n",
    "\n",
    "                # 如果达到保存模型的时间\n",
    "                if (total_steps % cfg['save_interval']  == 0) and (total_steps >= cfg['random_steps']):\n",
    "                    print(\"保存模型\")\n",
    "                    agent.save(total_steps)  # 保存模型\n",
    "\n",
    "                if (np.mean(scores_deque) >= cfg['mean_break']) and (len(scores_deque) >= cfg['deque_maxlen']):\n",
    "                    print('Environment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(total_steps, np.mean(scores_deque)))\n",
    "                    test_policy(eval_env, agent, total_steps, turns=1, path=cfg['path'], cfg=cfg)\n",
    "                    print(\"保存模型\")\n",
    "                    agent.save(total_steps)\n",
    "                    env.close()\n",
    "                    eval_env.close()\n",
    "                    return\n",
    "\n",
    "        env.close()\n",
    "        eval_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/q1001p/anaconda3/envs/DRL_3.11/lib/python3.11/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/q1001p/anaconda3/envs/DRL_3.11/lib/python3.11/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 50k score: 36\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 52k score: 85\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 54k score: 122\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 56k score: 128\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 58k score: 131\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 60k score: 233\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 62k score: 239\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 64k score: 233\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 66k score: 205\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 68k score: 162\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 70k score: 277\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 72k score: 153\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 74k score: 185\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 76k score: 249\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 78k score: 255\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 80k score: 302\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 82k score: 169\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 84k score: 330\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 86k score: 203\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 88k score: 196\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 90k score: 258\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 92k score: 348\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 94k score: 221\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 96k score: 227\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 98k score: 209\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 100k score: 302\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 102k score: 258\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 104k score: 107\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 106k score: 199\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 108k score: 134\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 110k score: 131\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 112k score: 146\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 114k score: 200\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 116k score: 197\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 118k score: 100\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 120k score: 150\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 122k score: 82\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 124k score: 100\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 126k score: 289\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 128k score: 135\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 130k score: 181\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 132k score: 168\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 134k score: 97\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 136k score: 147\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 138k score: 115\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 140k score: 97\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 142k score: 119\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 144k score: 200\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 146k score: 181\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 148k score: 110\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 150k score: 187\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 152k score: 95\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 154k score: 162\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 156k score: 159\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 158k score: 296\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 160k score: 174\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 162k score: 221\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 164k score: 212\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 166k score: 286\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 168k score: 355\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 170k score: 181\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 172k score: 200\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 174k score: 159\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 176k score: 128\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 178k score: 144\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 180k score: 224\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 182k score: 209\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 184k score: 153\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 186k score: 335\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 188k score: 304\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 190k score: 264\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 192k score: 227\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 194k score: 292\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 196k score: 78\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 198k score: 242\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 200k score: 134\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 202k score: 310\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 204k score: 119\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 206k score: 342\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 208k score: 330\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 210k score: 249\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 212k score: 304\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 214k score: 122\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 216k score: 209\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 218k score: 221\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 220k score: 76\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 222k score: 152\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 224k score: 106\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 226k score: 128\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 228k score: 91\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 230k score: 146\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 232k score: 94\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 234k score: 280\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 236k score: 268\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 238k score: 311\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 240k score: 165\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 242k score: 138\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 244k score: 172\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 246k score: 295\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 248k score: 79\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 250k score: 103\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 252k score: 79\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 254k score: 224\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 256k score: 227\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 258k score: 112\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 260k score: 138\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 262k score: 187\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 264k score: 211\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 266k score: 149\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 268k score: 333\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 270k score: 212\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 272k score: 107\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 274k score: 128\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 276k score: 239\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 278k score: 79\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 280k score: 212\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 282k score: 270\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 284k score: 107\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 286k score: 163\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 288k score: 113\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 290k score: 153\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 292k score: 249\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 294k score: 94\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 296k score: 141\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 298k score: 187\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 300k score: 122\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 302k score: 159\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 304k score: 119\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 306k score: 209\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 308k score: 88\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 310k score: 113\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 312k score: 147\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 314k score: 283\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 316k score: 203\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 318k score: 330\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 320k score: 218\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 322k score: 166\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 324k score: 147\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 326k score: 187\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 328k score: 163\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 330k score: 150\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 332k score: 169\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 334k score: 94\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 336k score: 163\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 338k score: 255\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 340k score: 178\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 342k score: 175\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 344k score: 159\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 346k score: 107\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 348k score: 122\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 350k score: 212\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 352k score: 165\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 354k score: 98\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 356k score: 144\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 358k score: 175\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 360k score: 225\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 362k score: 107\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 364k score: 267\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 366k score: 184\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 368k score: 221\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 370k score: 190\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 372k score: 218\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 374k score: 101\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 376k score: 203\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 378k score: 166\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 380k score: 122\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 382k score: 107\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 384k score: 91\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 386k score: 206\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 388k score: 212\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 390k score: 122\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 392k score: 157\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 394k score: 217\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 396k score: 184\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 398k score: 240\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 400k score: 128\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 402k score: 94\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 404k score: 147\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 406k score: 137\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 408k score: 209\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 410k score: 75\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 412k score: 166\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 414k score: 224\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 416k score: 194\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 418k score: 85\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 420k score: 227\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 422k score: 184\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 424k score: 140\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 426k score: 125\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 428k score: 132\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 430k score: 115\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 432k score: 156\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 434k score: 144\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 436k score: 165\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 438k score: 221\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 440k score: 125\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 442k score: 91\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 444k score: 104\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 446k score: 128\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 448k score: 159\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 450k score: 199\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 452k score: 175\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 454k score: 159\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 456k score: 119\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 458k score: 153\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 460k score: 181\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 462k score: 219\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 464k score: 130\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 466k score: 181\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 468k score: 104\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 470k score: 150\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 472k score: 146\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 474k score: 224\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 476k score: 199\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 478k score: 165\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 480k score: 159\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 482k score: 199\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 484k score: 345\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 486k score: 97\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 488k score: 141\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 490k score: 128\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 492k score: 199\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 494k score: 171\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 496k score: 153\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 498k score: 162\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 500k score: 240\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 502k score: 220\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 504k score: 140\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 506k score: 160\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 508k score: 215\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 510k score: 85\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 512k score: 165\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 514k score: 368\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 516k score: 205\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 518k score: 268\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 520k score: 125\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 522k score: 202\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 524k score: 276\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 526k score: 370\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 528k score: 245\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 530k score: 189\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 532k score: 326\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 534k score: 286\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 536k score: 308\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 538k score: 233\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 540k score: 194\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 542k score: 180\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 544k score: 113\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 546k score: 252\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 548k score: 166\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 550k score: 293\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 552k score: 237\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 554k score: 171\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 556k score: 82\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 558k score: 293\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 560k score: 144\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 562k score: 271\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 564k score: 317\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 566k score: 255\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 568k score: 301\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 570k score: 271\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 572k score: 298\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 574k score: 243\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 576k score: 218\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 578k score: 246\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 580k score: 143\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 582k score: 107\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 584k score: 265\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 586k score: 304\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 588k score: 104\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 590k score: 296\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 592k score: 133\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 594k score: 227\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 596k score: 501\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 598k score: 175\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 600k score: 413\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 602k score: 339\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 604k score: 130\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 606k score: 110\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 608k score: 274\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 610k score: 82\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 612k score: 280\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 614k score: 242\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 616k score: 153\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 618k score: 119\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 620k score: 209\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 622k score: 271\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 624k score: 156\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 626k score: 264\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 628k score: 354\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 630k score: 214\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 632k score: 396\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 634k score: 243\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 636k score: 271\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 638k score: 107\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 640k score: 169\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 642k score: 352\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 644k score: 118\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 646k score: 259\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 648k score: 183\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 650k score: 128\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 652k score: 323\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 654k score: 227\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 656k score: 168\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 658k score: 386\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 660k score: 186\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 662k score: 311\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 664k score: 361\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 666k score: 175\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 668k score: 302\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 670k score: 223\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 672k score: 240\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 674k score: 205\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 676k score: 150\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 678k score: 119\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 680k score: 180\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 682k score: 175\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 684k score: 169\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 686k score: 195\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 688k score: 200\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 690k score: 323\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 692k score: 181\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 694k score: 142\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 696k score: 243\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 698k score: 106\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 700k score: 122\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 702k score: 224\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 704k score: 193\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 706k score: 104\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 708k score: 121\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 710k score: 134\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 712k score: 119\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 714k score: 419\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 716k score: 156\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 718k score: 168\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 720k score: 158\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 722k score: 246\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 724k score: 131\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 726k score: 162\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 728k score: 162\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 730k score: 202\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 732k score: 150\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 734k score: 119\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 736k score: 217\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 738k score: 103\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 740k score: 91\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 742k score: 103\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 744k score: 121\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 746k score: 319\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 748k score: 178\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 750k score: 187\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 752k score: 150\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 754k score: 97\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 756k score: 149\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 758k score: 103\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 760k score: 103\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 762k score: 227\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 764k score: 175\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 766k score: 139\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 768k score: 122\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 770k score: 128\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 772k score: 100\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 774k score: 134\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 776k score: 94\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 778k score: 104\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 780k score: 85\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 782k score: 162\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 784k score: 100\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 786k score: 88\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 788k score: 116\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 790k score: 184\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 792k score: 103\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 794k score: 137\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 796k score: 158\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 798k score: 119\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 800k score: 100\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 802k score: 109\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 804k score: 162\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 806k score: 186\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 808k score: 152\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 810k score: 133\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 812k score: 222\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 814k score: 100\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 816k score: 152\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 818k score: 153\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 820k score: 156\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 822k score: 88\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 824k score: 85\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 826k score: 106\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 828k score: 137\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 830k score: 227\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 832k score: 155\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 834k score: 100\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 836k score: 192\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 838k score: 125\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 840k score: 128\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 842k score: 103\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 844k score: 79\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 846k score: 122\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 848k score: 229\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 850k score: 91\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 852k score: 94\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 854k score: 119\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 856k score: 122\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 858k score: 135\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 860k score: 139\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 862k score: 163\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 864k score: 106\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 866k score: 138\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 868k score: 156\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 870k score: 113\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 872k score: 116\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 874k score: 91\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 876k score: 91\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 878k score: 91\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 880k score: 119\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 882k score: 94\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 884k score: 107\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 886k score: 109\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 888k score: 91\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 890k score: 91\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 892k score: 103\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 894k score: 91\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 896k score: 150\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 898k score: 197\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 900k score: 125\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 902k score: 113\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 904k score: 110\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 906k score: 106\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 908k score: 97\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 910k score: 100\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 912k score: 82\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 914k score: 205\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 916k score: 90\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 918k score: 208\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 920k score: 134\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 922k score: 88\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 924k score: 107\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 926k score: 128\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 928k score: 149\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 930k score: 110\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 932k score: 87\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 934k score: 137\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 936k score: 229\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 938k score: 174\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 940k score: 103\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 942k score: 156\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 944k score: 103\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 946k score: 149\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 948k score: 113\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 950k score: 116\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 952k score: 76\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 954k score: 101\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 956k score: 168\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 958k score: 161\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 960k score: 100\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 962k score: 116\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 964k score: 177\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 966k score: 137\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 968k score: 196\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 970k score: 88\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 972k score: 103\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 974k score: 110\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 976k score: 131\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 978k score: 99\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 980k score: 140\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 982k score: 179\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 984k score: 221\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 986k score: 190\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 988k score: 107\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 990k score: 147\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 992k score: 125\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 994k score: 138\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 996k score: 144\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 998k score: 147\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1000k score: 128\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1002k score: 112\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1004k score: 100\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1006k score: 223\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1008k score: 128\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1010k score: 205\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1012k score: 141\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1014k score: 144\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1016k score: 174\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1018k score: 175\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1020k score: 88\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1022k score: 128\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1024k score: 124\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1026k score: 206\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1028k score: 179\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1030k score: 174\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1032k score: 181\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1034k score: 205\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1036k score: 110\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1038k score: 106\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1040k score: 97\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1042k score: 110\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1044k score: 107\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1046k score: 159\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1048k score: 88\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1050k score: 162\n",
      "测试模型\n",
      "保存视频\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/q1001p/Python/Ducuments/RL/PG/DDPG/utils.py:219: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). Consider using `matplotlib.pyplot.close()`.\n",
      "  plt.figure(figsize=(frames[0].shape[1]/72, frames[0].shape[0]/72), dpi=72)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1052k score: 128\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1054k score: 196\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1056k score: 196\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1058k score: 260\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1060k score: 82\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1062k score: 155\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1064k score: 100\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1066k score: 125\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1068k score: 155\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1070k score: 100\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1072k score: 100\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1074k score: 104\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1076k score: 122\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1078k score: 88\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1080k score: 167\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1082k score: 131\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1084k score: 126\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1086k score: 168\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1088k score: 110\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1090k score: 112\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1092k score: 279\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1094k score: 100\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1096k score: 94\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1098k score: 104\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1100k score: 170\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1102k score: 97\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1104k score: 97\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1106k score: 107\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1108k score: 209\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1110k score: 85\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1112k score: 101\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1114k score: 162\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1116k score: 98\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1118k score: 116\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1120k score: 106\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1122k score: 113\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1124k score: 91\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1126k score: 109\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1128k score: 205\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1130k score: 97\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1132k score: 174\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1134k score: 88\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1136k score: 226\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1138k score: 94\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1140k score: 113\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1142k score: 146\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1144k score: 116\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1146k score: 165\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1148k score: 125\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1150k score: 180\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1152k score: 112\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1154k score: 76\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1156k score: 110\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1158k score: 103\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1160k score: 136\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1162k score: 112\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1164k score: 97\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1166k score: 104\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1168k score: 130\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1170k score: 100\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1172k score: 116\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1174k score: 115\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1176k score: 119\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1178k score: 140\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1180k score: 94\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1182k score: 97\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1184k score: 103\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1186k score: 106\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1188k score: 143\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1190k score: 126\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1192k score: 85\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1194k score: 140\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1196k score: 76\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1198k score: 180\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1200k score: 101\n",
      "测试模型\n",
      "保存视频\n",
      "保存模型\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1202k score: 85\n",
      "EnvName: InvertedDoublePendulum-v4 seed: 1 steps: 1204k score: 128\n"
     ]
    }
   ],
   "source": [
    "train(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL_3.11",
   "language": "python",
   "name": "drl_3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
